<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Further Experiments">
<title>Deep Learning attempt for RNA inverse design?</title>

<link rel='canonical' href='https://mosfish.github.io/p/deep-learning-attempt-for-rna-inverse-design/'>

<link rel="stylesheet" href="/scss/style.min.6a692fd055deae459f2a9767f57f3855ba80cafd5041317f24f7360f6ca47cdf.css"><meta property='og:title' content="Deep Learning attempt for RNA inverse design?">
<meta property='og:description' content="Further Experiments">
<meta property='og:url' content='https://mosfish.github.io/p/deep-learning-attempt-for-rna-inverse-design/'>
<meta property='og:site_name' content='Mosfish&#39;s Blog'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2025-09-30T19:45:11&#43;08:00'/><meta property='article:modified_time' content='2025-09-30T19:45:11&#43;08:00'/>
<meta name="twitter:title" content="Deep Learning attempt for RNA inverse design?">
<meta name="twitter:description" content="Further Experiments">
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/head_hu_c416b63add1b23cc.webp" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Mosfish&#39;s Blog</a></h1>
            <h2 class="site-description">I still feel like it has something to do with those fries on the pier.</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='mailto:wenxy59@mail2.sysu.edu.cn'
                        target="_blank"
                        title="Email"
                        rel="me"
                    >
                        
                        
                            <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-mail"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 7a2 2 0 0 1 2 -2h14a2 2 0 0 1 2 2v10a2 2 0 0 1 -2 2h-14a2 2 0 0 1 -2 -2v-10z" /><path d="M3 7l9 6l9 -6" /></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://github.com/Mosfish'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://mosfish.github.io/'
                        target="_blank"
                        title="Homepage"
                        rel="me"
                    >
                        
                        
                            <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-building-bank"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 21l18 0" /><path d="M3 10l18 0" /><path d="M5 6l7 -3l7 3" /><path d="M4 10l0 11" /><path d="M20 10l0 11" /><path d="M8 14l0 3" /><path d="M12 14l0 3" /><path d="M16 14l0 3" /></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://www.linkedin.com/in/xiangyuwen-mosfish/'
                        target="_blank"
                        title="Linkedin"
                        rel="me"
                    >
                        
                        
                            <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-brand-linkedin"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 11v5" /><path d="M8 8v.01" /><path d="M12 16v-5" /><path d="M16 16v-3a2 2 0 1 0 -4 0" /><path d="M3 7a4 4 0 0 1 4 -4h10a4 4 0 0 1 4 4v10a4 4 0 0 1 -4 4h-10a4 4 0 0 1 -4 -4z" /></svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页 | Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='https://wenxy59.github.io/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于 | About</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>归档 | Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索 | Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E6%8A%80%E6%9C%AF%E9%93%BE%E6%8E%A5friends-links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>技术链接&amp;friends | Links</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#graph-mamba的模块添加">Graph-Mamba的模块添加</a>
      <ol>
        <li><a href="#文献graph-mamba-towards-long-range-graph-sequence-modeling-with--selective-state-spaces">文献：Graph-Mamba: Towards Long-Range Graph Sequence Modeling with  Selective State Spaces</a></li>
        <li><a href="#输入输出匹配与debug修改">输入输出匹配与debug修改</a></li>
        <li><a href="#实验测试">实验测试</a></li>
      </ol>
    </li>
    <li><a href="#transformer-gvp--preln">Transformer GVP + PreLN</a></li>
    <li><a href="#long-short-transformer-gvp">Long-short Transformer GVP</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/ai4s/" style="background-color: #800020; color: #fff;">
                AI4S
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/deep-learning-attempt-for-rna-inverse-design/">Deep Learning attempt for RNA inverse design?</a>
        </h2>
    
        
        <h3 class="article-subtitle">
            Further Experiments
        </h3>
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Sep 30, 2025</time>
            </div>
        

        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h1 id="正式修改与实验">正式修改与实验
</h1><p>在测试集均匀划分后，我首先对gRNAde模型进行了重新训练与测试，此时我发现结果不稳定，但是问题不大。简单画图如下：</p>
<div align=center><img src="grnanew.png" width=600/></div>
<center><font size=2>在测试集的结果</font></center>
<h2 id="graph-mamba的模块添加">Graph-Mamba的模块添加
</h2><h3 id="文献graph-mamba-towards-long-range-graph-sequence-modeling-with--selective-state-spaces">文献：Graph-Mamba: Towards Long-Range Graph Sequence Modeling with  Selective State Spaces
</h3><p>文献的source code在：https://github.com/bowang-lab/Graph-Mamba.</p>
<p>本文提出了基于mamba的gnn。</p>
<h3 id="输入输出匹配与debug修改">输入输出匹配与debug修改
</h3><p>把grnade的GVP-GNN层替换为graphmamba论文提到的gps_layer，同时由于本层使用的是其加载的gatedgnn本地模型，在向量标量与维度问题上面容易有不匹配的问题，所以需要进行修改。</p>
<p>首先是main.py，我也忘了改动哪里了，先放上来：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">dotenv</span>
</span></span><span class="line"><span class="cl"><span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">&#34;.env&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">warnings</span>
</span></span><span class="line"><span class="cl"><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&#34;ignore&#34;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&#34;ignore&#34;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">RuntimeWarning</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&#34;ignore&#34;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">argparse</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">wandb</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch_geometric</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.loader</span> <span class="kn">import</span> <span class="n">DataLoader</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.trainer</span> <span class="kn">import</span> <span class="n">train</span><span class="p">,</span> <span class="n">evaluate</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.data.dataset</span> <span class="kn">import</span> <span class="n">RNADesignDataset</span><span class="p">,</span> <span class="n">BatchSampler</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.models</span> <span class="kn">import</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">AutoregressiveMultiGNNv1</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">NonAutoregressiveMultiGNNv1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.constants</span> <span class="kn">import</span> <span class="n">DATA_PATH</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Main function for training and evaluating gRNAde.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Set seed</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_seed</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise model</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">config</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_param</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_param</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">MODEL</span><span class="se">\n</span><span class="s1">    </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="se">\n</span><span class="s1">    Total parameters: </span><span class="si">{</span><span class="n">total_param</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;total_param&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_param</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Load checkpoint</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">model_path</span> <span class="o">!=</span> <span class="s1">&#39;&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">evaluate</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Load test set</span>
</span></span><span class="line"><span class="cl">        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">test_list</span> <span class="o">=</span> <span class="n">get_data_splits</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">split_type</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">split</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">testset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">test_list</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&#34;test&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_loader</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">testset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Run evaluator + save designed structures</span>
</span></span><span class="line"><span class="cl">        <span class="n">results</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">config</span><span class="o">.</span><span class="n">n_samples</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">config</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">device</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">model_name</span><span class="o">=</span><span class="s2">&#34;test&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">,</span> <span class="s1">&#39;perplexity&#39;</span><span class="p">,</span> <span class="s1">&#39;sc_score_eternafold&#39;</span><span class="p">,</span> <span class="s1">&#39;sc_score_ribonanzanet&#39;</span><span class="p">,</span> <span class="s1">&#39;sc_score_rhofold&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">save_designs</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">df</span><span class="p">,</span> <span class="n">samples_list</span><span class="p">,</span> <span class="n">recovery_list</span><span class="p">,</span> <span class="n">perplexity_list</span><span class="p">,</span> \
</span></span><span class="line"><span class="cl">        <span class="n">scscore_list</span><span class="p">,</span> <span class="n">scscore_ribonanza_list</span><span class="p">,</span> \
</span></span><span class="line"><span class="cl">        <span class="n">scscore_rmsd_list</span><span class="p">,</span> <span class="n">scscore_tm_list</span><span class="p">,</span> <span class="n">scscore_gdt_list</span><span class="p">,</span> \
</span></span><span class="line"><span class="cl">        <span class="n">rmsd_within_thresh</span><span class="p">,</span> <span class="n">tm_within_thresh</span><span class="p">,</span> <span class="n">gdt_within_thresh</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Save results</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&#34;test_results.pt&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Update wandb summary metrics</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_recovery&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recovery_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_perplexity&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perplexity_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_scscore&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_scscore_ribonanza&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_ribonanza_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_scscore_rmsd&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_rmsd_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_scscore_tm&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_tm_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_scscore_gdt&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_gdt_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_rmsd_within_thresh&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rmsd_within_thresh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_tm_within_thresh&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tm_within_thresh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_test_gdt_within_thresh&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">gdt_within_thresh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;BEST test recovery: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recovery_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                perplexity: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perplexity_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                scscore: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                scscore_ribonanza: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_ribonanza_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                scscore_rmsd: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_rmsd_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                scscore_tm: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_tm_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                scscore_gdt: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_gdt_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                rmsd_within_thresh: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rmsd_within_thresh</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                tm_within_thresh: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tm_within_thresh</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                gdt_within_thresh: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">gdt_within_thresh</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Get train, val, test data samples as lists</span>
</span></span><span class="line"><span class="cl">        <span class="n">train_list</span><span class="p">,</span> <span class="n">val_list</span><span class="p">,</span> <span class="n">test_list</span> <span class="o">=</span> <span class="n">get_data_splits</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">split_type</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">split</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Load datasets</span>
</span></span><span class="line"><span class="cl">        <span class="n">trainset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_list</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&#34;train&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">valset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">val_list</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&#34;val&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">testset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">test_list</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&#34;test&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Prepare dataloaders</span>
</span></span><span class="line"><span class="cl">        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">trainset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">val_loader</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">valset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_loader</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">testset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Run trainer</span>
</span></span><span class="line"><span class="cl">        <span class="n">train</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_data_splits</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">split_type</span><span class="o">=</span><span class="s2">&#34;structsim_v2&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns train, val, test data splits as lists.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#data_list = list(torch.load(os.path.join(DATA_PATH, &#34;processed.pt&#34;)).values())</span>
</span></span><span class="line"><span class="cl">    <span class="n">data_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="s2">&#34;processed.pt&#34;</span><span class="p">),</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">index_list_by_indices</span><span class="p">(</span><span class="n">lst</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># return [lst[index] if 0 &lt;= index &lt; len(lst) else None for index in indices]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">[</span><span class="n">lst</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Pre-compute using notebooks/split_{split_type}.ipynb</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_idx_list</span><span class="p">,</span> <span class="n">val_idx_list</span><span class="p">,</span> <span class="n">test_idx_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">split_type</span><span class="si">}</span><span class="s2">_split.pt&#34;</span><span class="p">))</span> 
</span></span><span class="line"><span class="cl">    <span class="n">train_list</span> <span class="o">=</span> <span class="n">index_list_by_indices</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">train_idx_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_list</span> <span class="o">=</span> <span class="n">index_list_by_indices</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">val_idx_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_list</span> <span class="o">=</span> <span class="n">index_list_by_indices</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">test_idx_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">train_list</span><span class="p">,</span> <span class="n">val_list</span><span class="p">,</span> <span class="n">test_list</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_dataset</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">data_list</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&#34;train&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a Dataset for a given split.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">RNADesignDataset</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">data_list</span> <span class="o">=</span> <span class="n">data_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">split</span> <span class="o">=</span> <span class="n">split</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">radius</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">radius</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_k</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_rbf</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_rbf</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_posenc</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_posenc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">max_num_conformers</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_num_conformers</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">noise_scale</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">noise_scale</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">config</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">dataset</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">exclude_keys</span><span class="o">=</span><span class="p">[],</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a DataLoader for a given Dataset.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        dataset (RNADesignDataset): dataset object
</span></span></span><span class="line"><span class="cl"><span class="s2">        config (dict): wandb configuration dictionary
</span></span></span><span class="line"><span class="cl"><span class="s2">        shuffle (bool): whether to shuffle the dataset
</span></span></span><span class="line"><span class="cl"><span class="s2">        pin_memory (bool): whether to pin memory
</span></span></span><span class="line"><span class="cl"><span class="s2">        exclue_keys (list): list of keys to exclude during batching
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">dataset</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">num_workers</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">BatchSampler</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">node_counts</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">node_counts</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">max_nodes_batch</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_nodes_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_nodes_sample</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_nodes_sample</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">pin_memory</span> <span class="o">=</span> <span class="n">pin_memory</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">exclude_keys</span> <span class="o">=</span> <span class="n">exclude_keys</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a Model for a given config.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_class</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;ARv1&#39;</span> <span class="p">:</span> <span class="n">AutoregressiveMultiGNNv1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;NARv1&#39;</span><span class="p">:</span> <span class="n">NonAutoregressiveMultiGNNv1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">}[</span><span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model_class</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">node_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">node_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">node_h_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">edge_in_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">edge_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_h_dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">num_layers</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">drop_rate</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">drop_rate</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">out_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">out_dim</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Sets random seed for reproducibility.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s1">&#39;xpu&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--config&#39;</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s1">&#39;config&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;configs/default.yaml&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--expt_name&#39;</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s1">&#39;expt_name&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--tags&#39;</span><span class="p">,</span> <span class="n">nargs</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s1">&#39;tags&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">[])</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--no_wandb&#39;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s2">&#34;store_true&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span><span class="p">,</span> <span class="n">unknown</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise wandb</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">no_wandb</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">project</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;WANDB_PROJECT&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">entity</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;WANDB_ENTITY&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">config</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">name</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">expt_name</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;disabled&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">project</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;WANDB_PROJECT&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">entity</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;WANDB_ENTITY&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">config</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">name</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">expt_name</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">tags</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">tags</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;online&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">config</span>
</span></span><span class="line"><span class="cl">    <span class="n">config_str</span> <span class="o">=</span> <span class="s2">&#34;</span><span class="se">\n</span><span class="s2">CONFIG&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">config_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">    </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">config_str</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Set device (GPU/CPU/XPU)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;xpu&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">]: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">device_count</span><span class="p">())]</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;xpu:</span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">gpu</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda:</span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">gpu</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Run main function</span>
</span></span><span class="line"><span class="cl">    <span class="n">main</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>之后，需要把Graph-Mamba的文件拷贝到当前位置：
<img src="assets/image-20250930191543953.png" 
alt="image-20250930191543953" 
style="display: block; margin: 0 auto; zoom: 67%;" /></p>
<p>里面的adapters.py是来处理数据格式不匹配的问题。</p>
<ul>
<li>
<ol>
<li>创建了 <code>GVP_to_GPS_Adapter</code> 类，负责将 gRNAde 的 GVP 数据格式 <code>(标量, 向量)</code> 元组，转换为 <code>GPSLayer</code> 能识别的、带有 <code>.x</code> 和 <code>.edge_attr</code> 的标准 <code>Batch</code> 对象。</li>
<li>创建了 <code>GPS_to_GVP_Adapter</code> 类，负责反向操作，将 <code>GPSLayer</code> 的输出转换回 GVP 格式，以便后续的 gRNAde 解码器能够使用。</li>
<li><strong>后续升级1</strong>：为 <code>GVP_to_GPS_Adapter</code> 增加了<strong>展平 (flatten)</strong> 逻辑，使其能兼容训练和采样两种模式下维度不一致的输入。</li>
<li><strong>后续升级2</strong>：为 <code>GVP_to_GPS_Adapter</code> 增加了<strong>检查并创建 <code>batch.batch</code> 属性</strong>的逻辑。</li>
<li><strong>后续升级3</strong>：为 <code>GPS_to_GVP_Adapter</code> 增加了<strong>线性投射层</strong>，用于降维。</li>
</ol>
</li>
<li><strong>为什么这么做</strong>：
<ul>
<li><code>(1, 2)</code> 是为了解决 <strong>gRNAde 和 Graph-Mamba 之间最根本的数据格式不兼容</strong> 问题。</li>
<li><code>(3)</code> 是为了修复 <code>RuntimeError: Tensors must have same number of dimensions</code>，即拼接3D和2D张量的错误。</li>
<li><code>(4)</code> 是为了修复 <code>AttributeError: 'NoneType' object has no attribute 'gather'</code>，即 <code>lexsort</code> 函数找不到 <code>batch.batch</code> 属性的错误。</li>
<li><code>(5)</code> 是为了修复 <code>RuntimeError: shape '...' is invalid for input of size ...</code>，即 <code>GPSLayer</code> 输出的高维特征无法被正确拆分的错误。</li>
</ul>
</li>
</ul>
<p>adapters.py文件内容如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># File path: src/layers/adapters.py</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.data</span> <span class="kn">import</span> <span class="n">Batch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GVP_to_GPS_Adapter</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gvp_node_dims</span><span class="p">,</span> <span class="n">gvp_edge_dims</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_s_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_v_dim</span> <span class="o">=</span> <span class="n">gvp_node_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_s_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_v_dim</span> <span class="o">=</span> <span class="n">gvp_edge_dims</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gvp_node_data</span><span class="p">,</span> <span class="n">gvp_edge_data</span><span class="p">,</span> <span class="n">batch_obj</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Process node features ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_node</span><span class="p">,</span> <span class="n">v_node</span> <span class="o">=</span> <span class="n">gvp_node_data</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># [Key modification] Flatten both scalar and vector parts to 2D</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_node_flat</span> <span class="o">=</span> <span class="n">s_node</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s_node</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_node_flat</span> <span class="o">=</span> <span class="n">v_node</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_node</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Now both are 2D tensors, can safely concatenate</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s_node_flat</span><span class="p">,</span> <span class="n">v_node_flat</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Apply same processing to edge features for robustness ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_edge</span><span class="p">,</span> <span class="n">v_edge</span> <span class="o">=</span> <span class="n">gvp_edge_data</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">s_edge_flat</span> <span class="o">=</span> <span class="n">s_edge</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_edge_flat</span> <span class="o">=</span> <span class="n">v_edge</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">edge_attr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s_edge_flat</span><span class="p">,</span> <span class="n">v_edge_flat</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Create new Batch object (logic unchanged) ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_batch</span> <span class="o">=</span> <span class="n">batch_obj</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_batch</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_batch</span><span class="o">.</span><span class="n">edge_attr</span> <span class="o">=</span> <span class="n">edge_attr</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">new_batch</span><span class="p">,</span> <span class="s1">&#39;batch&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">new_batch</span><span class="o">.</span><span class="n">batch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># If batch attribute doesn&#39;t exist or is None, create a zero tensor</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># This means all nodes in the batch belong to the same graph (graph 0)</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">new_batch</span><span class="o">.</span><span class="n">num_nodes</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">new_batch</span><span class="p">,</span> <span class="s1">&#39;num_nodes&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="n">new_batch</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">new_batch</span><span class="o">.</span><span class="n">batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">new_batch</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">new_batch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GPS_to_GVP_Adapter</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gvp_node_dims</span><span class="p">,</span> <span class="n">gvp_edge_dims</span><span class="p">,</span> <span class="n">gps_node_dim</span><span class="p">,</span> <span class="n">gps_edge_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_s_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_v_dim</span> <span class="o">=</span> <span class="n">gvp_node_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_s_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_v_dim</span> <span class="o">=</span> <span class="n">gvp_edge_dims</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># [Core modification] Create linear projection layers for dimensionality reduction</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Node feature projection layer</span>
</span></span><span class="line"><span class="cl">        <span class="n">gvp_node_total_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_s_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_v_dim</span> <span class="o">*</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">gps_node_dim</span><span class="p">,</span> <span class="n">gvp_node_total_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Edge feature projection layer</span>
</span></span><span class="line"><span class="cl">        <span class="n">gvp_edge_total_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_s_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_v_dim</span> <span class="o">*</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">gps_edge_dim</span><span class="p">,</span> <span class="n">gvp_edge_total_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gps_out_batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Process node features ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">gps_out_batch</span><span class="o">.</span><span class="n">x</span> <span class="c1"># dimension: [n_nodes, 176]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># project to: [n_nodes, 176] (assuming node dimension unchanged)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">s_node</span> <span class="o">=</span> <span class="n">x_proj</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">node_s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_node_flat</span> <span class="o">=</span> <span class="n">x_proj</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_s_dim</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_node</span> <span class="o">=</span> <span class="n">v_node_flat</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_node_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_v_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Process edge features ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_attr</span> <span class="o">=</span> <span class="n">gps_out_batch</span><span class="o">.</span><span class="n">edge_attr</span> <span class="c1"># dimension: [n_edges, 176]</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_attr_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_proj</span><span class="p">(</span><span class="n">edge_attr</span><span class="p">)</span> <span class="c1"># [Core modification] project to: [n_edges, 76]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">s_edge</span> <span class="o">=</span> <span class="n">edge_attr_proj</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_edge_flat</span> <span class="o">=</span> <span class="n">edge_attr_proj</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_s_dim</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_edge</span> <span class="o">=</span> <span class="n">v_edge_flat</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_edge_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_v_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">s_node</span><span class="p">,</span> <span class="n">v_node</span><span class="p">),</span> <span class="p">(</span><span class="n">s_edge</span><span class="p">,</span> <span class="n">v_edge</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>以及trainer.py，在 <code>loop</code> 函数的 <code>for</code> 循环内部，<code>model(batch)</code> 调用之前，增加了一个 <code>if/else</code> 判断，为 <code>batch</code> 对象手动添加 <code>.split</code> 属性 (<code>'train'</code> 或 <code>'val'</code>)，来修复 <code>AttributeError: ... has no attribute 'split'</code>，因为 <code>GPSLayer</code> 需要这个属性来区分训练和评估模式，以执行不同的排序策略。具体：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span><span class="lnt">260
</span><span class="lnt">261
</span><span class="lnt">262
</span><span class="lnt">263
</span><span class="lnt">264
</span><span class="lnt">265
</span><span class="lnt">266
</span><span class="lnt">267
</span><span class="lnt">268
</span><span class="lnt">269
</span><span class="lnt">270
</span><span class="lnt">271
</span><span class="lnt">272
</span><span class="lnt">273
</span><span class="lnt">274
</span><span class="lnt">275
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">wandb</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">ReduceLROnPlateau</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.evaluator</span> <span class="kn">import</span> <span class="n">evaluate</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.constants</span> <span class="kn">import</span> <span class="n">NUM_TO_LETTER</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">config</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">train_loader</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">val_loader</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">test_loader</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">device</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Train RNA inverse folding model using the specified config and data loaders.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        config (dict): wandb configuration dictionary 
</span></span></span><span class="line"><span class="cl"><span class="s2">        model (nn.Module): RNA inverse folding model to be trained
</span></span></span><span class="line"><span class="cl"><span class="s2">        train_loader (DataLoader): training data loader
</span></span></span><span class="line"><span class="cl"><span class="s2">        val_loader (DataLoader): validation data loader
</span></span></span><span class="line"><span class="cl"><span class="s2">        test_loader (DataLoader): test data loader
</span></span></span><span class="line"><span class="cl"><span class="s2">        device (torch.device): device to train the model on
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise loss function</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">label_smoothing</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">label_smoothing</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">eval_loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise optimizer and scheduler</span>
</span></span><span class="line"><span class="cl">    <span class="n">lr</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">lr</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;xpu&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#=======</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Initialise save directory</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s2">&#34;..&#34;</span><span class="p">,</span> <span class="s2">&#34;mambagnnmodel&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#save_dir = os.path.abspath(save_dir)   </span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">#======</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise lookup table mapping integers to nucleotides</span>
</span></span><span class="line"><span class="cl">    <span class="n">lookup</span> <span class="o">=</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">featurizer</span><span class="o">.</span><span class="n">num_to_letter</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise best checkpoint information</span>
</span></span><span class="line"><span class="cl">    <span class="n">best_epoch</span><span class="p">,</span> <span class="n">best_val_loss</span><span class="p">,</span> <span class="n">best_val_acc</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">##################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Training loop over mini-batches</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Training iteration</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">train_confusion</span> <span class="o">=</span> <span class="n">loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">print_and_log</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">train_confusion</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;train&#34;</span><span class="p">,</span> <span class="n">lookup</span><span class="o">=</span><span class="n">lookup</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">config</span><span class="o">.</span><span class="n">val_every</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="c1"># Evaluate on validation set</span>
</span></span><span class="line"><span class="cl">                <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">val_confusion</span> <span class="o">=</span> <span class="n">loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">eval_loss_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">print_and_log</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">val_confusion</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;val&#34;</span><span class="p">,</span> <span class="n">lookup</span><span class="o">=</span><span class="n">lookup</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># LR scheduler step</span>
</span></span><span class="line"><span class="cl">                <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">val_acc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">lr</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">val_acc</span> <span class="o">&gt;</span> <span class="n">best_val_acc</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># Update best checkpoint</span>
</span></span><span class="line"><span class="cl">                    <span class="n">best_epoch</span><span class="p">,</span> <span class="n">best_val_loss</span><span class="p">,</span> <span class="n">best_val_acc</span> <span class="o">=</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># Evaluate on test set</span>
</span></span><span class="line"><span class="cl">                    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">,</span> <span class="n">test_confusion</span> <span class="o">=</span> <span class="n">loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">eval_loss_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">print_and_log</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">,</span> <span class="n">test_confusion</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;test&#34;</span><span class="p">,</span> <span class="n">lookup</span><span class="o">=</span><span class="n">lookup</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># Update wandb summary metrics</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_epoch&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_epoch</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_val_perp&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">best_val_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_val_acc&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_val_acc</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_test_perp&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_test_acc&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_acc</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                        <span class="c1"># Save best checkpoint</span>
</span></span><span class="line"><span class="cl">                        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s2">&#34;best_checkpoint.h5&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">checkpoint_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_checkpoint&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">checkpoint_path</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Save current epoch checkpoint</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s2">&#34;current_checkpoint.h5&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># End of training</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Evaluate best checkpoint</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;EVALUATION: loading </span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s1">&#39;best_checkpoint.h5&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2"> (epoch </span><span class="si">{</span><span class="n">best_epoch</span><span class="si">}</span><span class="s2">)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s1">&#39;best_checkpoint.h5&#39;</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">loader</span><span class="p">,</span> <span class="n">set_name</span> <span class="ow">in</span> <span class="p">[(</span><span class="n">test_loader</span><span class="p">,</span> <span class="s2">&#34;test&#34;</span><span class="p">),</span> <span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="s2">&#34;val&#34;</span><span class="p">)]:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Run evaluator</span>
</span></span><span class="line"><span class="cl">            <span class="n">results</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">model</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">config</span><span class="o">.</span><span class="n">n_samples</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">config</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">device</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">model_name</span><span class="o">=</span><span class="n">set_name</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;recovery&#39;</span><span class="p">,</span> <span class="s1">&#39;perplexity&#39;</span><span class="p">,</span> <span class="s1">&#39;sc_score_eternafold&#39;</span><span class="p">,</span> <span class="s1">&#39;sc_score_ribonanzanet&#39;</span><span class="p">,</span> <span class="s1">&#39;sc_score_rhofold&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">save_designs</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">df</span><span class="p">,</span> <span class="n">samples_list</span><span class="p">,</span> <span class="n">recovery_list</span><span class="p">,</span> <span class="n">perplexity_list</span><span class="p">,</span> \
</span></span><span class="line"><span class="cl">            <span class="n">scscore_list</span><span class="p">,</span> <span class="n">scscore_ribonanza_list</span><span class="p">,</span> \
</span></span><span class="line"><span class="cl">            <span class="n">scscore_rmsd_list</span><span class="p">,</span> <span class="n">scscore_tm_list</span><span class="p">,</span> <span class="n">scscore_gdt_list</span><span class="p">,</span> \
</span></span><span class="line"><span class="cl">            <span class="n">rmsd_within_thresh</span><span class="p">,</span> <span class="n">tm_within_thresh</span><span class="p">,</span> <span class="n">gdt_within_thresh</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Save results</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_results.pt&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Update wandb summary metrics</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_recovery&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recovery_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_perplexity&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perplexity_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_scscore&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_scscore_ribonanza&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_ribonanza_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_scscore_rmsd&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_rmsd_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_scscore_tm&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_tm_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_scscore_gdt&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_gdt_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_rmsd_within_thresh&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rmsd_within_thresh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_tm_within_thresh&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tm_within_thresh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;best_</span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2">_gdt_within_thresh&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">gdt_within_thresh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;BEST </span><span class="si">{</span><span class="n">set_name</span><span class="si">}</span><span class="s2"> recovery: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recovery_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                    perplexity: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">perplexity_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                    scscore: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                    scscore_ribonanza: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_ribonanza_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                    scscore_rmsd: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_rmsd_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                    scscore_tm: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_tm_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                    scscore_gdt: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scscore_gdt_list</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                    rmsd_within_thresh: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rmsd_within_thresh</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                    tm_within_thresh: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tm_within_thresh</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">                    gdt_within_thresh: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">gdt_within_thresh</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Training loop for a single epoch over the data loader.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        model (nn.Module): RNA inverse folding model
</span></span></span><span class="line"><span class="cl"><span class="s2">        dataloader (DataLoader): data loader for the current epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">        loss_fn (nn.Module): loss function to compute the loss
</span></span></span><span class="line"><span class="cl"><span class="s2">        optimizer (torch.optim): optimizer to update model parameters
</span></span></span><span class="line"><span class="cl"><span class="s2">        device (torch.device): device to train the model on
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Note:
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function is used for both training and evaluation loops.
</span></span></span><span class="line"><span class="cl"><span class="s2">        Not passing an optimizer will run the model in evaluation mode.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        float: average loss over the epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">        float: average accuracy over the epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">        np.ndarray: confusion matrix over the epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">confusion</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">model</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">out_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_loss</span><span class="p">,</span> <span class="n">total_correct</span><span class="p">,</span> <span class="n">total_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">t</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">        <span class="c1"># move batch to device</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1">##############main change here##############</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [Core modification] Add split attribute to batch object here</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">batch</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="s1">&#39;train&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">batch</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="s1">&#39;val&#39;</span> <span class="c1"># For evaluation mode, either &#39;val&#39; or &#39;test&#39; is fine</span>
</span></span><span class="line"><span class="cl">        <span class="c1">##########################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># forward pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="s2">&#34;CUDA out of memory&#34;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span> <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Skipped batch due to OOM&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="k">del</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>  <span class="c1"># free some memory</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># compute loss</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss_value</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">seq</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">optimizer</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># backpropagate loss and update parameters</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss_value</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># update metrics</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_nodes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">seq</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_loss</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss_value</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="o">*</span> <span class="n">num_nodes</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_count</span> <span class="o">+=</span> <span class="n">num_nodes</span>
</span></span><span class="line"><span class="cl">        <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">true</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">seq</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">pred</span> <span class="o">==</span> <span class="n">true</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">confusion</span> <span class="o">+=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">out_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">t</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&#34;</span><span class="si">%.5f</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_loss</span><span class="o">/</span><span class="n">total_count</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">total_count</span><span class="p">,</span> <span class="n">total_correct</span> <span class="o">/</span> <span class="n">total_count</span><span class="p">,</span> <span class="n">confusion</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">print_and_log</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">epoch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">confusion</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">recovery</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">lr</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">mode</span> <span class="o">=</span> <span class="s2">&#34;train&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">lookup</span> <span class="o">=</span> <span class="n">NUM_TO_LETTER</span><span class="p">,</span> <span class="c1"># reverse of {&#39;A&#39;: 0, &#39;G&#39;: 1, &#39;C&#39;: 2, &#39;U&#39;: 3}</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create log string and wandb metrics dict</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> perp: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> acc: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wandb_metrics</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">/loss&#34;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">/perp&#34;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">/acc&#34;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;epoch&#34;</span><span class="p">:</span> <span class="n">epoch</span> 
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Add learning rate to loggers</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&#34; lr: </span><span class="si">{</span><span class="n">lr</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb_metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;lr&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">recovery</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Add mean sequence recovery to loggers</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&#34; rec: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recovery</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb_metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">/recovery&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recovery</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">log_str</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">print_confusion</span><span class="p">(</span><span class="n">confusion</span><span class="p">,</span> <span class="n">lookup</span><span class="o">=</span><span class="n">lookup</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">wandb_metrics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">print_confusion</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">lookup</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">counts</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl">    <span class="n">mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">mat</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">res</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lookup</span><span class="o">.</span><span class="n">keys</span><span class="p">())):</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lookup</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Count</span><span class="se">\n</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lookup</span><span class="o">.</span><span class="n">keys</span><span class="p">())):</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="se">\t</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lookup</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">mat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="si">{}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>之后对models.py进行大改，只改autoregressive和上面一点的地方，</p>
<ol>
<li><strong>创建了 <code>EncoderBlock</code> 自定义模块</strong>，它将 <code>GVP_to_GPS_Adapter</code>、<code>GPSLayer</code> 和 <code>GPS_to_GVP_Adapter</code> 三个组件打包在一起，并定义了清晰的数据流。</li>
<li>在 <code>AutoregressiveMultiGNNv1</code> 的 <code>__init__</code> 函数中，用一个循环创建多个 <code>EncoderBlock</code> 实例，<strong>完全替换</strong>了原来创建 <code>MultiGVPConvLayer</code> 的 <code>self.encoder_layers</code>，为了解决 <code>nn.Sequential</code> 无法处理多输入的 <code>TypeError</code>。</li>
<li>在 <code>AutoregressiveMultiGNNv1</code> 的 <code>forward</code> 函数中，<strong>调整了运算顺序</strong>，将 <code>pool_multi_conf</code> 调用<strong>提前</strong>到编码器循环之前，以实现“先池化，再编码”的策略，让 <code>GPSLayer</code> 能正确处理多构象数据。</li>
<li>在 <code>AutoregressiveMultiGNNv1</code> 的 <code>sample</code> 函数中，<strong>完全同步</strong>了与 <code>forward</code> 函数一致的逻辑：添加 <code>.split</code> 属性、提前池化、并使用正确的参数调用新的 <code>EncoderBlock</code>。修复在<strong>推理/采样</strong>阶段出现的各种错误（<code>AttributeError</code>, <code>RuntimeError</code> 等），确保训练和推理两条代码路径的逻辑一致性。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">#####models.py changes######</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch_geometric</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Import layers</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.layers</span> <span class="kn">import</span> <span class="o">*</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.mgnn.gps_layer</span> <span class="kn">import</span> <span class="n">GPSLayer</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.mgnn.adapters</span> <span class="kn">import</span> <span class="n">GVP_to_GPS_Adapter</span><span class="p">,</span> <span class="n">GPS_to_GVP_Adapter</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># [Core modification] Create a custom EncoderBlock class to replace nn.Sequential</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">EncoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_h_dim</span><span class="p">,</span> <span class="n">edge_h_dim</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">local_gnn_type</span><span class="p">,</span> <span class="n">global_model_type</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Calculate various dimensions</span>
</span></span><span class="line"><span class="cl">        <span class="n">gvp_s_dim</span><span class="p">,</span> <span class="n">gvp_v_dim</span> <span class="o">=</span> <span class="n">node_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="n">gps_node_dim</span> <span class="o">=</span> <span class="n">gvp_s_dim</span> <span class="o">+</span> <span class="n">gvp_v_dim</span> <span class="o">*</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">gvp_edge_s_dim</span><span class="p">,</span> <span class="n">gvp_edge_v_dim</span> <span class="o">=</span> <span class="n">edge_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="n">gps_edge_dim</span> <span class="o">=</span> <span class="n">gvp_edge_s_dim</span> <span class="o">+</span> <span class="n">gvp_edge_v_dim</span> <span class="o">*</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Create three components internally</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">adapter_in</span> <span class="o">=</span> <span class="n">GVP_to_GPS_Adapter</span><span class="p">(</span><span class="n">gvp_node_dims</span><span class="o">=</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="n">gvp_edge_dims</span><span class="o">=</span><span class="n">edge_h_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gps_layer</span> <span class="o">=</span> <span class="n">GPSLayer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">dim_h</span><span class="o">=</span><span class="n">gps_node_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_dim_h</span><span class="o">=</span><span class="n">gps_edge_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">local_gnn_type</span><span class="o">=</span><span class="n">local_gnn_type</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">global_model_type</span><span class="o">=</span><span class="n">global_model_type</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">dropout</span><span class="o">=</span><span class="n">drop_rate</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">adapter_out</span> <span class="o">=</span> <span class="n">GPS_to_GVP_Adapter</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">gvp_node_dims</span><span class="o">=</span><span class="n">node_h_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">gvp_edge_dims</span><span class="o">=</span><span class="n">edge_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">gps_node_dim</span><span class="o">=</span><span class="n">gps_node_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">gps_edge_dim</span><span class="o">=</span><span class="n">gps_node_dim</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Explicitly define data flow</span>
</span></span><span class="line"><span class="cl">        <span class="n">gps_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapter_in</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">gps_out_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gps_layer</span><span class="p">(</span><span class="n">gps_batch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V_out</span><span class="p">,</span> <span class="n">h_E_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adapter_out</span><span class="p">(</span><span class="n">gps_out_batch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">h_V_out</span><span class="p">,</span> <span class="n">h_E_out</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>最后，修改gatedgcn文件，从 <code>graphmamba</code> 项目中拿来的 <code>GatedGCNLayer</code> 不够灵活，无法直接用于我们的混合维度场景。</p>
<ol>
<li>在 <code>GatedGCNLayer</code> 的 <code>__init__</code> 函数中，增加了一个 <code>edge_dim</code> 参数，并用它来初始化处理边特征的线性层 <code>self.C</code>，以修复 <code>RuntimeError: mat1 and mat2 shapes cannot be multiplied</code>，即边特征（76维）和线性层权重（期望176维）不匹配的错误。</li>
<li>同样在 <code>__init__</code> 中，增加了一个 <code>self.residual_e_proj</code> 线性投射层，它只在 <code>edge_dim</code> 和 <code>out_dim</code> 不相等时被创建；在 <code>forward</code> 函数的残差连接部分，增加了判断逻辑，如果 <code>self.residual_e_proj</code> 存在，就用它来对齐 <code>e_in</code> 的维度，然后再相加。这一点为了修复 <code>RuntimeError: The size of tensor a (76) must match...</code>，即边特征残差连接时，输入（76维）和输出（176维）维度不匹配的错误。</li>
</ol>
<p>具体代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch_geometric.nn</span> <span class="k">as</span> <span class="nn">pyg_nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.graphgym.models.layer</span> <span class="kn">import</span> <span class="n">LayerConfig</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_scatter</span> <span class="kn">import</span> <span class="n">scatter</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.graphgym.config</span> <span class="kn">import</span> <span class="n">cfg</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.graphgym.register</span> <span class="kn">import</span> <span class="n">register_layer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GatedGCNLayer</span><span class="p">(</span><span class="n">pyg_nn</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">MessagePassing</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        GatedGCN layer
</span></span></span><span class="line"><span class="cl"><span class="s2">        Residual Gated Graph ConvNets
</span></span></span><span class="line"><span class="cl"><span class="s2">        https://arxiv.org/pdf/1711.07553.pdf
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">edge_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">equivstable_pe</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">edge_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 【核心修改】如果边特征的输入输出维度不同，则添加一个投射层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">residual_e_proj</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">edge_dim</span> <span class="o">!=</span> <span class="n">out_dim</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">residual_e_proj</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">edge_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Handling for Equivariant and Stable PE using LapPE</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ICLR 2022 https://openreview.net/pdf?id=e95i1IHcWj</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">EquivStablePE</span> <span class="o">=</span> <span class="n">equivstable_pe</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">EquivStablePE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">mlp_r_ij</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">bn_node_x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">bn_edge_e</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">e</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_attr</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        x               : [n_nodes, in_dim]
</span></span></span><span class="line"><span class="cl"><span class="s2">        e               : [n_edges, in_dim]
</span></span></span><span class="line"><span class="cl"><span class="s2">        edge_index      : [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x_in</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">            <span class="n">e_in</span> <span class="o">=</span> <span class="n">e</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">Ax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Bx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Ce</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Dx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Ex</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">E</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Handling for Equivariant and Stable PE using LapPE</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ICLR 2022 https://openreview.net/pdf?id=e95i1IHcWj</span>
</span></span><span class="line"><span class="cl">        <span class="n">pe_LapPE</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">pe_EquivStableLapPE</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">EquivStablePE</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">,</span> <span class="n">e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">Bx</span><span class="o">=</span><span class="n">Bx</span><span class="p">,</span> <span class="n">Dx</span><span class="o">=</span><span class="n">Dx</span><span class="p">,</span> <span class="n">Ex</span><span class="o">=</span><span class="n">Ex</span><span class="p">,</span> <span class="n">Ce</span><span class="o">=</span><span class="n">Ce</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">e</span><span class="o">=</span><span class="n">e</span><span class="p">,</span> <span class="n">Ax</span><span class="o">=</span><span class="n">Ax</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">PE</span><span class="o">=</span><span class="n">pe_LapPE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_node_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_edge_e</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">e</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">e</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x_in</span> <span class="o">+</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_e_proj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">e_in</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_e_proj</span><span class="p">(</span><span class="n">e_in</span><span class="p">)</span> <span class="c1"># 将 76 维的 e_in 投射到 176 维</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">e</span> <span class="o">=</span> <span class="n">e_in</span> <span class="o">+</span> <span class="n">e</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">batch</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch</span><span class="o">.</span><span class="n">edge_attr</span> <span class="o">=</span> <span class="n">e</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">batch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Dx_i</span><span class="p">,</span> <span class="n">Ex_j</span><span class="p">,</span> <span class="n">PE_i</span><span class="p">,</span> <span class="n">PE_j</span><span class="p">,</span> <span class="n">Ce</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        </span><span class="si">{}</span><span class="s2">x_i           : [n_edges, out_dim]
</span></span></span><span class="line"><span class="cl"><span class="s2">        </span><span class="si">{}</span><span class="s2">x_j           : [n_edges, out_dim]
</span></span></span><span class="line"><span class="cl"><span class="s2">        </span><span class="si">{}</span><span class="s2">e             : [n_edges, out_dim]
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">e_ij</span> <span class="o">=</span> <span class="n">Dx_i</span> <span class="o">+</span> <span class="n">Ex_j</span> <span class="o">+</span> <span class="n">Ce</span>
</span></span><span class="line"><span class="cl">        <span class="n">sigma_ij</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">e_ij</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Handling for Equivariant and Stable PE using LapPE</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ICLR 2022 https://openreview.net/pdf?id=e95i1IHcWj</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">EquivStablePE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">r_ij</span> <span class="o">=</span> <span class="p">((</span><span class="n">PE_i</span> <span class="o">-</span> <span class="n">PE_j</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">r_ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_r_ij</span><span class="p">(</span><span class="n">r_ij</span><span class="p">)</span>  <span class="c1"># the MLP is 1 dim --&gt; hidden_dim --&gt; 1 dim</span>
</span></span><span class="line"><span class="cl">            <span class="n">sigma_ij</span> <span class="o">=</span> <span class="n">sigma_ij</span> <span class="o">*</span> <span class="n">r_ij</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">e</span> <span class="o">=</span> <span class="n">e_ij</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">sigma_ij</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">aggregate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sigma_ij</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">Bx_j</span><span class="p">,</span> <span class="n">Bx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        sigma_ij        : [n_edges, out_dim]  ; is the output from message() function
</span></span></span><span class="line"><span class="cl"><span class="s2">        index           : [n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s2">        </span><span class="si">{}</span><span class="s2">x_j           : [n_edges, out_dim]
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">dim_size</span> <span class="o">=</span> <span class="n">Bx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># or None ??   &lt;--- Double check this</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">sum_sigma_x</span> <span class="o">=</span> <span class="n">sigma_ij</span> <span class="o">*</span> <span class="n">Bx_j</span>
</span></span><span class="line"><span class="cl">        <span class="n">numerator_eta_xj</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">sum_sigma_x</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">reduce</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">sum_sigma</span> <span class="o">=</span> <span class="n">sigma_ij</span>
</span></span><span class="line"><span class="cl">        <span class="n">denominator_eta_xj</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">sum_sigma</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dim_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">reduce</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">numerator_eta_xj</span> <span class="o">/</span> <span class="p">(</span><span class="n">denominator_eta_xj</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aggr_out</span><span class="p">,</span> <span class="n">Ax</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        aggr_out        : [n_nodes, out_dim] ; is the output from aggregate() function after the aggregation
</span></span></span><span class="line"><span class="cl"><span class="s2">        </span><span class="si">{}</span><span class="s2">x             : [n_nodes, out_dim]
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">Ax</span> <span class="o">+</span> <span class="n">aggr_out</span>
</span></span><span class="line"><span class="cl">        <span class="n">e_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">e</span>
</span></span><span class="line"><span class="cl">        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">e</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">e_out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@register_layer</span><span class="p">(</span><span class="s1">&#39;gatedgcnconv&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GatedGCNGraphGymLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;GatedGCN layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Residual Gated Graph ConvNets
</span></span></span><span class="line"><span class="cl"><span class="s2">    https://arxiv.org/pdf/1711.07553.pdf
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_config</span><span class="p">:</span> <span class="n">LayerConfig</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">GatedGCNLayer</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">layer_config</span><span class="o">.</span><span class="n">dim_in</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">out_dim</span><span class="o">=</span><span class="n">layer_config</span><span class="o">.</span><span class="n">dim_out</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">dropout</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>  <span class="c1"># Dropout is handled by GraphGym&#39;s `GeneralLayer` wrapper</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Residual connections are handled by GraphGym&#39;s `GNNStackStage` wrapper</span>
</span></span><span class="line"><span class="cl">                                   <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="实验测试">实验测试
</h3><p>根据同样50个epoch和数据集、split训练后进行测试，计算recovery和scc如下：</p>
<div align=center><img src="picture2.png" width=600/></div>
<center><font size=2>纳尼？情报系假嘚？</font></center>
<p>怎么会比sota指标低这么多，全线的低？我好菜。</p>
<p>于是问了问谷歌哈基米（gemini），也没感觉什么所以然来。可能是因为gpslayer和gatedgnn本来就不是为了rna识别而设计的，所以有一些逻辑可能直接拿来用也不是很ok。</p>
<p>emm，其实multi-scale attention也挺好对不啦？得了，明天汇报完再说吧。怎么论文里全是attention之类的奇奇怪怪的。</p>
<h2 id="transformer-gvp--preln">Transformer GVP + PreLN
</h2><p>在0.0001和0.0002学习率分别利用早停法训练，作者没有用早停法，我们设置200个epoch，每4个epoch进行一次validation，连续20个epoch不更新就停止。</p>
<p>于是，trainers.py需要修改为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">wandb</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">ReduceLROnPlateau</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.evaluator</span> <span class="kn">import</span> <span class="n">evaluate</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.constants</span> <span class="kn">import</span> <span class="n">NUM_TO_LETTER</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">config</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">train_loader</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">val_loader</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">test_loader</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">device</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Train RNA inverse folding model using the specified config and data loaders.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        config (dict): wandb configuration dictionary 
</span></span></span><span class="line"><span class="cl"><span class="s2">        model (nn.Module): RNA inverse folding model to be trained
</span></span></span><span class="line"><span class="cl"><span class="s2">        train_loader (DataLoader): training data loader
</span></span></span><span class="line"><span class="cl"><span class="s2">        val_loader (DataLoader): validation data loader
</span></span></span><span class="line"><span class="cl"><span class="s2">        test_loader (DataLoader): test data loader
</span></span></span><span class="line"><span class="cl"><span class="s2">        device (torch.device): device to train the model on
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise loss function</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">label_smoothing</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">label_smoothing</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">eval_loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise optimizer and scheduler</span>
</span></span><span class="line"><span class="cl">    <span class="n">lr</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">lr</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;xpu&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#=======</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Initialise save directory</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s2">&#34;..&#34;</span><span class="p">,</span> <span class="s2">&#34;mymodel&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#save_dir = os.path.abspath(save_dir)   </span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">#======</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise lookup table mapping integers to nucleotides</span>
</span></span><span class="line"><span class="cl">    <span class="n">lookup</span> <span class="o">=</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">featurizer</span><span class="o">.</span><span class="n">num_to_letter</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialise best checkpoint information</span>
</span></span><span class="line"><span class="cl">    <span class="n">best_epoch</span><span class="p">,</span> <span class="n">best_val_loss</span><span class="p">,</span> <span class="n">best_val_acc</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">patience</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s1">&#39;patience&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">    <span class="n">early_stopping_counter</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">##################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Training loop over mini-batches</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Training iteration</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">train_confusion</span> <span class="o">=</span> <span class="n">loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">train_loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">print_and_log</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">train_confusion</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;train&#34;</span><span class="p">,</span> <span class="n">lookup</span><span class="o">=</span><span class="n">lookup</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">config</span><span class="o">.</span><span class="n">val_every</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">config</span><span class="o">.</span><span class="n">epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="c1"># Evaluate on validation set</span>
</span></span><span class="line"><span class="cl">                <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">val_confusion</span> <span class="o">=</span> <span class="n">loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">eval_loss_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">print_and_log</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">val_confusion</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;val&#34;</span><span class="p">,</span> <span class="n">lookup</span><span class="o">=</span><span class="n">lookup</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># LR scheduler step</span>
</span></span><span class="line"><span class="cl">                <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">val_acc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">lr</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">val_acc</span> <span class="o">&gt;</span> <span class="n">best_val_acc</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># ============== 修改: 早停法逻辑 ==============</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Validation accuracy improved (</span><span class="si">{</span><span class="n">best_val_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> --&gt; </span><span class="si">{</span><span class="n">val_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">). Saving model...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">early_stopping_counter</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 重置计数器</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># ============================================</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># Update best checkpoint</span>
</span></span><span class="line"><span class="cl">                    <span class="n">best_epoch</span><span class="p">,</span> <span class="n">best_val_loss</span><span class="p">,</span> <span class="n">best_val_acc</span> <span class="o">=</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># Evaluate on test set</span>
</span></span><span class="line"><span class="cl">                    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">,</span> <span class="n">test_confusion</span> <span class="o">=</span> <span class="n">loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">eval_loss_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">print_and_log</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">,</span> <span class="n">test_confusion</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;test&#34;</span><span class="p">,</span> <span class="n">lookup</span><span class="o">=</span><span class="n">lookup</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># Update wandb summary metrics</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_epoch&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_epoch</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_val_perp&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">best_val_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_val_acc&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_val_acc</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_test_perp&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_test_acc&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_acc</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                        <span class="c1"># Save best checkpoint</span>
</span></span><span class="line"><span class="cl">                        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s2">&#34;best_checkpoint.h5&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">checkpoint_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">summary</span><span class="p">[</span><span class="s2">&#34;best_checkpoint&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">checkpoint_path</span>
</span></span><span class="line"><span class="cl">                    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># ============== 新增: 早停法逻辑 ==============</span>
</span></span><span class="line"><span class="cl">                        <span class="n">early_stopping_counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Validation accuracy did not improve. EarlyStopping counter: </span><span class="si">{</span><span class="n">early_stopping_counter</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">patience</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">early_stopping_counter</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Early stopping triggered after </span><span class="si">{</span><span class="n">patience</span><span class="si">}</span><span class="s2"> epochs of no improvement.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                            <span class="k">break</span>  <span class="c1"># 跳出训练循环</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># ============================================</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Save current epoch checkpoint</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s2">&#34;current_checkpoint.h5&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># End of training (可能是跑满epochs结束，也可能是被早停法break)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;--- End of Training ---&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">save</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Evaluate best checkpoint</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;EVALUATION: loading </span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s1">&#39;best_checkpoint.h5&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2"> (epoch </span><span class="si">{</span><span class="n">best_epoch</span><span class="si">}</span><span class="s2">)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 确保模型存在，如果训练过早停止可能没有保存过best checkpoint</span>
</span></span><span class="line"><span class="cl">        <span class="n">best_checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="s1">&#39;best_checkpoint.h5&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">best_checkpoint_path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;No best checkpoint was saved. Skipping final evaluation.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Training loop for a single epoch over the data loader.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        model (nn.Module): RNA inverse folding model
</span></span></span><span class="line"><span class="cl"><span class="s2">        dataloader (DataLoader): data loader for the current epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">        loss_fn (nn.Module): loss function to compute the loss
</span></span></span><span class="line"><span class="cl"><span class="s2">        optimizer (torch.optim): optimizer to update model parameters
</span></span></span><span class="line"><span class="cl"><span class="s2">        device (torch.device): device to train the model on
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Note:
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function is used for both training and evaluation loops.
</span></span></span><span class="line"><span class="cl"><span class="s2">        Not passing an optimizer will run the model in evaluation mode.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        float: average loss over the epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">        float: average accuracy over the epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">        np.ndarray: confusion matrix over the epoch
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">confusion</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">model</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">out_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_loss</span><span class="p">,</span> <span class="n">total_correct</span><span class="p">,</span> <span class="n">total_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">t</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">        <span class="c1"># move batch to device</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="s2">&#34;CUDA out of memory&#34;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span> <span class="k">raise</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Skipped batch due to OOM&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="k">del</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>  <span class="c1"># free some memory</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># compute loss</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss_value</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">seq</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">optimizer</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># backpropagate loss and update parameters</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss_value</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># update metrics</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_nodes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">seq</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_loss</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss_value</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="o">*</span> <span class="n">num_nodes</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_count</span> <span class="o">+=</span> <span class="n">num_nodes</span>
</span></span><span class="line"><span class="cl">        <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">true</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">seq</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">pred</span> <span class="o">==</span> <span class="n">true</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">confusion</span> <span class="o">+=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">out_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">t</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&#34;</span><span class="si">%.5f</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_loss</span><span class="o">/</span><span class="n">total_count</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">total_count</span><span class="p">,</span> <span class="n">total_correct</span> <span class="o">/</span> <span class="n">total_count</span><span class="p">,</span> <span class="n">confusion</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">print_and_log</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">epoch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">confusion</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">recovery</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">lr</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">mode</span> <span class="o">=</span> <span class="s2">&#34;train&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">lookup</span> <span class="o">=</span> <span class="n">NUM_TO_LETTER</span><span class="p">,</span> <span class="c1"># reverse of {&#39;A&#39;: 0, &#39;G&#39;: 1, &#39;C&#39;: 2, &#39;U&#39;: 3}</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create log string and wandb metrics dict</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> perp: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> acc: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">wandb_metrics</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">/loss&#34;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">/perp&#34;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">/acc&#34;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;epoch&#34;</span><span class="p">:</span> <span class="n">epoch</span> 
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Add learning rate to loggers</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&#34; lr: </span><span class="si">{</span><span class="n">lr</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb_metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;lr&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">recovery</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Add mean sequence recovery to loggers</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&#34; rec: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recovery</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">wandb_metrics</span><span class="p">[</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">/recovery&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recovery</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">log_str</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">print_confusion</span><span class="p">(</span><span class="n">confusion</span><span class="p">,</span> <span class="n">lookup</span><span class="o">=</span><span class="n">lookup</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">wandb_metrics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">print_confusion</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">lookup</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">counts</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl">    <span class="n">mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">mat</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">res</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lookup</span><span class="o">.</span><span class="n">keys</span><span class="p">())):</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lookup</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Count</span><span class="se">\n</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lookup</span><span class="o">.</span><span class="n">keys</span><span class="p">())):</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="se">\t</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lookup</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">mat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="si">{}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>模型加入了layers.py:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span><span class="lnt">260
</span><span class="lnt">261
</span><span class="lnt">262
</span><span class="lnt">263
</span><span class="lnt">264
</span><span class="lnt">265
</span><span class="lnt">266
</span><span class="lnt">267
</span><span class="lnt">268
</span><span class="lnt">269
</span><span class="lnt">270
</span><span class="lnt">271
</span><span class="lnt">272
</span><span class="lnt">273
</span><span class="lnt">274
</span><span class="lnt">275
</span><span class="lnt">276
</span><span class="lnt">277
</span><span class="lnt">278
</span><span class="lnt">279
</span><span class="lnt">280
</span><span class="lnt">281
</span><span class="lnt">282
</span><span class="lnt">283
</span><span class="lnt">284
</span><span class="lnt">285
</span><span class="lnt">286
</span><span class="lnt">287
</span><span class="lnt">288
</span><span class="lnt">289
</span><span class="lnt">290
</span><span class="lnt">291
</span><span class="lnt">292
</span><span class="lnt">293
</span><span class="lnt">294
</span><span class="lnt">295
</span><span class="lnt">296
</span><span class="lnt">297
</span><span class="lnt">298
</span><span class="lnt">299
</span><span class="lnt">300
</span><span class="lnt">301
</span><span class="lnt">302
</span><span class="lnt">303
</span><span class="lnt">304
</span><span class="lnt">305
</span><span class="lnt">306
</span><span class="lnt">307
</span><span class="lnt">308
</span><span class="lnt">309
</span><span class="lnt">310
</span><span class="lnt">311
</span><span class="lnt">312
</span><span class="lnt">313
</span><span class="lnt">314
</span><span class="lnt">315
</span><span class="lnt">316
</span><span class="lnt">317
</span><span class="lnt">318
</span><span class="lnt">319
</span><span class="lnt">320
</span><span class="lnt">321
</span><span class="lnt">322
</span><span class="lnt">323
</span><span class="lnt">324
</span><span class="lnt">325
</span><span class="lnt">326
</span><span class="lnt">327
</span><span class="lnt">328
</span><span class="lnt">329
</span><span class="lnt">330
</span><span class="lnt">331
</span><span class="lnt">332
</span><span class="lnt">333
</span><span class="lnt">334
</span><span class="lnt">335
</span><span class="lnt">336
</span><span class="lnt">337
</span><span class="lnt">338
</span><span class="lnt">339
</span><span class="lnt">340
</span><span class="lnt">341
</span><span class="lnt">342
</span><span class="lnt">343
</span><span class="lnt">344
</span><span class="lnt">345
</span><span class="lnt">346
</span><span class="lnt">347
</span><span class="lnt">348
</span><span class="lnt">349
</span><span class="lnt">350
</span><span class="lnt">351
</span><span class="lnt">352
</span><span class="lnt">353
</span><span class="lnt">354
</span><span class="lnt">355
</span><span class="lnt">356
</span><span class="lnt">357
</span><span class="lnt">358
</span><span class="lnt">359
</span><span class="lnt">360
</span><span class="lnt">361
</span><span class="lnt">362
</span><span class="lnt">363
</span><span class="lnt">364
</span><span class="lnt">365
</span><span class="lnt">366
</span><span class="lnt">367
</span><span class="lnt">368
</span><span class="lnt">369
</span><span class="lnt">370
</span><span class="lnt">371
</span><span class="lnt">372
</span><span class="lnt">373
</span><span class="lnt">374
</span><span class="lnt">375
</span><span class="lnt">376
</span><span class="lnt">377
</span><span class="lnt">378
</span><span class="lnt">379
</span><span class="lnt">380
</span><span class="lnt">381
</span><span class="lnt">382
</span><span class="lnt">383
</span><span class="lnt">384
</span><span class="lnt">385
</span><span class="lnt">386
</span><span class="lnt">387
</span><span class="lnt">388
</span><span class="lnt">389
</span><span class="lnt">390
</span><span class="lnt">391
</span><span class="lnt">392
</span><span class="lnt">393
</span><span class="lnt">394
</span><span class="lnt">395
</span><span class="lnt">396
</span><span class="lnt">397
</span><span class="lnt">398
</span><span class="lnt">399
</span><span class="lnt">400
</span><span class="lnt">401
</span><span class="lnt">402
</span><span class="lnt">403
</span><span class="lnt">404
</span><span class="lnt">405
</span><span class="lnt">406
</span><span class="lnt">407
</span><span class="lnt">408
</span><span class="lnt">409
</span><span class="lnt">410
</span><span class="lnt">411
</span><span class="lnt">412
</span><span class="lnt">413
</span><span class="lnt">414
</span><span class="lnt">415
</span><span class="lnt">416
</span><span class="lnt">417
</span><span class="lnt">418
</span><span class="lnt">419
</span><span class="lnt">420
</span><span class="lnt">421
</span><span class="lnt">422
</span><span class="lnt">423
</span><span class="lnt">424
</span><span class="lnt">425
</span><span class="lnt">426
</span><span class="lnt">427
</span><span class="lnt">428
</span><span class="lnt">429
</span><span class="lnt">430
</span><span class="lnt">431
</span><span class="lnt">432
</span><span class="lnt">433
</span><span class="lnt">434
</span><span class="lnt">435
</span><span class="lnt">436
</span><span class="lnt">437
</span><span class="lnt">438
</span><span class="lnt">439
</span><span class="lnt">440
</span><span class="lnt">441
</span><span class="lnt">442
</span><span class="lnt">443
</span><span class="lnt">444
</span><span class="lnt">445
</span><span class="lnt">446
</span><span class="lnt">447
</span><span class="lnt">448
</span><span class="lnt">449
</span><span class="lnt">450
</span><span class="lnt">451
</span><span class="lnt">452
</span><span class="lnt">453
</span><span class="lnt">454
</span><span class="lnt">455
</span><span class="lnt">456
</span><span class="lnt">457
</span><span class="lnt">458
</span><span class="lnt">459
</span><span class="lnt">460
</span><span class="lnt">461
</span><span class="lnt">462
</span><span class="lnt">463
</span><span class="lnt">464
</span><span class="lnt">465
</span><span class="lnt">466
</span><span class="lnt">467
</span><span class="lnt">468
</span><span class="lnt">469
</span><span class="lnt">470
</span><span class="lnt">471
</span><span class="lnt">472
</span><span class="lnt">473
</span><span class="lnt">474
</span><span class="lnt">475
</span><span class="lnt">476
</span><span class="lnt">477
</span><span class="lnt">478
</span><span class="lnt">479
</span><span class="lnt">480
</span><span class="lnt">481
</span><span class="lnt">482
</span><span class="lnt">483
</span><span class="lnt">484
</span><span class="lnt">485
</span><span class="lnt">486
</span><span class="lnt">487
</span><span class="lnt">488
</span><span class="lnt">489
</span><span class="lnt">490
</span><span class="lnt">491
</span><span class="lnt">492
</span><span class="lnt">493
</span><span class="lnt">494
</span><span class="lnt">495
</span><span class="lnt">496
</span><span class="lnt">497
</span><span class="lnt">498
</span><span class="lnt">499
</span><span class="lnt">500
</span><span class="lnt">501
</span><span class="lnt">502
</span><span class="lnt">503
</span><span class="lnt">504
</span><span class="lnt">505
</span><span class="lnt">506
</span><span class="lnt">507
</span><span class="lnt">508
</span><span class="lnt">509
</span><span class="lnt">510
</span><span class="lnt">511
</span><span class="lnt">512
</span><span class="lnt">513
</span><span class="lnt">514
</span><span class="lnt">515
</span><span class="lnt">516
</span><span class="lnt">517
</span><span class="lnt">518
</span><span class="lnt">519
</span><span class="lnt">520
</span><span class="lnt">521
</span><span class="lnt">522
</span><span class="lnt">523
</span><span class="lnt">524
</span><span class="lnt">525
</span><span class="lnt">526
</span><span class="lnt">527
</span><span class="lnt">528
</span><span class="lnt">529
</span><span class="lnt">530
</span><span class="lnt">531
</span><span class="lnt">532
</span><span class="lnt">533
</span><span class="lnt">534
</span><span class="lnt">535
</span><span class="lnt">536
</span><span class="lnt">537
</span><span class="lnt">538
</span><span class="lnt">539
</span><span class="lnt">540
</span><span class="lnt">541
</span><span class="lnt">542
</span><span class="lnt">543
</span><span class="lnt">544
</span><span class="lnt">545
</span><span class="lnt">546
</span><span class="lnt">547
</span><span class="lnt">548
</span><span class="lnt">549
</span><span class="lnt">550
</span><span class="lnt">551
</span><span class="lnt">552
</span><span class="lnt">553
</span><span class="lnt">554
</span><span class="lnt">555
</span><span class="lnt">556
</span><span class="lnt">557
</span><span class="lnt">558
</span><span class="lnt">559
</span><span class="lnt">560
</span><span class="lnt">561
</span><span class="lnt">562
</span><span class="lnt">563
</span><span class="lnt">564
</span><span class="lnt">565
</span><span class="lnt">566
</span><span class="lnt">567
</span><span class="lnt">568
</span><span class="lnt">569
</span><span class="lnt">570
</span><span class="lnt">571
</span><span class="lnt">572
</span><span class="lnt">573
</span><span class="lnt">574
</span><span class="lnt">575
</span><span class="lnt">576
</span><span class="lnt">577
</span><span class="lnt">578
</span><span class="lnt">579
</span><span class="lnt">580
</span><span class="lnt">581
</span><span class="lnt">582
</span><span class="lnt">583
</span><span class="lnt">584
</span><span class="lnt">585
</span><span class="lnt">586
</span><span class="lnt">587
</span><span class="lnt">588
</span><span class="lnt">589
</span><span class="lnt">590
</span><span class="lnt">591
</span><span class="lnt">592
</span><span class="lnt">593
</span><span class="lnt">594
</span><span class="lnt">595
</span><span class="lnt">596
</span><span class="lnt">597
</span><span class="lnt">598
</span><span class="lnt">599
</span><span class="lnt">600
</span><span class="lnt">601
</span><span class="lnt">602
</span><span class="lnt">603
</span><span class="lnt">604
</span><span class="lnt">605
</span><span class="lnt">606
</span><span class="lnt">607
</span><span class="lnt">608
</span><span class="lnt">609
</span><span class="lnt">610
</span><span class="lnt">611
</span><span class="lnt">612
</span><span class="lnt">613
</span><span class="lnt">614
</span><span class="lnt">615
</span><span class="lnt">616
</span><span class="lnt">617
</span><span class="lnt">618
</span><span class="lnt">619
</span><span class="lnt">620
</span><span class="lnt">621
</span><span class="lnt">622
</span><span class="lnt">623
</span><span class="lnt">624
</span><span class="lnt">625
</span><span class="lnt">626
</span><span class="lnt">627
</span><span class="lnt">628
</span><span class="lnt">629
</span><span class="lnt">630
</span><span class="lnt">631
</span><span class="lnt">632
</span><span class="lnt">633
</span><span class="lnt">634
</span><span class="lnt">635
</span><span class="lnt">636
</span><span class="lnt">637
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Generalisation of Geometric Vector Perceptron, Jing et al.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># for explicit multi-state biomolecule representation learning.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Original repository: https://github.com/drorlab/gvp-pytorch</span>
</span></span><span class="line"><span class="cl"><span class="c1"># MODIFIED: Added HybridGVPTransformerLayer for deep fusion of</span>
</span></span><span class="line"><span class="cl"><span class="c1"># local geometric message passing and global self-attention,</span>
</span></span><span class="line"><span class="cl"><span class="c1"># with a robust Pre-LN Transformer block structure.</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">functools</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch_geometric</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">MessagePassing</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_scatter</span> <span class="kn">import</span> <span class="n">scatter_add</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 新增: 一个标准的 Position-wise Feed-Forward Network</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; A two-layer Feed-Forward-Network. &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_hid</span><span class="p">,</span> <span class="n">d_in</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">silu</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 新增: 我们设计的核心混合层 (Hybrid Layer) - 优化版</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HybridGVPTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    将GVP-GNN与Graph Transformer思想深度融合的混合层。
</span></span></span><span class="line"><span class="cl"><span class="s1">    遵循标准的Pre-LN Transformer Block结构，以增强训练稳定性。
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">node_dims</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_attn_head</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">n_message</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">drop_rate</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HybridGVPTransformerLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. 几何局部路径：使用原始的MultiGVPConv作为消息传递模块</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gvp_conv</span> <span class="o">=</span> <span class="n">MultiGVPConv</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span> <span class="n">n_message</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_gvp</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. 全局交互路径 (Transformer Block)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 2.1. Attention 子层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># Pre-LN: Attention前的Norm (只作用于标量)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">embed_dim</span><span class="o">=</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_attn_head</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">dropout</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2.2. FFN 子层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm_ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># Pre-LN: FFN前的Norm (只作用于标量)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_in</span><span class="o">=</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d_hid</span><span class="o">=</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`. 
</span></span></span><span class="line"><span class="cl"><span class="s1">                  s shape: (n_nodes, n_conf, d_s), V shape: (n_nodes, n_conf, d_v, 3)
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_index: array of shape [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_attr: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 步骤1: 几何感知的局部消息传递 (GVP Path) ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">dh_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gvp_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_local</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_gvp</span><span class="p">(</span><span class="n">dh_local</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_local</span><span class="p">,</span> <span class="n">v_local</span> <span class="o">=</span> <span class="n">x_local</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 步骤2: 标准的Transformer Block (作用于标量特征) ---</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2.1 Attention Sub-layer with Pre-LN and Residual Connection</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_res_attn</span> <span class="o">=</span> <span class="n">s_local</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_norm_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_attn</span><span class="p">(</span><span class="n">s_local</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">n_nodes</span><span class="p">,</span> <span class="n">n_conf</span><span class="p">,</span> <span class="n">d_s</span> <span class="o">=</span> <span class="n">s_norm_attn</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_reshaped</span> <span class="o">=</span> <span class="n">s_norm_attn</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># -&gt; (n_conf, n_nodes, d_s)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">s_attn_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">s_reshaped</span><span class="p">,</span> <span class="n">s_reshaped</span><span class="p">,</span> <span class="n">s_reshaped</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">s_attn_out</span> <span class="o">=</span> <span class="n">s_attn_out</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># -&gt; (n_nodes, n_conf, d_s)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_after_attn</span> <span class="o">=</span> <span class="n">s_res_attn</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_attn</span><span class="p">(</span><span class="n">s_attn_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 2.2 Feed-Forward Sub-layer with Pre-LN and Residual Connection</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_res_ffn</span> <span class="o">=</span> <span class="n">s_after_attn</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_norm_ffn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ffn</span><span class="p">(</span><span class="n">s_after_attn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_ffn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">s_norm_ffn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_final</span> <span class="o">=</span> <span class="n">s_res_ffn</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ffn</span><span class="p">(</span><span class="n">s_ffn_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 步骤3: 最终输出 ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 矢量特征直接继承自局部路径</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_final</span> <span class="o">=</span> <span class="n">v_local</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">s_final</span><span class="p">,</span> <span class="n">v_final</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GVPConvLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Full graph convolution / message passing layer with 
</span></span></span><span class="line"><span class="cl"><span class="s1">    Geometric Vector Perceptrons. Residually updates node embeddings with
</span></span></span><span class="line"><span class="cl"><span class="s1">    aggregated incoming messages, applies a pointwise feedforward 
</span></span></span><span class="line"><span class="cl"><span class="s1">    network to node embeddings, and returns updated node embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    To only compute the aggregated messages, see `GVPConv`.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param node_dims: node embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param n_message: number of GVPs to use in message function
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param n_feedforward: number of GVPs to use in feedforward function
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param drop_rate: drop probability in all dropout layers
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param autoregressive: if `True`, this `GVPConvLayer` will be used
</span></span></span><span class="line"><span class="cl"><span class="s1">           with a different set of input node embeddings for messages
</span></span></span><span class="line"><span class="cl"><span class="s1">           where src &gt;= dst
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param vector_gate: whether to use vector gating.
</span></span></span><span class="line"><span class="cl"><span class="s1">                        (vector_act will be used as sigma^+ in vector gating if `True`)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">node_dims</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">n_message</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">n_feedforward</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">drop_rate</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">autoregressive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">norm_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">GVPConvLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">GVPConv</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span> <span class="n">n_message</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;add&#34;</span> <span class="k">if</span> <span class="n">autoregressive</span> <span class="k">else</span> <span class="s2">&#34;mean&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">GVP_</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">node_dims</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">ff_func</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">n_feedforward</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">hid_dims</span> <span class="o">=</span> <span class="mi">4</span><span class="o">*</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="o">*</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_feedforward</span><span class="o">-</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">ff_func</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span> <span class="o">=</span> <span class="n">norm_first</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">autoregressive_x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">node_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_index: array of shape [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_attr: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param autoregressive_x: tuple (s, V) of `torch.Tensor`. 
</span></span></span><span class="line"><span class="cl"><span class="s1">                If not `None`, will be used as src node embeddings
</span></span></span><span class="line"><span class="cl"><span class="s1">                for forming messages where src &gt;= dst. The current node 
</span></span></span><span class="line"><span class="cl"><span class="s1">                embeddings `x` will still be the base of the update and the 
</span></span></span><span class="line"><span class="cl"><span class="s1">                pointwise feedforward.
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param node_mask: array of type `bool` to index into the first
</span></span></span><span class="line"><span class="cl"><span class="s1">                dim of node embeddings (s, V). If not `None`, only
</span></span></span><span class="line"><span class="cl"><span class="s1">                these nodes will be updated.
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">autoregressive_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">src</span><span class="p">,</span> <span class="n">dst</span> <span class="o">=</span> <span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">            <span class="n">mask</span> <span class="o">=</span> <span class="n">src</span> <span class="o">&lt;</span> <span class="n">dst</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_index_forward</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[:,</span> <span class="n">mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_index_backward</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[:,</span> <span class="o">~</span><span class="n">mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_attr_forward</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">edge_attr</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_attr_backward</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">edge_attr</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index_forward</span><span class="p">,</span> <span class="n">edge_attr_forward</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">autoregressive_x</span><span class="p">,</span> <span class="n">edge_index_backward</span><span class="p">,</span> <span class="n">edge_attr_backward</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">count</span> <span class="o">=</span> <span class="n">scatter_add</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">dst</span><span class="p">),</span> <span class="n">dst</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">dim_size</span><span class="o">=</span><span class="n">dh</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="n">dh</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">count</span><span class="p">,</span> <span class="n">dh</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">count</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">),</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">node_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x_</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">,</span> <span class="n">dh</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">node_mask</span><span class="p">),</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">dh</span><span class="p">,</span> <span class="n">node_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">dh</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">dh</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">dh</span><span class="p">)))</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="k">else</span> <span class="n">dh</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">dh</span><span class="p">)))</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="k">else</span> <span class="n">dh</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">node_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">node_mask</span><span class="p">],</span> <span class="n">x_</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">node_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x_</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GVPConv</span><span class="p">(</span><span class="n">MessagePassing</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Graph convolution / message passing with Geometric Vector Perceptrons.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes in a graph with node and edge embeddings,
</span></span></span><span class="line"><span class="cl"><span class="s1">    and returns new node embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    This does NOT do residual updates and pointwise feedforward layers
</span></span></span><span class="line"><span class="cl"><span class="s1">    ---see `GVPConvLayer`.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param in_dims: input node embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param out_dims: output node embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param n_layers: number of GVPs in the message function
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param module_list: preconstructed message function, overrides n_layers
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param aggr: should be &#34;add&#34; if some incoming edges are masked, as in
</span></span></span><span class="line"><span class="cl"><span class="s1">                 a masked autoregressive decoder architecture, otherwise &#34;mean&#34;
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param vector_gate: whether to use vector gating.
</span></span></span><span class="line"><span class="cl"><span class="s1">                        (vector_act will be used as sigma^+ in vector gating if `True`)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">module_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                 <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">GVPConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="n">aggr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">=</span> <span class="n">in_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span> <span class="o">=</span> <span class="n">out_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span> <span class="o">=</span> <span class="n">edge_dims</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">GVP_</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">module_list</span> <span class="o">=</span> <span class="n">module_list</span> <span class="ow">or</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">module_list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">GVP_</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">GVP_</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span><span class="p">),</span> <span class="n">out_dims</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                       <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">message_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">module_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_index: array of shape [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_attr: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_s</span><span class="p">,</span> <span class="n">x_v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">s</span><span class="o">=</span><span class="n">x_s</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">x_v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">edge_attr</span><span class="o">=</span><span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_split</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">v_i</span><span class="p">,</span> <span class="n">s_j</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_j</span> <span class="o">=</span> <span class="n">v_j</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_i</span> <span class="o">=</span> <span class="n">v_i</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="n">tuple_cat</span><span class="p">((</span><span class="n">s_j</span><span class="p">,</span> <span class="n">v_j</span><span class="p">),</span> <span class="n">edge_attr</span><span class="p">,</span> <span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">v_i</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">message_func</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_merge</span><span class="p">(</span><span class="o">*</span><span class="n">message</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MultiGVPConvLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    GVPConvLayer for handling multiple conformations (encoder-only)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">node_dims</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">n_message</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">n_feedforward</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">drop_rate</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">norm_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">MultiGVPConvLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">MultiGVPConv</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span> <span class="n">n_message</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">GVP_</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">node_dims</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">ff_func</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">n_feedforward</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">hid_dims</span> <span class="o">=</span> <span class="mi">4</span><span class="o">*</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="o">*</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_feedforward</span><span class="o">-</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">ff_func</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span> <span class="o">=</span> <span class="n">norm_first</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_index: array of shape [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_attr: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">),</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">dh</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">dh</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">dh</span><span class="p">)))</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="k">else</span> <span class="n">dh</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">dh</span><span class="p">)))</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="k">else</span> <span class="n">dh</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MultiGVPConv</span><span class="p">(</span><span class="n">MessagePassing</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    GVPConv for handling multiple conformations
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">module_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                 <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">MultiGVPConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="n">aggr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">=</span> <span class="n">in_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span> <span class="o">=</span> <span class="n">out_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span> <span class="o">=</span> <span class="n">edge_dims</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">GVP_</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">module_list</span> <span class="o">=</span> <span class="n">module_list</span> <span class="ow">or</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">module_list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">GVP_</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">GVP_</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span><span class="p">),</span> <span class="n">out_dims</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                       <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">message_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">module_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_index: array of shape [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_attr: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_s</span><span class="p">,</span> <span class="n">x_v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_conf</span> <span class="o">=</span> <span class="n">x_s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># x_s: [n_nodes, n_conf, d] -&gt; [n_nodes, n_conf * d]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_s</span> <span class="o">=</span> <span class="n">x_s</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x_s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>        
</span></span><span class="line"><span class="cl">        <span class="c1"># x_v: [n_nodes, n_conf, d, 3] -&gt; [n_nodes, n_conf * d * 3]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_v</span> <span class="o">=</span> <span class="n">x_v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">x_s</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">x_v</span><span class="p">,</span> <span class="n">edge_attr</span><span class="o">=</span><span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_split_multi</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">,</span> <span class="n">n_conf</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">v_i</span><span class="p">,</span> <span class="n">s_j</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [n_nodes, n_conf * d] -&gt; [n_nodes, n_conf, d]</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_i</span> <span class="o">=</span> <span class="n">s_i</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_j</span> <span class="o">=</span> <span class="n">s_j</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [n_nodes, n_conf * d * 3] -&gt; [n_nodes, n_conf, d, 3]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_i</span> <span class="o">=</span> <span class="n">v_i</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">*</span> <span class="mi">3</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_j</span> <span class="o">=</span> <span class="n">v_j</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">*</span> <span class="mi">3</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="n">tuple_cat</span><span class="p">((</span><span class="n">s_j</span><span class="p">,</span> <span class="n">v_j</span><span class="p">),</span> <span class="n">edge_attr</span><span class="p">,</span> <span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">v_i</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">message_func</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_merge_multi</span><span class="p">(</span><span class="o">*</span><span class="n">message</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GVP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Geometric Vector Perceptron. See manuscript and README.md
</span></span></span><span class="line"><span class="cl"><span class="s1">    for more details.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param in_dims: tuple (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param out_dims: tuple (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param h_dim: intermediate number of vector channels, optional
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param activations: tuple of functions (scalar_act, vector_act)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param vector_gate: whether to use vector gating.
</span></span></span><span class="line"><span class="cl"><span class="s1">                        (vector_act will be used as sigma^+ in vector gating if `True`)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">h_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">=</span> <span class="n">in_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span> <span class="o">=</span> <span class="n">out_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">vector_gate</span> <span class="o">=</span> <span class="n">vector_gate</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span> <span class="o">=</span> <span class="n">h_dim</span> <span class="ow">or</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ws</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_gate</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">wsv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ws</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">scalar_act</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span> <span class="o">=</span> <span class="n">activations</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dummy_param</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`, 
</span></span></span><span class="line"><span class="cl"><span class="s1">                  or (if vectors_in is 0), a single `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :return: tuple (s, V) of `torch.Tensor`,
</span></span></span><span class="line"><span class="cl"><span class="s1">                 or (if vectors_out is 0), a single `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">vh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wh</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>    
</span></span><span class="line"><span class="cl">            <span class="n">vn</span> <span class="o">=</span> <span class="n">_norm_no_nan</span><span class="p">(</span><span class="n">vh</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">vn</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">                <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">vh</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">                <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_gate</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                        <span class="n">gate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wsv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                        <span class="n">gate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wsv</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gate</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="n">_norm_no_nan</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dummy_param</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar_act</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar_act</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span> <span class="k">else</span> <span class="n">s</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">_VDropout</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Vector channel dropout where the elements of each
</span></span></span><span class="line"><span class="cl"><span class="s1">    vector channel are dropped together.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">_VDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">drop_rate</span> <span class="o">=</span> <span class="n">drop_rate</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dummy_param</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: `torch.Tensor` corresponding to vector channels
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dummy_param</span><span class="o">.</span><span class="n">device</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Combined dropout for tuples (s, V).
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes tuples (s, V) as input and as output.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">Dropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">sdropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">vdropout</span> <span class="o">=</span> <span class="n">_VDropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`,
</span></span></span><span class="line"><span class="cl"><span class="s1">                  or single `torch.Tensor` 
</span></span></span><span class="line"><span class="cl"><span class="s1">                  (will be assumed to be scalar channels)
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sdropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sdropout</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vdropout</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Combined LayerNorm for tuples (s, V).
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes tuples (s, V) as input and as output.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dims</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">scalar_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`,
</span></span></span><span class="line"><span class="cl"><span class="s1">                  or single `torch.Tensor` 
</span></span></span><span class="line"><span class="cl"><span class="s1">                  (will be assumed to be scalar channels)
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">vn</span> <span class="o">=</span> <span class="n">_norm_no_nan</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sqrt</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">vn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">vn</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar_norm</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">v</span> <span class="o">/</span> <span class="n">vn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tuple_sum</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Sums any number of tuples (s, V) elementwise.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">sum</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tuple_cat</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Concatenates any number of tuples (s, V) elementwise.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param dim: dimension along which to concatenate when viewed
</span></span></span><span class="line"><span class="cl"><span class="s1">                as the `dim` index for the scalar-channel tensors.
</span></span></span><span class="line"><span class="cl"><span class="s1">                This means that `dim=-1` will be applied as
</span></span></span><span class="line"><span class="cl"><span class="s1">                `dim=-2` for the vector-channel tensors.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dim</span> <span class="o">%=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">s_args</span><span class="p">,</span> <span class="n">v_args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">s_args</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">v_args</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tuple_index</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Indexes into a tuple (s, V) along the first dimension.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param idx: any object which can be used to index into a `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&#34;cpu&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Returns random tuples (s, V) drawn elementwise from a normal distribution.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param n: number of data points
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param dims: tuple of dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :return: (s, V) with s.shape = (n, n_scalar) and
</span></span></span><span class="line"><span class="cl"><span class="s1">             V.shape = (n, n_vector, 3)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> \
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_norm_no_nan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">sqrt</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    L2 norm of tensor clamped above a minimum value `eps`.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param sqrt: if `False`, returns the square of the L2 norm
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="k">if</span> <span class="n">sqrt</span> <span class="k">else</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nv</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Splits a merged representation of (s, V) back into a tuple. 
</span></span></span><span class="line"><span class="cl"><span class="s1">    Should be used only with `_merge(s, V)` and only if the tuple 
</span></span></span><span class="line"><span class="cl"><span class="s1">    representation cannot be used.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param x: the `torch.Tensor` returned from `_merge`
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param nv: the number of vector channels in the input to `_merge`
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">s</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">nv</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">nv</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nv</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_merge</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Merges a tuple (s, V) into a single `torch.Tensor`, where the
</span></span></span><span class="line"><span class="cl"><span class="s1">    vector channels are flattened and appended to the scalar channels.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Should be used only if the tuple representation cannot be used.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Use `_split(x, nv)` to reverse.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_split_multi</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">nv</span><span class="p">,</span> <span class="n">n_conf</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    _split for multiple conformers
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">s</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">nv</span> <span class="o">*</span> <span class="n">n_conf</span><span class="p">]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_conf</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">nv</span> <span class="o">*</span> <span class="n">n_conf</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_conf</span><span class="p">,</span> <span class="n">nv</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_merge_multi</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    _merge for multiple conformers
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># s: [n_nodes, n_conf, d] -&gt; [n_nodes, n_conf * d]</span>
</span></span><span class="line"><span class="cl">    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># v: [n_nodes, n_conf, d, 3] -&gt; [n_nodes, n_conf * d * 3]</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后更新models.py，如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span><span class="lnt">260
</span><span class="lnt">261
</span><span class="lnt">262
</span><span class="lnt">263
</span><span class="lnt">264
</span><span class="lnt">265
</span><span class="lnt">266
</span><span class="lnt">267
</span><span class="lnt">268
</span><span class="lnt">269
</span><span class="lnt">270
</span><span class="lnt">271
</span><span class="lnt">272
</span><span class="lnt">273
</span><span class="lnt">274
</span><span class="lnt">275
</span><span class="lnt">276
</span><span class="lnt">277
</span><span class="lnt">278
</span><span class="lnt">279
</span><span class="lnt">280
</span><span class="lnt">281
</span><span class="lnt">282
</span><span class="lnt">283
</span><span class="lnt">284
</span><span class="lnt">285
</span><span class="lnt">286
</span><span class="lnt">287
</span><span class="lnt">288
</span><span class="lnt">289
</span><span class="lnt">290
</span><span class="lnt">291
</span><span class="lnt">292
</span><span class="lnt">293
</span><span class="lnt">294
</span><span class="lnt">295
</span><span class="lnt">296
</span><span class="lnt">297
</span><span class="lnt">298
</span><span class="lnt">299
</span><span class="lnt">300
</span><span class="lnt">301
</span><span class="lnt">302
</span><span class="lnt">303
</span><span class="lnt">304
</span><span class="lnt">305
</span><span class="lnt">306
</span><span class="lnt">307
</span><span class="lnt">308
</span><span class="lnt">309
</span><span class="lnt">310
</span><span class="lnt">311
</span><span class="lnt">312
</span><span class="lnt">313
</span><span class="lnt">314
</span><span class="lnt">315
</span><span class="lnt">316
</span><span class="lnt">317
</span><span class="lnt">318
</span><span class="lnt">319
</span><span class="lnt">320
</span><span class="lnt">321
</span><span class="lnt">322
</span><span class="lnt">323
</span><span class="lnt">324
</span><span class="lnt">325
</span><span class="lnt">326
</span><span class="lnt">327
</span><span class="lnt">328
</span><span class="lnt">329
</span><span class="lnt">330
</span><span class="lnt">331
</span><span class="lnt">332
</span><span class="lnt">333
</span><span class="lnt">334
</span><span class="lnt">335
</span><span class="lnt">336
</span><span class="lnt">337
</span><span class="lnt">338
</span><span class="lnt">339
</span><span class="lnt">340
</span><span class="lnt">341
</span><span class="lnt">342
</span><span class="lnt">343
</span><span class="lnt">344
</span><span class="lnt">345
</span><span class="lnt">346
</span><span class="lnt">347
</span><span class="lnt">348
</span><span class="lnt">349
</span><span class="lnt">350
</span><span class="lnt">351
</span><span class="lnt">352
</span><span class="lnt">353
</span><span class="lnt">354
</span><span class="lnt">355
</span><span class="lnt">356
</span><span class="lnt">357
</span><span class="lnt">358
</span><span class="lnt">359
</span><span class="lnt">360
</span><span class="lnt">361
</span><span class="lnt">362
</span><span class="lnt">363
</span><span class="lnt">364
</span><span class="lnt">365
</span><span class="lnt">366
</span><span class="lnt">367
</span><span class="lnt">368
</span><span class="lnt">369
</span><span class="lnt">370
</span><span class="lnt">371
</span><span class="lnt">372
</span><span class="lnt">373
</span><span class="lnt">374
</span><span class="lnt">375
</span><span class="lnt">376
</span><span class="lnt">377
</span><span class="lnt">378
</span><span class="lnt">379
</span><span class="lnt">380
</span><span class="lnt">381
</span><span class="lnt">382
</span><span class="lnt">383
</span><span class="lnt">384
</span><span class="lnt">385
</span><span class="lnt">386
</span><span class="lnt">387
</span><span class="lnt">388
</span><span class="lnt">389
</span><span class="lnt">390
</span><span class="lnt">391
</span><span class="lnt">392
</span><span class="lnt">393
</span><span class="lnt">394
</span><span class="lnt">395
</span><span class="lnt">396
</span><span class="lnt">397
</span><span class="lnt">398
</span><span class="lnt">399
</span><span class="lnt">400
</span><span class="lnt">401
</span><span class="lnt">402
</span><span class="lnt">403
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Generalisation of Geometric Vector Perceptron, Jing et al.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># for explicit multi-state biomolecule representation learning.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Original repository: https://github.com/drorlab/gvp-pytorch</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch_geometric</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.layers</span> <span class="kn">import</span> <span class="o">*</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AutoregressiveMultiGNNv1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Autoregressive GVP-GNN for **multiple** structure-conditioned RNA design.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes in RNA structure graphs of type `torch_geometric.data.Data` 
</span></span></span><span class="line"><span class="cl"><span class="s1">    or `torch_geometric.data.Batch` and returns a categorical distribution
</span></span></span><span class="line"><span class="cl"><span class="s1">    over 4 bases at each position in a `torch.Tensor` of shape [n_nodes, 4].
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    The standard forward pass requires sequence information as input
</span></span></span><span class="line"><span class="cl"><span class="s1">    and should be used for training or evaluating likelihood.
</span></span></span><span class="line"><span class="cl"><span class="s1">    For sampling or design, use `self.sample`.
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_in_dim (tuple): node dimensions in input graph
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_h_dim (tuple): node dimensions to use in GVP-GNN layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_in_dim (tuple): edge dimensions in input graph
</span></span></span><span class="line"><span class="cl"><span class="s1">        edge_h_dim (tuple): edge dimensions to embed in GVP-GNN layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        num_layers (int): number of GVP-GNN layers in encoder/decoder
</span></span></span><span class="line"><span class="cl"><span class="s1">        drop_rate (float): rate to use in all dropout layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        out_dim (int): output dimension (4 bases)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">node_in_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">node_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">edge_in_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">edge_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">drop_rate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">out_dim</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="c1">######### 新增: 注意力头数</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_attn_head</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span> <span class="o">=</span> <span class="n">node_in_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span> <span class="o">=</span> <span class="n">node_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span> <span class="o">=</span> <span class="n">edge_in_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span> <span class="o">=</span> <span class="n">edge_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
</span></span><span class="line"><span class="cl">        <span class="n">activations</span> <span class="o">=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Node input embedding</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Edge input embedding</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Encoder layers (supports multiple conformations)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 修改: 将 MultiGVPConvLayer 替换为我们设计的新混合层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">HybridGVPTransformerLayer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">node_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">edge_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">num_attn_head</span><span class="o">=</span><span class="n">num_attn_head</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">drop_rate</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Decoder layers</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_s</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">GVPConvLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                             <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                             <span class="n">drop_rate</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">,</span> <span class="n">autoregressive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">norm_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Output</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">node_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">node_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">seq</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span><span class="p">(</span><span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Pool multi-conformation features: </span>
</span></span><span class="line"><span class="cl">        <span class="c1"># nodes: (n_nodes, d_s), (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># edges: (n_edges, d_se), (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_multi_conf</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">mask_confs</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">encoder_embeddings</span> <span class="o">=</span> <span class="n">h_V</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">h_S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_s</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_S</span> <span class="o">=</span> <span class="n">h_S</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_S</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">h_S</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">autoregressive_x</span> <span class="o">=</span> <span class="n">encoder_embeddings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">logits</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nd">@torch.no_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">batch</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">n_samples</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">return_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        Samples sequences autoregressively from the distribution
</span></span></span><span class="line"><span class="cl"><span class="s1">        learned by the model.
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">        Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">            batch (torch_geometric.data.Data): mini-batch containing one
</span></span></span><span class="line"><span class="cl"><span class="s1">                RNA backbone to design sequences for
</span></span></span><span class="line"><span class="cl"><span class="s1">            n_samples (int): number of samples
</span></span></span><span class="line"><span class="cl"><span class="s1">            temperature (float): temperature to use in softmax over 
</span></span></span><span class="line"><span class="cl"><span class="s1">                the categorical distribution
</span></span></span><span class="line"><span class="cl"><span class="s1">            logit_bias (torch.Tensor): bias to add to logits during sampling
</span></span></span><span class="line"><span class="cl"><span class="s1">                to manually fix or control nucleotides in designed sequences,
</span></span></span><span class="line"><span class="cl"><span class="s1">                of shape [n_nodes, 4]
</span></span></span><span class="line"><span class="cl"><span class="s1">            return_logits (bool): whether to return logits or not
</span></span></span><span class="line"><span class="cl"><span class="s1">        
</span></span></span><span class="line"><span class="cl"><span class="s1">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s1">            seq (torch.Tensor): int tensor of shape [n_samples, n_nodes]
</span></span></span><span class="line"><span class="cl"><span class="s1">                                based on the residue-to-int mapping of
</span></span></span><span class="line"><span class="cl"><span class="s1">                                the original training data
</span></span></span><span class="line"><span class="cl"><span class="s1">            logits (torch.Tensor): logits of shape [n_samples, n_nodes, 4]
</span></span></span><span class="line"><span class="cl"><span class="s1">                                   (only if return_logits is True)
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span> 
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">node_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">node_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">edge_index</span><span class="o">.</span><span class="n">device</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span><span class="p">(</span><span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Pool multi-conformation features</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># nodes: (n_nodes, d_s), (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># edges: (n_edges, d_se), (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_multi_conf</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">mask_confs</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Repeat features for sampling n_samples times</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Expand edge index for autoregressive decoding</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">edge_index</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">offset</span> <span class="o">=</span> <span class="n">num_nodes</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">edge_index</span> <span class="o">+</span> <span class="n">offset</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This is akin to &#39;batching&#39; (in PyG style) n_samples copies of the graph</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h_V_cache</span> <span class="o">=</span> <span class="p">[(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Decode one token at a time</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">h_S_</span> <span class="o">=</span> <span class="n">h_S</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_S_</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E_</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">h_S_</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">            <span class="n">edge_mask</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">num_nodes</span> <span class="o">==</span> <span class="n">i</span>  <span class="c1"># True for all edges where dst is node i</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_index_</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[:,</span> <span class="n">edge_mask</span><span class="p">]</span>  <span class="c1"># subset all incoming edges to node i</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E_</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">h_E_</span><span class="p">,</span> <span class="n">edge_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">node_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">node_mask</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># True for all nodes i and its repeats</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">out</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V_cache</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">edge_index_</span><span class="p">,</span> <span class="n">h_E_</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">autoregressive_x</span><span class="o">=</span><span class="n">h_V_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">node_mask</span><span class="o">=</span><span class="n">node_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="n">out</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">node_mask</span><span class="p">)</span>  <span class="c1"># subset out to only node i and its repeats</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">h_V_cache</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="n">h_V_cache</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">            <span class="n">lgts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Add logit bias if provided to fix or bias positions</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">logit_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">lgts</span> <span class="o">+=</span> <span class="n">logit_bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Sample from logits</span>
</span></span><span class="line"><span class="cl">            <span class="n">seq</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">lgts</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_S</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_s</span><span class="p">(</span><span class="n">seq</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">lgts</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">return_logits</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">seq</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">),</span> <span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>    
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">seq</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">pool_multi_conf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">mask_confs</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Number of conformations is 1, no need to pool</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># True num_conf for masked mean pooling</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_conf_true</span> <span class="o">=</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (n_nodes, 1)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask scalar features</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V0</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E0</span> <span class="o">=</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask vector features</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, 1, 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V1</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E1</span> <span class="o">=</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Average pooling multi-conformation features</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">,</span>               <span class="c1"># (n_nodes, d_s)</span>
</span></span><span class="line"><span class="cl">               <span class="n">h_V1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_E0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>               <span class="c1"># (n_edges, d_se)</span>
</span></span><span class="line"><span class="cl">               <span class="n">h_E1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">NonAutoregressiveMultiGNNv1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Non-Autoregressive GVP-GNN for **multiple** structure-conditioned RNA design.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes in RNA structure graphs of type `torch_geometric.data.Data` 
</span></span></span><span class="line"><span class="cl"><span class="s1">    or `torch_geometric.data.Batch` and returns a categorical distribution
</span></span></span><span class="line"><span class="cl"><span class="s1">    over 4 bases at each position in a `torch.Tensor` of shape [n_nodes, 4].
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    The standard forward pass requires sequence information as input
</span></span></span><span class="line"><span class="cl"><span class="s1">    and should be used for training or evaluating likelihood.
</span></span></span><span class="line"><span class="cl"><span class="s1">    For sampling or design, use `self.sample`.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_in_dim (tuple): node dimensions in input graph
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_h_dim (tuple): node dimensions to use in GVP-GNN layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_in_dim (tuple): edge dimensions in input graph
</span></span></span><span class="line"><span class="cl"><span class="s1">        edge_h_dim (tuple): edge dimensions to embed in GVP-GNN layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        num_layers (int): number of GVP-GNN layers in encoder/decoder
</span></span></span><span class="line"><span class="cl"><span class="s1">        drop_rate (float): rate to use in all dropout layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        out_dim (int): output dimension (4 bases)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">node_in_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">node_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">edge_in_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">edge_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">drop_rate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">out_dim</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span> <span class="o">=</span> <span class="n">node_in_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span> <span class="o">=</span> <span class="n">node_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span> <span class="o">=</span> <span class="n">edge_in_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span> <span class="o">=</span> <span class="n">edge_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
</span></span><span class="line"><span class="cl">        <span class="n">activations</span> <span class="o">=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Node input embedding</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Edge input embedding</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Encoder layers (supports multiple conformations)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">MultiGVPConvLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                  <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                  <span class="n">drop_rate</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">,</span> <span class="n">norm_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Output</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>   
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">node_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">node_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span><span class="p">(</span><span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Pool multi-conformation features: </span>
</span></span><span class="line"><span class="cl">        <span class="c1"># nodes: (n_nodes, d_s), (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># edges: (n_edges, d_se), (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, out_dim)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">logits</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">return_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">node_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">node_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_index</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span><span class="p">(</span><span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># Pool multi-conformation features</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index)</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, out_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_samples)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">return_logits</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">seq</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">logits</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">seq</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">pool_multi_conf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">mask_confs</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Number of conformations is 1, no need to pool</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># True num_conf for masked mean pooling</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_conf_true</span> <span class="o">=</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (n_nodes, 1)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask scalar features</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V0</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E0</span> <span class="o">=</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask vector features</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, 1, 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V1</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E1</span> <span class="o">=</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Average pooling multi-conformation features</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">,</span>               <span class="c1"># (n_nodes, d_s)</span>
</span></span><span class="line"><span class="cl">               <span class="n">h_V1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_E0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>               <span class="c1"># (n_edges, d_se)</span>
</span></span><span class="line"><span class="cl">               <span class="n">h_E1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="long-short-transformer-gvp">Long-short Transformer GVP
</h2><p>同样，layers.py:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span><span class="lnt">260
</span><span class="lnt">261
</span><span class="lnt">262
</span><span class="lnt">263
</span><span class="lnt">264
</span><span class="lnt">265
</span><span class="lnt">266
</span><span class="lnt">267
</span><span class="lnt">268
</span><span class="lnt">269
</span><span class="lnt">270
</span><span class="lnt">271
</span><span class="lnt">272
</span><span class="lnt">273
</span><span class="lnt">274
</span><span class="lnt">275
</span><span class="lnt">276
</span><span class="lnt">277
</span><span class="lnt">278
</span><span class="lnt">279
</span><span class="lnt">280
</span><span class="lnt">281
</span><span class="lnt">282
</span><span class="lnt">283
</span><span class="lnt">284
</span><span class="lnt">285
</span><span class="lnt">286
</span><span class="lnt">287
</span><span class="lnt">288
</span><span class="lnt">289
</span><span class="lnt">290
</span><span class="lnt">291
</span><span class="lnt">292
</span><span class="lnt">293
</span><span class="lnt">294
</span><span class="lnt">295
</span><span class="lnt">296
</span><span class="lnt">297
</span><span class="lnt">298
</span><span class="lnt">299
</span><span class="lnt">300
</span><span class="lnt">301
</span><span class="lnt">302
</span><span class="lnt">303
</span><span class="lnt">304
</span><span class="lnt">305
</span><span class="lnt">306
</span><span class="lnt">307
</span><span class="lnt">308
</span><span class="lnt">309
</span><span class="lnt">310
</span><span class="lnt">311
</span><span class="lnt">312
</span><span class="lnt">313
</span><span class="lnt">314
</span><span class="lnt">315
</span><span class="lnt">316
</span><span class="lnt">317
</span><span class="lnt">318
</span><span class="lnt">319
</span><span class="lnt">320
</span><span class="lnt">321
</span><span class="lnt">322
</span><span class="lnt">323
</span><span class="lnt">324
</span><span class="lnt">325
</span><span class="lnt">326
</span><span class="lnt">327
</span><span class="lnt">328
</span><span class="lnt">329
</span><span class="lnt">330
</span><span class="lnt">331
</span><span class="lnt">332
</span><span class="lnt">333
</span><span class="lnt">334
</span><span class="lnt">335
</span><span class="lnt">336
</span><span class="lnt">337
</span><span class="lnt">338
</span><span class="lnt">339
</span><span class="lnt">340
</span><span class="lnt">341
</span><span class="lnt">342
</span><span class="lnt">343
</span><span class="lnt">344
</span><span class="lnt">345
</span><span class="lnt">346
</span><span class="lnt">347
</span><span class="lnt">348
</span><span class="lnt">349
</span><span class="lnt">350
</span><span class="lnt">351
</span><span class="lnt">352
</span><span class="lnt">353
</span><span class="lnt">354
</span><span class="lnt">355
</span><span class="lnt">356
</span><span class="lnt">357
</span><span class="lnt">358
</span><span class="lnt">359
</span><span class="lnt">360
</span><span class="lnt">361
</span><span class="lnt">362
</span><span class="lnt">363
</span><span class="lnt">364
</span><span class="lnt">365
</span><span class="lnt">366
</span><span class="lnt">367
</span><span class="lnt">368
</span><span class="lnt">369
</span><span class="lnt">370
</span><span class="lnt">371
</span><span class="lnt">372
</span><span class="lnt">373
</span><span class="lnt">374
</span><span class="lnt">375
</span><span class="lnt">376
</span><span class="lnt">377
</span><span class="lnt">378
</span><span class="lnt">379
</span><span class="lnt">380
</span><span class="lnt">381
</span><span class="lnt">382
</span><span class="lnt">383
</span><span class="lnt">384
</span><span class="lnt">385
</span><span class="lnt">386
</span><span class="lnt">387
</span><span class="lnt">388
</span><span class="lnt">389
</span><span class="lnt">390
</span><span class="lnt">391
</span><span class="lnt">392
</span><span class="lnt">393
</span><span class="lnt">394
</span><span class="lnt">395
</span><span class="lnt">396
</span><span class="lnt">397
</span><span class="lnt">398
</span><span class="lnt">399
</span><span class="lnt">400
</span><span class="lnt">401
</span><span class="lnt">402
</span><span class="lnt">403
</span><span class="lnt">404
</span><span class="lnt">405
</span><span class="lnt">406
</span><span class="lnt">407
</span><span class="lnt">408
</span><span class="lnt">409
</span><span class="lnt">410
</span><span class="lnt">411
</span><span class="lnt">412
</span><span class="lnt">413
</span><span class="lnt">414
</span><span class="lnt">415
</span><span class="lnt">416
</span><span class="lnt">417
</span><span class="lnt">418
</span><span class="lnt">419
</span><span class="lnt">420
</span><span class="lnt">421
</span><span class="lnt">422
</span><span class="lnt">423
</span><span class="lnt">424
</span><span class="lnt">425
</span><span class="lnt">426
</span><span class="lnt">427
</span><span class="lnt">428
</span><span class="lnt">429
</span><span class="lnt">430
</span><span class="lnt">431
</span><span class="lnt">432
</span><span class="lnt">433
</span><span class="lnt">434
</span><span class="lnt">435
</span><span class="lnt">436
</span><span class="lnt">437
</span><span class="lnt">438
</span><span class="lnt">439
</span><span class="lnt">440
</span><span class="lnt">441
</span><span class="lnt">442
</span><span class="lnt">443
</span><span class="lnt">444
</span><span class="lnt">445
</span><span class="lnt">446
</span><span class="lnt">447
</span><span class="lnt">448
</span><span class="lnt">449
</span><span class="lnt">450
</span><span class="lnt">451
</span><span class="lnt">452
</span><span class="lnt">453
</span><span class="lnt">454
</span><span class="lnt">455
</span><span class="lnt">456
</span><span class="lnt">457
</span><span class="lnt">458
</span><span class="lnt">459
</span><span class="lnt">460
</span><span class="lnt">461
</span><span class="lnt">462
</span><span class="lnt">463
</span><span class="lnt">464
</span><span class="lnt">465
</span><span class="lnt">466
</span><span class="lnt">467
</span><span class="lnt">468
</span><span class="lnt">469
</span><span class="lnt">470
</span><span class="lnt">471
</span><span class="lnt">472
</span><span class="lnt">473
</span><span class="lnt">474
</span><span class="lnt">475
</span><span class="lnt">476
</span><span class="lnt">477
</span><span class="lnt">478
</span><span class="lnt">479
</span><span class="lnt">480
</span><span class="lnt">481
</span><span class="lnt">482
</span><span class="lnt">483
</span><span class="lnt">484
</span><span class="lnt">485
</span><span class="lnt">486
</span><span class="lnt">487
</span><span class="lnt">488
</span><span class="lnt">489
</span><span class="lnt">490
</span><span class="lnt">491
</span><span class="lnt">492
</span><span class="lnt">493
</span><span class="lnt">494
</span><span class="lnt">495
</span><span class="lnt">496
</span><span class="lnt">497
</span><span class="lnt">498
</span><span class="lnt">499
</span><span class="lnt">500
</span><span class="lnt">501
</span><span class="lnt">502
</span><span class="lnt">503
</span><span class="lnt">504
</span><span class="lnt">505
</span><span class="lnt">506
</span><span class="lnt">507
</span><span class="lnt">508
</span><span class="lnt">509
</span><span class="lnt">510
</span><span class="lnt">511
</span><span class="lnt">512
</span><span class="lnt">513
</span><span class="lnt">514
</span><span class="lnt">515
</span><span class="lnt">516
</span><span class="lnt">517
</span><span class="lnt">518
</span><span class="lnt">519
</span><span class="lnt">520
</span><span class="lnt">521
</span><span class="lnt">522
</span><span class="lnt">523
</span><span class="lnt">524
</span><span class="lnt">525
</span><span class="lnt">526
</span><span class="lnt">527
</span><span class="lnt">528
</span><span class="lnt">529
</span><span class="lnt">530
</span><span class="lnt">531
</span><span class="lnt">532
</span><span class="lnt">533
</span><span class="lnt">534
</span><span class="lnt">535
</span><span class="lnt">536
</span><span class="lnt">537
</span><span class="lnt">538
</span><span class="lnt">539
</span><span class="lnt">540
</span><span class="lnt">541
</span><span class="lnt">542
</span><span class="lnt">543
</span><span class="lnt">544
</span><span class="lnt">545
</span><span class="lnt">546
</span><span class="lnt">547
</span><span class="lnt">548
</span><span class="lnt">549
</span><span class="lnt">550
</span><span class="lnt">551
</span><span class="lnt">552
</span><span class="lnt">553
</span><span class="lnt">554
</span><span class="lnt">555
</span><span class="lnt">556
</span><span class="lnt">557
</span><span class="lnt">558
</span><span class="lnt">559
</span><span class="lnt">560
</span><span class="lnt">561
</span><span class="lnt">562
</span><span class="lnt">563
</span><span class="lnt">564
</span><span class="lnt">565
</span><span class="lnt">566
</span><span class="lnt">567
</span><span class="lnt">568
</span><span class="lnt">569
</span><span class="lnt">570
</span><span class="lnt">571
</span><span class="lnt">572
</span><span class="lnt">573
</span><span class="lnt">574
</span><span class="lnt">575
</span><span class="lnt">576
</span><span class="lnt">577
</span><span class="lnt">578
</span><span class="lnt">579
</span><span class="lnt">580
</span><span class="lnt">581
</span><span class="lnt">582
</span><span class="lnt">583
</span><span class="lnt">584
</span><span class="lnt">585
</span><span class="lnt">586
</span><span class="lnt">587
</span><span class="lnt">588
</span><span class="lnt">589
</span><span class="lnt">590
</span><span class="lnt">591
</span><span class="lnt">592
</span><span class="lnt">593
</span><span class="lnt">594
</span><span class="lnt">595
</span><span class="lnt">596
</span><span class="lnt">597
</span><span class="lnt">598
</span><span class="lnt">599
</span><span class="lnt">600
</span><span class="lnt">601
</span><span class="lnt">602
</span><span class="lnt">603
</span><span class="lnt">604
</span><span class="lnt">605
</span><span class="lnt">606
</span><span class="lnt">607
</span><span class="lnt">608
</span><span class="lnt">609
</span><span class="lnt">610
</span><span class="lnt">611
</span><span class="lnt">612
</span><span class="lnt">613
</span><span class="lnt">614
</span><span class="lnt">615
</span><span class="lnt">616
</span><span class="lnt">617
</span><span class="lnt">618
</span><span class="lnt">619
</span><span class="lnt">620
</span><span class="lnt">621
</span><span class="lnt">622
</span><span class="lnt">623
</span><span class="lnt">624
</span><span class="lnt">625
</span><span class="lnt">626
</span><span class="lnt">627
</span><span class="lnt">628
</span><span class="lnt">629
</span><span class="lnt">630
</span><span class="lnt">631
</span><span class="lnt">632
</span><span class="lnt">633
</span><span class="lnt">634
</span><span class="lnt">635
</span><span class="lnt">636
</span><span class="lnt">637
</span><span class="lnt">638
</span><span class="lnt">639
</span><span class="lnt">640
</span><span class="lnt">641
</span><span class="lnt">642
</span><span class="lnt">643
</span><span class="lnt">644
</span><span class="lnt">645
</span><span class="lnt">646
</span><span class="lnt">647
</span><span class="lnt">648
</span><span class="lnt">649
</span><span class="lnt">650
</span><span class="lnt">651
</span><span class="lnt">652
</span><span class="lnt">653
</span><span class="lnt">654
</span><span class="lnt">655
</span><span class="lnt">656
</span><span class="lnt">657
</span><span class="lnt">658
</span><span class="lnt">659
</span><span class="lnt">660
</span><span class="lnt">661
</span><span class="lnt">662
</span><span class="lnt">663
</span><span class="lnt">664
</span><span class="lnt">665
</span><span class="lnt">666
</span><span class="lnt">667
</span><span class="lnt">668
</span><span class="lnt">669
</span><span class="lnt">670
</span><span class="lnt">671
</span><span class="lnt">672
</span><span class="lnt">673
</span><span class="lnt">674
</span><span class="lnt">675
</span><span class="lnt">676
</span><span class="lnt">677
</span><span class="lnt">678
</span><span class="lnt">679
</span><span class="lnt">680
</span><span class="lnt">681
</span><span class="lnt">682
</span><span class="lnt">683
</span><span class="lnt">684
</span><span class="lnt">685
</span><span class="lnt">686
</span><span class="lnt">687
</span><span class="lnt">688
</span><span class="lnt">689
</span><span class="lnt">690
</span><span class="lnt">691
</span><span class="lnt">692
</span><span class="lnt">693
</span><span class="lnt">694
</span><span class="lnt">695
</span><span class="lnt">696
</span><span class="lnt">697
</span><span class="lnt">698
</span><span class="lnt">699
</span><span class="lnt">700
</span><span class="lnt">701
</span><span class="lnt">702
</span><span class="lnt">703
</span><span class="lnt">704
</span><span class="lnt">705
</span><span class="lnt">706
</span><span class="lnt">707
</span><span class="lnt">708
</span><span class="lnt">709
</span><span class="lnt">710
</span><span class="lnt">711
</span><span class="lnt">712
</span><span class="lnt">713
</span><span class="lnt">714
</span><span class="lnt">715
</span><span class="lnt">716
</span><span class="lnt">717
</span><span class="lnt">718
</span><span class="lnt">719
</span><span class="lnt">720
</span><span class="lnt">721
</span><span class="lnt">722
</span><span class="lnt">723
</span><span class="lnt">724
</span><span class="lnt">725
</span><span class="lnt">726
</span><span class="lnt">727
</span><span class="lnt">728
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Generalisation of Geometric Vector Perceptron, Jing et al.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># for explicit multi-state biomolecule representation learning.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Original repository: https://github.com/drorlab/gvp-pytorch</span>
</span></span><span class="line"><span class="cl"><span class="c1"># MODIFIED: Added HybridGVPLongShortLayer for deep fusion of</span>
</span></span><span class="line"><span class="cl"><span class="c1"># local geometric message passing and an efficient long-short attention mechanism.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># VERSION 3: Final robust version incorporating all valid feedback.</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">functools</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch_geometric</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">MessagePassing</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch_scatter</span> <span class="kn">import</span> <span class="n">scatter_add</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># FINAL VERSION: 辅助函数</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_create_batch_mask</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;根据 torch_geometric 的 batch 向量创建块对角注意力掩码。&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">batch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">None</span> <span class="c1"># 返回None，在主函数中处理</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">same_graph</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">batch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mask</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">same_graph</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_create_window_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">window_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; 高效的向量化窗口掩码生成 &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">half</span> <span class="o">=</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">idx</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mask</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">dist</span> <span class="o">&gt;</span> <span class="n">half</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># FINAL VERSION: Long-Short Attention 模块</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LongShortAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    实现了Long-Short Transformer思想的注意力模块.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - 短程: 滑动窗口自注意力
</span></span></span><span class="line"><span class="cl"><span class="s2">    - 长程: 动态投影
</span></span></span><span class="line"><span class="cl"><span class="s2">    - 融合: DualLN + 拼接注意力得分
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">proj_rank</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;embed_dim 必须能被 num_heads 整除&#34;</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">proj_rank</span> <span class="o">=</span> <span class="n">proj_rank</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_proj_matrix_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">proj_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ln_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 采纳建议：为K和V使用独立的LN</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ln_k_short</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ln_v_short</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ln_k_long</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ln_v_long</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">x_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">k_short</span><span class="p">,</span> <span class="n">v_short</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln_k_short</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln_v_short</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_long</span><span class="p">,</span> <span class="n">v_long</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln_k_long</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln_v_long</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">q_s</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_s</span> <span class="o">=</span> <span class="n">k_short</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_s</span> <span class="o">=</span> <span class="n">v_short</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">window_mask</span> <span class="o">=</span> <span class="n">_create_window_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_mask</span> <span class="o">=</span> <span class="n">_create_batch_mask</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 采纳建议：在主函数中处理None</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">batch_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">window_mask</span> <span class="o">+</span> <span class="n">batch_mask</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">window_mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">attn_scores_short</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_s</span><span class="p">,</span> <span class="n">k_s</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 在使用时添加广播维度</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_scores_short</span> <span class="o">=</span> <span class="n">attn_scores_short</span> <span class="o">+</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">proj_matrix</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_proj_matrix_proj</span><span class="p">(</span><span class="n">k_long</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_compressed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">proj_matrix</span><span class="p">,</span> <span class="n">k_long</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">        <span class="n">v_compressed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">proj_matrix</span><span class="p">,</span> <span class="n">v_long</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">q_l</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_c</span> <span class="o">=</span> <span class="n">k_compressed</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_c</span> <span class="o">=</span> <span class="n">v_compressed</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">attn_scores_long</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_l</span><span class="p">,</span> <span class="n">k_c</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">attn_scores_fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">attn_scores_short</span><span class="p">,</span> <span class="n">attn_scores_long</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores_fused</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">attn_probs_short</span> <span class="o">=</span> <span class="n">attn_probs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_probs_long</span> <span class="o">=</span> <span class="n">attn_probs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">output_short</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_probs_short</span><span class="p">,</span> <span class="n">v_s</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_long</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_probs_long</span><span class="p">,</span> <span class="n">v_c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output_short</span> <span class="o">+</span> <span class="n">output_long</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># (PositionwiseFeedForward 类保持不变)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; A two-layer Feed-Forward-Network. &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">w_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_hid</span><span class="p">,</span> <span class="n">d_in</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">silu</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># FINAL VERSION: HybridGVPLongShortLayer</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HybridGVPLongShortLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    将GVP-GNN与Long-Short Attention思想深度融合的混合层。
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">node_dims</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_attn_head</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">ls_window_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">ls_proj_rank</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">n_message</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">drop_rate</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HybridGVPLongShortLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm_gvp</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">node_dims</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gvp_conv</span> <span class="o">=</span> <span class="n">MultiGVPConv</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span> <span class="n">n_message</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_gvp</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ls_attention</span> <span class="o">=</span> <span class="n">LongShortAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_attn_head</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                               <span class="n">window_size</span><span class="o">=</span><span class="n">ls_window_size</span><span class="p">,</span> <span class="n">proj_rank</span><span class="o">=</span><span class="n">ls_proj_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                               <span class="n">dropout</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm_ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_in</span><span class="o">=</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d_hid</span><span class="o">=</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">x_res_gvp</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_norm_gvp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_gvp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">dh_local</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gvp_conv</span><span class="p">(</span><span class="n">x_norm_gvp</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_local</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x_res_gvp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_gvp</span><span class="p">(</span><span class="n">dh_local</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_local</span><span class="p">,</span> <span class="n">v_local</span> <span class="o">=</span> <span class="n">x_local</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">s_res_attn</span> <span class="o">=</span> <span class="n">s_local</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_norm_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_attn</span><span class="p">(</span><span class="n">s_local</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">n_nodes</span><span class="p">,</span> <span class="n">n_conf</span><span class="p">,</span> <span class="n">d_s</span> <span class="o">=</span> <span class="n">s_norm_attn</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_reshaped</span> <span class="o">=</span> <span class="n">s_norm_attn</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">s_attn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ls_attention</span><span class="p">(</span><span class="n">s_reshaped</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">s_attn_out</span> <span class="o">=</span> <span class="n">s_attn_out</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_after_attn</span> <span class="o">=</span> <span class="n">s_res_attn</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_attn</span><span class="p">(</span><span class="n">s_attn_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">s_res_ffn</span> <span class="o">=</span> <span class="n">s_after_attn</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_norm_ffn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ffn</span><span class="p">(</span><span class="n">s_after_attn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_ffn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">s_norm_ffn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_final</span> <span class="o">=</span> <span class="n">s_res_ffn</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ffn</span><span class="p">(</span><span class="n">s_ffn_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">v_final</span> <span class="o">=</span> <span class="n">v_local</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">s_final</span><span class="p">,</span> <span class="n">v_final</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GVPConvLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Full graph convolution / message passing layer with 
</span></span></span><span class="line"><span class="cl"><span class="s1">    Geometric Vector Perceptrons. Residually updates node embeddings with
</span></span></span><span class="line"><span class="cl"><span class="s1">    aggregated incoming messages, applies a pointwise feedforward 
</span></span></span><span class="line"><span class="cl"><span class="s1">    network to node embeddings, and returns updated node embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    To only compute the aggregated messages, see `GVPConv`.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param node_dims: node embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param n_message: number of GVPs to use in message function
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param n_feedforward: number of GVPs to use in feedforward function
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param drop_rate: drop probability in all dropout layers
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param autoregressive: if `True`, this `GVPConvLayer` will be used
</span></span></span><span class="line"><span class="cl"><span class="s1">           with a different set of input node embeddings for messages
</span></span></span><span class="line"><span class="cl"><span class="s1">           where src &gt;= dst
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param vector_gate: whether to use vector gating.
</span></span></span><span class="line"><span class="cl"><span class="s1">                        (vector_act will be used as sigma^+ in vector gating if `True`)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">node_dims</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">n_message</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">n_feedforward</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">drop_rate</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">autoregressive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">norm_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">GVPConvLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">GVPConv</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span> <span class="n">n_message</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;add&#34;</span> <span class="k">if</span> <span class="n">autoregressive</span> <span class="k">else</span> <span class="s2">&#34;mean&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">GVP_</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">node_dims</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">ff_func</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">n_feedforward</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">hid_dims</span> <span class="o">=</span> <span class="mi">4</span><span class="o">*</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="o">*</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_feedforward</span><span class="o">-</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">ff_func</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span> <span class="o">=</span> <span class="n">norm_first</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">autoregressive_x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">node_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_index: array of shape [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_attr: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param autoregressive_x: tuple (s, V) of `torch.Tensor`. 
</span></span></span><span class="line"><span class="cl"><span class="s1">                If not `None`, will be used as src node embeddings
</span></span></span><span class="line"><span class="cl"><span class="s1">                for forming messages where src &gt;= dst. The current node 
</span></span></span><span class="line"><span class="cl"><span class="s1">                embeddings `x` will still be the base of the update and the 
</span></span></span><span class="line"><span class="cl"><span class="s1">                pointwise feedforward.
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param node_mask: array of type `bool` to index into the first
</span></span></span><span class="line"><span class="cl"><span class="s1">                dim of node embeddings (s, V). If not `None`, only
</span></span></span><span class="line"><span class="cl"><span class="s1">                these nodes will be updated.
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">autoregressive_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">src</span><span class="p">,</span> <span class="n">dst</span> <span class="o">=</span> <span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">            <span class="n">mask</span> <span class="o">=</span> <span class="n">src</span> <span class="o">&lt;</span> <span class="n">dst</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_index_forward</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[:,</span> <span class="n">mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_index_backward</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[:,</span> <span class="o">~</span><span class="n">mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_attr_forward</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">edge_attr</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_attr_backward</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">edge_attr</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index_forward</span><span class="p">,</span> <span class="n">edge_attr_forward</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">autoregressive_x</span><span class="p">,</span> <span class="n">edge_index_backward</span><span class="p">,</span> <span class="n">edge_attr_backward</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">count</span> <span class="o">=</span> <span class="n">scatter_add</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">dst</span><span class="p">),</span> <span class="n">dst</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">dim_size</span><span class="o">=</span><span class="n">dh</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="n">dh</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">count</span><span class="p">,</span> <span class="n">dh</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">count</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">),</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">node_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x_</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">,</span> <span class="n">dh</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">node_mask</span><span class="p">),</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">dh</span><span class="p">,</span> <span class="n">node_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">dh</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">dh</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">dh</span><span class="p">)))</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="k">else</span> <span class="n">dh</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">dh</span><span class="p">)))</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="k">else</span> <span class="n">dh</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">node_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">node_mask</span><span class="p">],</span> <span class="n">x_</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">node_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">x_</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GVPConv</span><span class="p">(</span><span class="n">MessagePassing</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Graph convolution / message passing with Geometric Vector Perceptrons.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes in a graph with node and edge embeddings,
</span></span></span><span class="line"><span class="cl"><span class="s1">    and returns new node embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    This does NOT do residual updates and pointwise feedforward layers
</span></span></span><span class="line"><span class="cl"><span class="s1">    ---see `GVPConvLayer`.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param in_dims: input node embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param out_dims: output node embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param n_layers: number of GVPs in the message function
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param module_list: preconstructed message function, overrides n_layers
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param aggr: should be &#34;add&#34; if some incoming edges are masked, as in
</span></span></span><span class="line"><span class="cl"><span class="s1">                 a masked autoregressive decoder architecture, otherwise &#34;mean&#34;
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param vector_gate: whether to use vector gating.
</span></span></span><span class="line"><span class="cl"><span class="s1">                        (vector_act will be used as sigma^+ in vector gating if `True`)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">module_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                 <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">GVPConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="n">aggr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">=</span> <span class="n">in_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span> <span class="o">=</span> <span class="n">out_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span> <span class="o">=</span> <span class="n">edge_dims</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">GVP_</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">module_list</span> <span class="o">=</span> <span class="n">module_list</span> <span class="ow">or</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">module_list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">GVP_</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">GVP_</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span><span class="p">),</span> <span class="n">out_dims</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                       <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">message_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">module_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_index: array of shape [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_attr: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_s</span><span class="p">,</span> <span class="n">x_v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">s</span><span class="o">=</span><span class="n">x_s</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">x_v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">edge_attr</span><span class="o">=</span><span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_split</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">v_i</span><span class="p">,</span> <span class="n">s_j</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_j</span> <span class="o">=</span> <span class="n">v_j</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_i</span> <span class="o">=</span> <span class="n">v_i</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="n">tuple_cat</span><span class="p">((</span><span class="n">s_j</span><span class="p">,</span> <span class="n">v_j</span><span class="p">),</span> <span class="n">edge_attr</span><span class="p">,</span> <span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">v_i</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">message_func</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_merge</span><span class="p">(</span><span class="o">*</span><span class="n">message</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MultiGVPConvLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    GVPConvLayer for handling multiple conformations (encoder-only)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">node_dims</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">n_message</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">n_feedforward</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">drop_rate</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">norm_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">MultiGVPConvLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">MultiGVPConv</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span> <span class="n">n_message</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">GVP_</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">node_dims</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">ff_func</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">n_feedforward</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">hid_dims</span> <span class="o">=</span> <span class="mi">4</span><span class="o">*</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="o">*</span><span class="n">node_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">node_dims</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_feedforward</span><span class="o">-</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">ff_func</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">,</span> <span class="n">node_dims</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">ff_func</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span> <span class="o">=</span> <span class="n">norm_first</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_index: array of shape [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_attr: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_first</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">),</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">dh</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">dh</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">dh</span><span class="p">)))</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="k">else</span> <span class="n">dh</span>
</span></span><span class="line"><span class="cl">            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">tuple_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">dh</span><span class="p">)))</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="k">else</span> <span class="n">dh</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MultiGVPConv</span><span class="p">(</span><span class="n">MessagePassing</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    GVPConv for handling multiple conformations
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">edge_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">module_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aggr</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                 <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">MultiGVPConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="n">aggr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">=</span> <span class="n">in_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span> <span class="o">=</span> <span class="n">out_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span> <span class="o">=</span> <span class="n">edge_dims</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">GVP_</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="n">vector_gate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">module_list</span> <span class="o">=</span> <span class="n">module_list</span> <span class="ow">or</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">module_list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">GVP_</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">GVP_</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">se</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ve</span><span class="p">),</span> <span class="n">out_dims</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="n">module_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GVP_</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                       <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">message_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">module_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_index: array of shape [2, n_edges]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param edge_attr: tuple (s, V) of `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_s</span><span class="p">,</span> <span class="n">x_v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_conf</span> <span class="o">=</span> <span class="n">x_s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># x_s: [n_nodes, n_conf, d] -&gt; [n_nodes, n_conf * d]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_s</span> <span class="o">=</span> <span class="n">x_s</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x_s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>        
</span></span><span class="line"><span class="cl">        <span class="c1"># x_v: [n_nodes, n_conf, d, 3] -&gt; [n_nodes, n_conf * d * 3]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_v</span> <span class="o">=</span> <span class="n">x_v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">x_s</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">x_v</span><span class="p">,</span> <span class="n">edge_attr</span><span class="o">=</span><span class="n">edge_attr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_split_multi</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">,</span> <span class="n">n_conf</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s_i</span><span class="p">,</span> <span class="n">v_i</span><span class="p">,</span> <span class="n">s_j</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [n_nodes, n_conf * d] -&gt; [n_nodes, n_conf, d]</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_i</span> <span class="o">=</span> <span class="n">s_i</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s_j</span> <span class="o">=</span> <span class="n">s_j</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [n_nodes, n_conf * d * 3] -&gt; [n_nodes, n_conf, d, 3]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_i</span> <span class="o">=</span> <span class="n">v_i</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">*</span> <span class="mi">3</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_j</span> <span class="o">=</span> <span class="n">v_j</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v_j</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">//</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">*</span> <span class="mi">3</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="n">tuple_cat</span><span class="p">((</span><span class="n">s_j</span><span class="p">,</span> <span class="n">v_j</span><span class="p">),</span> <span class="n">edge_attr</span><span class="p">,</span> <span class="p">(</span><span class="n">s_i</span><span class="p">,</span> <span class="n">v_i</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">message</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">message_func</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_merge_multi</span><span class="p">(</span><span class="o">*</span><span class="n">message</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GVP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Geometric Vector Perceptron. See manuscript and README.md
</span></span></span><span class="line"><span class="cl"><span class="s1">    for more details.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param in_dims: tuple (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param out_dims: tuple (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param h_dim: intermediate number of vector channels, optional
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param activations: tuple of functions (scalar_act, vector_act)
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param vector_gate: whether to use vector gating.
</span></span></span><span class="line"><span class="cl"><span class="s1">                        (vector_act will be used as sigma^+ in vector gating if `True`)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dims</span><span class="p">,</span> <span class="n">out_dims</span><span class="p">,</span> <span class="n">h_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">GVP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span> <span class="o">=</span> <span class="n">in_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span> <span class="o">=</span> <span class="n">out_dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">vector_gate</span> <span class="o">=</span> <span class="n">vector_gate</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span> <span class="o">=</span> <span class="n">h_dim</span> <span class="ow">or</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ws</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_gate</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">wsv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ws</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">si</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">so</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">scalar_act</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span> <span class="o">=</span> <span class="n">activations</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dummy_param</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`, 
</span></span></span><span class="line"><span class="cl"><span class="s1">                  or (if vectors_in is 0), a single `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        :return: tuple (s, V) of `torch.Tensor`,
</span></span></span><span class="line"><span class="cl"><span class="s1">                 or (if vectors_out is 0), a single `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">vh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wh</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>    
</span></span><span class="line"><span class="cl">            <span class="n">vn</span> <span class="o">=</span> <span class="n">_norm_no_nan</span><span class="p">(</span><span class="n">vh</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">vn</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">                <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">vh</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">                <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_gate</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                        <span class="n">gate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wsv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                        <span class="n">gate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wsv</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gate</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_act</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="n">_norm_no_nan</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dummy_param</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar_act</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar_act</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vo</span> <span class="k">else</span> <span class="n">s</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">_VDropout</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Vector channel dropout where the elements of each
</span></span></span><span class="line"><span class="cl"><span class="s1">    vector channel are dropped together.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">_VDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">drop_rate</span> <span class="o">=</span> <span class="n">drop_rate</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dummy_param</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: `torch.Tensor` corresponding to vector channels
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dummy_param</span><span class="o">.</span><span class="n">device</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Combined dropout for tuples (s, V).
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes tuples (s, V) as input and as output.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">Dropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">sdropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">vdropout</span> <span class="o">=</span> <span class="n">_VDropout</span><span class="p">(</span><span class="n">drop_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`,
</span></span></span><span class="line"><span class="cl"><span class="s1">                  or single `torch.Tensor` 
</span></span></span><span class="line"><span class="cl"><span class="s1">                  (will be assumed to be scalar channels)
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sdropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sdropout</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vdropout</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Combined LayerNorm for tuples (s, V).
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes tuples (s, V) as input and as output.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dims</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">dims</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">scalar_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        :param x: tuple (s, V) of `torch.Tensor`,
</span></span></span><span class="line"><span class="cl"><span class="s1">                  or single `torch.Tensor` 
</span></span></span><span class="line"><span class="cl"><span class="s1">                  (will be assumed to be scalar channels)
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">vn</span> <span class="o">=</span> <span class="n">_norm_no_nan</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sqrt</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">vn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">vn</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scalar_norm</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">v</span> <span class="o">/</span> <span class="n">vn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tuple_sum</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Sums any number of tuples (s, V) elementwise.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">sum</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tuple_cat</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Concatenates any number of tuples (s, V) elementwise.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param dim: dimension along which to concatenate when viewed
</span></span></span><span class="line"><span class="cl"><span class="s1">                as the `dim` index for the scalar-channel tensors.
</span></span></span><span class="line"><span class="cl"><span class="s1">                This means that `dim=-1` will be applied as
</span></span></span><span class="line"><span class="cl"><span class="s1">                `dim=-2` for the vector-channel tensors.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dim</span> <span class="o">%=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">s_args</span><span class="p">,</span> <span class="n">v_args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">s_args</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">v_args</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tuple_index</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Indexes into a tuple (s, V) along the first dimension.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param idx: any object which can be used to index into a `torch.Tensor`
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&#34;cpu&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Returns random tuples (s, V) drawn elementwise from a normal distribution.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param n: number of data points
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param dims: tuple of dimensions (n_scalar, n_vector)
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :return: (s, V) with s.shape = (n, n_scalar) and
</span></span></span><span class="line"><span class="cl"><span class="s1">             V.shape = (n, n_vector, 3)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> \
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_norm_no_nan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">sqrt</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    L2 norm of tensor clamped above a minimum value `eps`.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param sqrt: if `False`, returns the square of the L2 norm
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="k">if</span> <span class="n">sqrt</span> <span class="k">else</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">nv</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Splits a merged representation of (s, V) back into a tuple. 
</span></span></span><span class="line"><span class="cl"><span class="s1">    Should be used only with `_merge(s, V)` and only if the tuple 
</span></span></span><span class="line"><span class="cl"><span class="s1">    representation cannot be used.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param x: the `torch.Tensor` returned from `_merge`
</span></span></span><span class="line"><span class="cl"><span class="s1">    :param nv: the number of vector channels in the input to `_merge`
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">s</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">nv</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">nv</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nv</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_merge</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Merges a tuple (s, V) into a single `torch.Tensor`, where the
</span></span></span><span class="line"><span class="cl"><span class="s1">    vector channels are flattened and appended to the scalar channels.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Should be used only if the tuple representation cannot be used.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Use `_split(x, nv)` to reverse.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_split_multi</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">nv</span><span class="p">,</span> <span class="n">n_conf</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    _split for multiple conformers
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">s</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">nv</span> <span class="o">*</span> <span class="n">n_conf</span><span class="p">]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_conf</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">nv</span> <span class="o">*</span> <span class="n">n_conf</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_conf</span><span class="p">,</span> <span class="n">nv</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_merge_multi</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    _merge for multiple conformers
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># s: [n_nodes, n_conf, d] -&gt; [n_nodes, n_conf * d]</span>
</span></span><span class="line"><span class="cl">    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># v: [n_nodes, n_conf, d, 3] -&gt; [n_nodes, n_conf * d * 3]</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>以及models.py:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span><span class="lnt">260
</span><span class="lnt">261
</span><span class="lnt">262
</span><span class="lnt">263
</span><span class="lnt">264
</span><span class="lnt">265
</span><span class="lnt">266
</span><span class="lnt">267
</span><span class="lnt">268
</span><span class="lnt">269
</span><span class="lnt">270
</span><span class="lnt">271
</span><span class="lnt">272
</span><span class="lnt">273
</span><span class="lnt">274
</span><span class="lnt">275
</span><span class="lnt">276
</span><span class="lnt">277
</span><span class="lnt">278
</span><span class="lnt">279
</span><span class="lnt">280
</span><span class="lnt">281
</span><span class="lnt">282
</span><span class="lnt">283
</span><span class="lnt">284
</span><span class="lnt">285
</span><span class="lnt">286
</span><span class="lnt">287
</span><span class="lnt">288
</span><span class="lnt">289
</span><span class="lnt">290
</span><span class="lnt">291
</span><span class="lnt">292
</span><span class="lnt">293
</span><span class="lnt">294
</span><span class="lnt">295
</span><span class="lnt">296
</span><span class="lnt">297
</span><span class="lnt">298
</span><span class="lnt">299
</span><span class="lnt">300
</span><span class="lnt">301
</span><span class="lnt">302
</span><span class="lnt">303
</span><span class="lnt">304
</span><span class="lnt">305
</span><span class="lnt">306
</span><span class="lnt">307
</span><span class="lnt">308
</span><span class="lnt">309
</span><span class="lnt">310
</span><span class="lnt">311
</span><span class="lnt">312
</span><span class="lnt">313
</span><span class="lnt">314
</span><span class="lnt">315
</span><span class="lnt">316
</span><span class="lnt">317
</span><span class="lnt">318
</span><span class="lnt">319
</span><span class="lnt">320
</span><span class="lnt">321
</span><span class="lnt">322
</span><span class="lnt">323
</span><span class="lnt">324
</span><span class="lnt">325
</span><span class="lnt">326
</span><span class="lnt">327
</span><span class="lnt">328
</span><span class="lnt">329
</span><span class="lnt">330
</span><span class="lnt">331
</span><span class="lnt">332
</span><span class="lnt">333
</span><span class="lnt">334
</span><span class="lnt">335
</span><span class="lnt">336
</span><span class="lnt">337
</span><span class="lnt">338
</span><span class="lnt">339
</span><span class="lnt">340
</span><span class="lnt">341
</span><span class="lnt">342
</span><span class="lnt">343
</span><span class="lnt">344
</span><span class="lnt">345
</span><span class="lnt">346
</span><span class="lnt">347
</span><span class="lnt">348
</span><span class="lnt">349
</span><span class="lnt">350
</span><span class="lnt">351
</span><span class="lnt">352
</span><span class="lnt">353
</span><span class="lnt">354
</span><span class="lnt">355
</span><span class="lnt">356
</span><span class="lnt">357
</span><span class="lnt">358
</span><span class="lnt">359
</span><span class="lnt">360
</span><span class="lnt">361
</span><span class="lnt">362
</span><span class="lnt">363
</span><span class="lnt">364
</span><span class="lnt">365
</span><span class="lnt">366
</span><span class="lnt">367
</span><span class="lnt">368
</span><span class="lnt">369
</span><span class="lnt">370
</span><span class="lnt">371
</span><span class="lnt">372
</span><span class="lnt">373
</span><span class="lnt">374
</span><span class="lnt">375
</span><span class="lnt">376
</span><span class="lnt">377
</span><span class="lnt">378
</span><span class="lnt">379
</span><span class="lnt">380
</span><span class="lnt">381
</span><span class="lnt">382
</span><span class="lnt">383
</span><span class="lnt">384
</span><span class="lnt">385
</span><span class="lnt">386
</span><span class="lnt">387
</span><span class="lnt">388
</span><span class="lnt">389
</span><span class="lnt">390
</span><span class="lnt">391
</span><span class="lnt">392
</span><span class="lnt">393
</span><span class="lnt">394
</span><span class="lnt">395
</span><span class="lnt">396
</span><span class="lnt">397
</span><span class="lnt">398
</span><span class="lnt">399
</span><span class="lnt">400
</span><span class="lnt">401
</span><span class="lnt">402
</span><span class="lnt">403
</span><span class="lnt">404
</span><span class="lnt">405
</span><span class="lnt">406
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Generalisation of Geometric Vector Perceptron, Jing et al.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># for explicit multi-state biomolecule representation learning.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Original repository: https://github.com/drorlab/gvp-pytorch</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch_geometric</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">src.layers</span> <span class="kn">import</span> <span class="o">*</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AutoregressiveMultiGNNv1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Autoregressive GVP-GNN for **multiple** structure-conditioned RNA design.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes in RNA structure graphs of type `torch_geometric.data.Data` 
</span></span></span><span class="line"><span class="cl"><span class="s1">    or `torch_geometric.data.Batch` and returns a categorical distribution
</span></span></span><span class="line"><span class="cl"><span class="s1">    over 4 bases at each position in a `torch.Tensor` of shape [n_nodes, 4].
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    The standard forward pass requires sequence information as input
</span></span></span><span class="line"><span class="cl"><span class="s1">    and should be used for training or evaluating likelihood.
</span></span></span><span class="line"><span class="cl"><span class="s1">    For sampling or design, use `self.sample`.
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_in_dim (tuple): node dimensions in input graph
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_h_dim (tuple): node dimensions to use in GVP-GNN layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_in_dim (tuple): edge dimensions in input graph
</span></span></span><span class="line"><span class="cl"><span class="s1">        edge_h_dim (tuple): edge dimensions to embed in GVP-GNN layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        num_layers (int): number of GVP-GNN layers in encoder/decoder
</span></span></span><span class="line"><span class="cl"><span class="s1">        drop_rate (float): rate to use in all dropout layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        out_dim (int): output dimension (4 bases)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">node_in_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">node_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">edge_in_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">edge_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">drop_rate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">out_dim</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_attn_head</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">ls_window_size</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">ls_proj_rank</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span> <span class="o">=</span> <span class="n">node_in_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span> <span class="o">=</span> <span class="n">node_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span> <span class="o">=</span> <span class="n">edge_in_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span> <span class="o">=</span> <span class="n">edge_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
</span></span><span class="line"><span class="cl">        <span class="n">activations</span> <span class="o">=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Node input embedding</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Edge input embedding</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Encoder layers (supports multiple conformations)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">HybridGVPLongShortLayer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">node_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">edge_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">num_attn_head</span><span class="o">=</span><span class="n">num_attn_head</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">ls_window_size</span><span class="o">=</span><span class="n">ls_window_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">ls_proj_rank</span><span class="o">=</span><span class="n">ls_proj_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">drop_rate</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Decoder layers</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_s</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">GVPConvLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                             <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                             <span class="n">drop_rate</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">,</span> <span class="n">autoregressive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">norm_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Output</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">node_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">node_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">seq</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span><span class="p">(</span><span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">batch</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Pool multi-conformation features: </span>
</span></span><span class="line"><span class="cl">        <span class="c1"># nodes: (n_nodes, d_s), (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># edges: (n_edges, d_se), (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_multi_conf</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">mask_confs</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">encoder_embeddings</span> <span class="o">=</span> <span class="n">h_V</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">h_S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_s</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_S</span> <span class="o">=</span> <span class="n">h_S</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_S</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">h_S</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">autoregressive_x</span> <span class="o">=</span> <span class="n">encoder_embeddings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">logits</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nd">@torch.no_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">batch</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">n_samples</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">return_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        Samples sequences autoregressively from the distribution
</span></span></span><span class="line"><span class="cl"><span class="s1">        learned by the model.
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">        Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">            batch (torch_geometric.data.Data): mini-batch containing one
</span></span></span><span class="line"><span class="cl"><span class="s1">                RNA backbone to design sequences for
</span></span></span><span class="line"><span class="cl"><span class="s1">            n_samples (int): number of samples
</span></span></span><span class="line"><span class="cl"><span class="s1">            temperature (float): temperature to use in softmax over 
</span></span></span><span class="line"><span class="cl"><span class="s1">                the categorical distribution
</span></span></span><span class="line"><span class="cl"><span class="s1">            logit_bias (torch.Tensor): bias to add to logits during sampling
</span></span></span><span class="line"><span class="cl"><span class="s1">                to manually fix or control nucleotides in designed sequences,
</span></span></span><span class="line"><span class="cl"><span class="s1">                of shape [n_nodes, 4]
</span></span></span><span class="line"><span class="cl"><span class="s1">            return_logits (bool): whether to return logits or not
</span></span></span><span class="line"><span class="cl"><span class="s1">        
</span></span></span><span class="line"><span class="cl"><span class="s1">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s1">            seq (torch.Tensor): int tensor of shape [n_samples, n_nodes]
</span></span></span><span class="line"><span class="cl"><span class="s1">                                based on the residue-to-int mapping of
</span></span></span><span class="line"><span class="cl"><span class="s1">                                the original training data
</span></span></span><span class="line"><span class="cl"><span class="s1">            logits (torch.Tensor): logits of shape [n_samples, n_nodes, 4]
</span></span></span><span class="line"><span class="cl"><span class="s1">                                   (only if return_logits is True)
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span> 
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">node_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">node_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">edge_index</span><span class="o">.</span><span class="n">device</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span><span class="p">(</span><span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Pool multi-conformation features</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># nodes: (n_nodes, d_s), (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># edges: (n_edges, d_se), (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_multi_conf</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">mask_confs</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Repeat features for sampling n_samples times</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Expand edge index for autoregressive decoding</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">edge_index</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">offset</span> <span class="o">=</span> <span class="n">num_nodes</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">edge_index</span> <span class="o">+</span> <span class="n">offset</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This is akin to &#39;batching&#39; (in PyG style) n_samples copies of the graph</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h_V_cache</span> <span class="o">=</span> <span class="p">[(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Decode one token at a time</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">h_S_</span> <span class="o">=</span> <span class="n">h_S</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_S_</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E_</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">h_S_</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">            <span class="n">edge_mask</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">num_nodes</span> <span class="o">==</span> <span class="n">i</span>  <span class="c1"># True for all edges where dst is node i</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_index_</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[:,</span> <span class="n">edge_mask</span><span class="p">]</span>  <span class="c1"># subset all incoming edges to node i</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E_</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">h_E_</span><span class="p">,</span> <span class="n">edge_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">node_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">node_mask</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># True for all nodes i and its repeats</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">out</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V_cache</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">edge_index_</span><span class="p">,</span> <span class="n">h_E_</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">autoregressive_x</span><span class="o">=</span><span class="n">h_V_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">node_mask</span><span class="o">=</span><span class="n">node_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="n">out</span> <span class="o">=</span> <span class="n">tuple_index</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">node_mask</span><span class="p">)</span>  <span class="c1"># subset out to only node i and its repeats</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">h_V_cache</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="n">h_V_cache</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">            <span class="n">lgts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Add logit bias if provided to fix or bias positions</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">logit_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">lgts</span> <span class="o">+=</span> <span class="n">logit_bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Sample from logits</span>
</span></span><span class="line"><span class="cl">            <span class="n">seq</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">lgts</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_S</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_s</span><span class="p">(</span><span class="n">seq</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">::</span><span class="n">num_nodes</span><span class="p">]</span> <span class="o">=</span> <span class="n">lgts</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">return_logits</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">seq</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">),</span> <span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>    
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">seq</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">pool_multi_conf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">mask_confs</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Number of conformations is 1, no need to pool</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># True num_conf for masked mean pooling</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_conf_true</span> <span class="o">=</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (n_nodes, 1)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask scalar features</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V0</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E0</span> <span class="o">=</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask vector features</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, 1, 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V1</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E1</span> <span class="o">=</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Average pooling multi-conformation features</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">,</span>               <span class="c1"># (n_nodes, d_s)</span>
</span></span><span class="line"><span class="cl">               <span class="n">h_V1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_E0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>               <span class="c1"># (n_edges, d_se)</span>
</span></span><span class="line"><span class="cl">               <span class="n">h_E1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">NonAutoregressiveMultiGNNv1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Non-Autoregressive GVP-GNN for **multiple** structure-conditioned RNA design.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    Takes in RNA structure graphs of type `torch_geometric.data.Data` 
</span></span></span><span class="line"><span class="cl"><span class="s1">    or `torch_geometric.data.Batch` and returns a categorical distribution
</span></span></span><span class="line"><span class="cl"><span class="s1">    over 4 bases at each position in a `torch.Tensor` of shape [n_nodes, 4].
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    The standard forward pass requires sequence information as input
</span></span></span><span class="line"><span class="cl"><span class="s1">    and should be used for training or evaluating likelihood.
</span></span></span><span class="line"><span class="cl"><span class="s1">    For sampling or design, use `self.sample`.
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_in_dim (tuple): node dimensions in input graph
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_h_dim (tuple): node dimensions to use in GVP-GNN layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        node_in_dim (tuple): edge dimensions in input graph
</span></span></span><span class="line"><span class="cl"><span class="s1">        edge_h_dim (tuple): edge dimensions to embed in GVP-GNN layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        num_layers (int): number of GVP-GNN layers in encoder/decoder
</span></span></span><span class="line"><span class="cl"><span class="s1">        drop_rate (float): rate to use in all dropout layers
</span></span></span><span class="line"><span class="cl"><span class="s1">        out_dim (int): output dimension (4 bases)
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">node_in_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">node_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">edge_in_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">edge_h_dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">drop_rate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">out_dim</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span> <span class="o">=</span> <span class="n">node_in_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span> <span class="o">=</span> <span class="n">node_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span> <span class="o">=</span> <span class="n">edge_in_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span> <span class="o">=</span> <span class="n">edge_h_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
</span></span><span class="line"><span class="cl">        <span class="n">activations</span> <span class="o">=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Node input embedding</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Edge input embedding</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Encoder layers (supports multiple conformations)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">MultiGVPConvLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_h_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                  <span class="n">activations</span><span class="o">=</span><span class="n">activations</span><span class="p">,</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                  <span class="n">drop_rate</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">,</span> <span class="n">norm_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Output</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">vector_gate</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">GVP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_h_dim</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">activations</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>   
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">node_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">node_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span><span class="p">(</span><span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Pool multi-conformation features: </span>
</span></span><span class="line"><span class="cl">        <span class="c1"># nodes: (n_nodes, d_s), (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># edges: (n_edges, d_se), (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, out_dim)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">logits</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">return_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">node_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">node_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">edge_s</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">edge_index</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">edge_index</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_e</span><span class="p">(</span><span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">h_V</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_V</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">h_E</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># Pool multi-conformation features</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index)</span>
</span></span><span class="line"><span class="cl">            <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_out</span><span class="p">(</span><span class="n">h_V</span><span class="p">)</span>  <span class="c1"># (n_nodes, out_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_samples)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">return_logits</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">seq</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">logits</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">seq</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">pool_multi_conf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span><span class="p">,</span> <span class="n">mask_confs</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Number of conformations is 1, no need to pool</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># True num_conf for masked mean pooling</span>
</span></span><span class="line"><span class="cl">        <span class="n">n_conf_true</span> <span class="o">=</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (n_nodes, 1)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask scalar features</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask_confs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V0</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E0</span> <span class="o">=</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Mask vector features</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># (n_nodes, n_conf, 1, 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V1</span> <span class="o">=</span> <span class="n">h_V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E1</span> <span class="o">=</span> <span class="n">h_E</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Average pooling multi-conformation features</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_V</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_V0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">,</span>               <span class="c1"># (n_nodes, d_s)</span>
</span></span><span class="line"><span class="cl">               <span class="n">h_V1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># (n_nodes, d_v, 3)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_E</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_E0</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>               <span class="c1"># (n_edges, d_se)</span>
</span></span><span class="line"><span class="cl">               <span class="n">h_E1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_conf_true</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># (n_edges, d_ve, 3)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">h_V</span><span class="p">,</span> <span class="n">h_E</span>
</span></span></code></pre></td></tr></table>
</div>
</div>
</section>


    <footer class="article-footer">
    

    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">相关文章</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/p/%E5%BE%9C%E5%BE%89%E5%9C%A8arxiv%E5%92%8Cbioarxiv%E7%9A%84%E6%B5%B7%E6%B4%8B%E9%87%8C/">
        
        
            <div class="article-image">
                <img src="/p/%E5%BE%9C%E5%BE%89%E5%9C%A8arxiv%E5%92%8Cbioarxiv%E7%9A%84%E6%B5%B7%E6%B4%8B%E9%87%8C/1.a2a284e858a6260395fb14d5b3e3f387_hu_7b1f37f7ffa701b7.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 徜徉在arxiv和bioarxiv的海洋里"
                        
                        data-hash="md5-oqKE6FimJgOV&#43;xTVs&#43;Pzhw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">徜徉在arxiv和bioarxiv的海洋里</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/how-about-rna-inverse-folding-first-attempt./">
        
        

        <div class="article-details">
            <h2 class="article-title">How about RNA inverse folding? First attempt.</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2024 - 
        
        2025 Xiangyu Wen
    </section>
    
    <section class="powerby">
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>
<style>
  body {
    background: url(https://mosfish.github.io/background/bac.webp) no-repeat center top;
    background-size: cover;
    background-attachment: fixed;
  }
   @font-face {
    font-family: 'note';
    src: url(https://mosfish.github.io/font/suc.ttf) format('truetype');
  }

  :root {
    --base-font-family: 'note';
    --code-font-family: 'note';
  }
</style>
<style>
    .highlight {
         
        max-height: 400px;
        overflow: hidden;
    }

    .code-show {
        max-height: none !important;
    }

    .code-more-box {
        width: 100%;
        padding-top: 78px;
        background-image: -webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0)), to(#fff));
        position: absolute;
        left: 0;
        right: 0;
        bottom: 0;
        z-index: 1;
    }

    .code-more-btn {
        display: block;
        margin: auto;
        width: 44px;
        height: 22px;
        background: #f0f0f5;
        border-top-left-radius: 8px;
        border-top-right-radius: 8px;
        padding-top: 6px;
        cursor: pointer;
    }

    .code-more-img {
        cursor: pointer !important;
        display: block;
        margin: auto;
        width: 22px;
        height: 16px;
    }
</style>

<script>
  function initCodeMoreBox() {
    let codeBlocks = document.querySelectorAll(".highlight");
    if (!codeBlocks) {
      return;
    }
    codeBlocks.forEach(codeBlock => {
      
      if (codeBlock.scrollHeight <= codeBlock.clientHeight) {
        return;
      }
      
      
      let codeMoreBox = document.createElement('div');
      codeMoreBox.classList.add('code-more-box');
      
      let codeMoreBtn = document.createElement('span');
      codeMoreBtn.classList.add('code-more-btn');
      codeMoreBtn.addEventListener('click', () => {
        codeBlock.classList.add('code-show');
        codeMoreBox.style.display = 'none';
        
        window.dispatchEvent(new Event('resize'))
      })
      
      let img = document.createElement('img');
      img.classList.add('code-more-img');
      img.src = "https://mosfish.github.io/icons/codeMore.png"
      
      codeMoreBtn.appendChild(img);
      codeMoreBox.appendChild(codeMoreBtn);
      codeBlock.appendChild(codeMoreBox)
    })
  }
  
  initCodeMoreBox();
</script>



    </body>
</html>
