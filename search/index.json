[{"content":"Attempt to build a Long-Range specific model I feel excited to begin my research internship in NC State. Considering the outstanding article:\ngRNAde: Geometric Deep Learning for 3D RNA Inverse Design.\nThis paper is clear and friendly to such \u0026ldquo;rookie\u0026rdquo; like me. Besides, it\u0026rsquo;s easy to reproduce and the source code is released at:\nhttps://github.com/chaitjo/geometric-rna-design\nRemote server initialization operations For convenience, I rent the GPU server from AutoDL: https://www.autodl.com/, and my friend let me use his RTX4090 in U.K.\nHowever, servers in China and UK need different settings. For the AutoDL Server, I need to use the data disk autodl-tmp more instead of the system disk, for the limited memory of system disk.\nBut, that leads to a problem: if use the data disk more, the CPU’s data reading and processing become slower because the data disk is cloud-based, which causes low GPU utilization (like on an RTX 5090). So it depends.\nThe author required the environment as follows:\nPython 3.10.12 and CUDA 11.8, numpy \u0026lt;2.0\nIn fact, we need python=3.10 and CUDA 12.8(for RTX5090).\nMirror source acceleration 1 git clone https://ghfast.top/github.com/chaitjo/geometric-rna-design.git 1 cd /root/autodl-tmp/geometric-rna-design Environment If our system disk doesn’t have enough space, let\u0026rsquo; s:\n1 mamba create -p /root/autodl-tmp/rna python=3.10 1 mamba activate /root/autodl-tmp/rna And we can clean rubbish:\n1 2 pip cache purge conda clean --all For current shell, we have the followings. It is suggested that we can add it to bashrc or .env file, then source ~/.bashrc. 1 2 3 4 5 6 export PROJECT_PATH=\u0026#39;/root/autodl-tmp/geometric-rna-design/\u0026#39; export ETERNAFOLD=\u0026#39;/root/autodl-tmp/geometric-rna-design/tools/EternaFold\u0026#39; export X3DNA=\u0026#39;/root/autodl-tmp/geometric-rna-design/tools/x3dna-v2.4\u0026#39; export PATH=\u0026#34;/root/autodl-tmp/geometric-rna-design/tools/x3dna-v2.4/bin:$PATH\u0026#34; export PATH=\u0026#34;/root/autodl-tmp/cdhit:$PATH\u0026#34; PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 For UK server, this is mine:\n1 2 3 4 5 6 7 8 9 export PROJECT_PATH=\u0026#39;/home/remote1/geometric-rna-design/\u0026#39; export DATA_PATH=\u0026#39;/home/remote1/geometric-rna-design/data/\u0026#39; export WANDB_PROJECT=\u0026#39;rna\u0026#39; export WANDB_ENTITY=\u0026#39;wenxy59-sun-yat-sen-university\u0026#39; export WANDB_DIR=\u0026#39;/home/remote1/geometric-rna-design/\u0026#39; export ETERNAFOLD=\u0026#39;/home/remote1/geometric-rna-design/tools/EternaFold\u0026#39; export X3DNA=\u0026#39;/home/remote1/geometric-rna-design/tools/x3dna-v2.4\u0026#39; export PATH=\u0026#34;/home/remote1/geometric-rna-design/tools/x3dna-v2.4/bin:$PATH\u0026#34; PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 When we training the model Data pre-process Before we start, we need to run the process_data.py and obtain the process.pt , and then run the notebook to get das_split.pt.\nIn the progress, we need USalign\u0026amp;qTMclust,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #Install CD-HIT for sequence identity clustering mamba install cd-hit -c bioconda #Install US-align/qTMclust for structural similarity clustering cd ~/geometric-rna-design/tools/ git clone https://github.com/pylelab/USalign.git \u0026amp;\u0026amp; cd USalign/ \u0026amp;\u0026amp; git checkout 97325d3aad852f8a4407649f25e697bbaa17e186 g++ -static -O3 -ffast-math -lm -o USalign USalign.cpp g++ -static -O3 -ffast-math -lm -o qTMclust qTMclust.cpp # (Optional) Install ViennaRNA, mainly used for plotting in design notebook cd ~/geometric-rna-design/tools/ tar -zxvf ViennaRNA-2.6.4.tar.gz cd ViennaRNA-2.6.4 ./configure # ./configure --enable-macosx-installer make sudo make install After that, we can make a validation byUSalign -h and qTMclust -h. Pay attention to your path, such as src/data/clustering_units.py\nSuccessfully process the data We will skip any RNA molecules whose structural data are incomplete once we find out.\nproblem data Reproduce the author\u0026rsquo;s result The author offered an .py file or we can start by command like:\n1 python gRNAde.py --pdb_filepath data/raw/6J6G_1_L-E.pdb --output_filepath tutorial/lnc/po/114.fasta --split das --max_num_conformers 1 --n_samples 16 --temperature 0.5 let's run gRNAde We then test it on the whole RNASolo Dataset(both train and test) first as a rough experiment. The model’s performance drops when dealing with long RNA sequences (over 100 nts). Although the recovery drops a little, the self-consistency score for secondary structure (SC Score) shows a clear linear decline.\ncomparison metircs SC Score(left) and Recovery(right) with Sequence Another way: Diffusion Model like RiboDiffusion, RIdiffusion, DiffRNAfold .etc RiboDiffusion To reproduce RiboDiffusion, we can set the environment like this on the RTX4090:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 conda create -n rna2 python=3.10 -y conda activate rna2 pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116 pip install absl-py==0.15.0 pip install biopython==1.80 pip install dm_tree==0.1.7 pip install fair-esm==2.0.0 pip install ml_collections==0.1.1 pip install numpy==1.24.3 pip install scipy\u0026gt;=1.10.0 pip install tqdm==4.64.1 pip install torch-cluster==1.6.1+pt113cu116 -f https://data.pyg.org/whl/torch-1.13.0+cu116.html pip install torch-scatter==2.1.1+pt113cu116 -f https://data.pyg.org/whl/torch-1.13.0+cu116.html pip install torch-geometric==2.3.1 Although we can run it well and it shows a promising way to generate RNA Molecules, maybe there are some problems. Let\u0026rsquo;s look at the picture below, the recovery rate(you can understand it like \u0026ldquo;accuracy\u0026rdquo;) will often comes to 100%! And it drops up and down which seems no clear pattern.\nRiboDiffusion输出 To solve my confuse, I test it with RNASolo Data set(manually), but it seemed that is not obvious.\nRiboDiffusion模型的Recovery在对数坐标(上)与常数坐标(下)表示下随Sequence的变化 We will follow the scripts as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 import torch from tqdm import tqdm import numpy as np import random from models import * from utils import * from diffusion import NoiseScheduleVP from sampling import get_sampling_fn from datasets import utils as du import functools import tree from configs.inference_ribodiffusion import get_config config = get_config() # Choose heckpoint name checkpoint_path = \u0026#39;./ckpts/exp_inf.pth\u0026#39; # checkpoint_path = \u0026#39;./ckpts/exp_inf_large.pth\u0026#39; config.eval.sampling_steps = 100 # config.eval.sampling_steps = 100 NUM_TO_LETTER = np.array([\u0026#39;A\u0026#39;, \u0026#39;G\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;U\u0026#39;]) def get_optimizer(config, params): \u0026#34;\u0026#34;\u0026#34;Return a flax optimizer object based on `config`.\u0026#34;\u0026#34;\u0026#34; if config.optim.optimizer == \u0026#39;Adam\u0026#39;: optimizer = optim.Adam(params, lr=config.optim.lr, betas=(config.optim.beta1, 0.999), eps=config.optim.eps, weight_decay=config.optim.weight_decay) elif config.optim.optimizer == \u0026#39;AdamW\u0026#39;: optimizer = torch.optim.AdamW(params, lr=config.optim.lr, amsgrad=True, weight_decay=1e-12) else: raise NotImplementedError(f\u0026#39;Optimizer {config.optim.optimizer} not supported yet!\u0026#39;) return optimizer # Initialize model model = create_model(config) ema = ExponentialMovingAverage(model.parameters(), decay=config.model.ema_decay) params = model.parameters() optimizer = get_optimizer(config, model.parameters()) state = dict(optimizer=optimizer, model=model, ema=ema, step=0) model_size = sum(p.numel() for p in model.parameters()) * 4 / 2 ** 20 print(\u0026#39;model size: {:.1f}MB\u0026#39;.format(model_size)) # Load checkpoint state = restore_checkpoint(checkpoint_path, state, device=config.device) ema.copy_to(model.parameters()) # Initialize noise scheduler noise_scheduler = NoiseScheduleVP(config.sde.schedule, continuous_beta_0=config.sde.continuous_beta_0, continuous_beta_1=config.sde.continuous_beta_1) # Obtain data scalar and inverse scalar inverse_scaler = get_data_inverse_scaler(config) # Setup sampling function test_sampling_fn = get_sampling_fn(config, noise_scheduler, config.eval.sampling_steps, inverse_scaler) pdb2data = functools.partial(du.PDBtoData, num_posenc=config.data.num_posenc, num_rbf=config.data.num_rbf, knn_num=config.data.knn_num) # Run inference on a single p pdb_file= \u0026#39;/home/remote1/geometric-rna-design/data/raw/1FIR_1_A.pdb\u0026#39; pdb_id = pdb_file.replace(\u0026#39;.pdb\u0026#39;, \u0026#39;\u0026#39;) if \u0026#39;/\u0026#39; in pdb_id: pdb_id = pdb_id.split(\u0026#39;/\u0026#39;)[-1] config.eval.dynamic_threshold=True config.eval.cond_scale=0.4 config.eval.n_samples=16 test_sampling_fn = get_sampling_fn(config, noise_scheduler, config.eval.sampling_steps, inverse_scaler) struct_data = pdb2data(pdb_file) struct_data = tree.map_structure(lambda x:x.unsqueeze(0).repeat_interleave(config.eval.n_samples, dim=0).to(config.device), struct_data) samples = test_sampling_fn(model, struct_data) print(f\u0026#39;PDB ID: {pdb_id}\u0026#39;) native_seq = \u0026#39;\u0026#39;.join(list(NUM_TO_LETTER[struct_data[\u0026#39;seq\u0026#39;][0].cpu().numpy()])) print(f\u0026#39;Native sequence: {native_seq}\u0026#39;) for i in range(len(samples)): # native_seq = \u0026#39;\u0026#39;.join(list(NUM_TO_LETTER[struct_data[\u0026#39;seq\u0026#39;].squeeze(0).cpu().numpy()])) # print(f\u0026#39;Native sequence: {native_seq}\u0026#39;) designed_seq = \u0026#39;\u0026#39;.join(list(NUM_TO_LETTER[samples[i].cpu().numpy()])) print(f\u0026#39;Generated sequence {i+1}: {designed_seq}\u0026#39;) recovery_ = samples[i].eq(struct_data[\u0026#39;seq\u0026#39;][0]).float().mean().item() print(f\u0026#39;Recovery rate {i+1}: {recovery_:.4f}\u0026#39;) However, too slow, too tired, and I design a automatic scripts like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 import torch from tqdm import tqdm import numpy as np import random import os import sys from datetime import datetime # Add the gRNAde path to import evaluation tools sys.path.append(\u0026#39;/home/remote1/geometric-rna-design/src\u0026#39;) sys.path.append(\u0026#39;/home/remote1/geometric-rna-design/\u0026#39;) from models import * from utils import * from diffusion import NoiseScheduleVP from sampling import get_sampling_fn from datasets import utils as du import functools import tree from configs.inference_ribodiffusion import get_config # Import gRNAde evaluation functions from src.evaluator import ( self_consistency_score_eternafold, edit_distance ) from src.data.data_utils import pdb_to_tensor, get_c4p_coords from src.constants import NUM_TO_LETTER, PROJECT_PATH # Import BioPython for sequence handling from Bio import SeqIO from Bio.Seq import Seq from Bio.SeqRecord import SeqRecord config = get_config() # Choose checkpoint name checkpoint_path = \u0026#39;./ckpts/exp_inf.pth\u0026#39; config.eval.sampling_steps = 100 def get_optimizer(config, params): \u0026#34;\u0026#34;\u0026#34;Return a flax optimizer object based on `config`.\u0026#34;\u0026#34;\u0026#34; if config.optim.optimizer == \u0026#39;Adam\u0026#39;: optimizer = optim.Adam(params, lr=config.optim.lr, betas=(config.optim.beta1, 0.999), eps=config.optim.eps, weight_decay=config.optim.weight_decay) elif config.optim.optimizer == \u0026#39;AdamW\u0026#39;: optimizer = torch.optim.AdamW(params, lr=config.optim.lr, amsgrad=True, weight_decay=1e-12) else: raise NotImplementedError(f\u0026#39;Optimizer {config.optim.optimizer} not supported yet!\u0026#39;) return optimizer def extract_sec_struct_from_pdb(pdb_file): \u0026#34;\u0026#34;\u0026#34; Extract secondary structure from PDB file using gRNAde\u0026#39;s data utilities. \u0026#34;\u0026#34;\u0026#34; try: sequence, coords, sec_struct, sasa = pdb_to_tensor( pdb_file, return_sec_struct=True, return_sasa=True, keep_insertions=False ) return [sec_struct] if sec_struct else None except Exception as e: print(f\u0026#34;Warning: Could not extract secondary structure from {pdb_file}: {e}\u0026#34;) return None def prepare_raw_data_for_grnade_eval(pdb_file, native_seq): \u0026#34;\u0026#34;\u0026#34; Prepare raw data structure compatible with gRNAde evaluator. \u0026#34;\u0026#34;\u0026#34; try: sequence, coords, sec_struct, sasa = pdb_to_tensor( pdb_file, return_sec_struct=True, return_sasa=True, keep_insertions=False ) raw_data = { \u0026#39;sequence\u0026#39;: native_seq, \u0026#39;coords_list\u0026#39;: [coords] if coords is not None else [], \u0026#39;sec_struct_list\u0026#39;: [sec_struct] if sec_struct else [\u0026#34;.\u0026#34;] * len(native_seq), \u0026#39;sasa_list\u0026#39;: [sasa] if sasa is not None else [np.ones(len(native_seq))] } return raw_data except Exception as e: print(f\u0026#34;Warning: Could not prepare raw data from {pdb_file}: {e}\u0026#34;) # Return minimal raw data structure return { \u0026#39;sequence\u0026#39;: native_seq, \u0026#39;coords_list\u0026#39;: [], \u0026#39;sec_struct_list\u0026#39;: [\u0026#34;.\u0026#34;] * len(native_seq), \u0026#39;sasa_list\u0026#39;: [np.ones(len(native_seq))] } def evaluate_ribodiffusion_with_grnade_sc(samples, native_seq, pdb_file, pdb_id, output_dir=None, save_results=True): \u0026#34;\u0026#34;\u0026#34; Evaluate RiboDiffusion samples using gRNAde\u0026#39;s self-consistency evaluation. \u0026#34;\u0026#34;\u0026#34; # Prepare raw data for gRNAde evaluator raw_data = prepare_raw_data_for_grnade_eval(pdb_file, native_seq) # Convert RiboDiffusion samples to numpy arrays sample_arrays = [] for sample in samples: if isinstance(sample, torch.Tensor): sample_arrays.append(sample.cpu().numpy()) else: sample_arrays.append(np.array(sample)) # Create mask for coordinates (assume all positions are valid for now) mask_coords = np.ones(len(native_seq), dtype=bool) # Calculate basic metrics results = { \u0026#39;pdb_id\u0026#39;: pdb_id, \u0026#39;native_seq\u0026#39;: native_seq, \u0026#39;samples\u0026#39;: [], \u0026#39;recovery_rates\u0026#39;: [], \u0026#39;edit_distances\u0026#39;: [], \u0026#39;sc_scores_eternafold\u0026#39;: [] } # Convert native sequence to numerical for recovery calculation letter_to_num = {letter: idx for idx, letter in enumerate(NUM_TO_LETTER)} native_array = np.array([letter_to_num[char] for char in native_seq]) print(f\u0026#34;\\nEvaluating {len(samples)} samples for {pdb_id}:\u0026#34;) for i, sample_array in enumerate(sample_arrays): designed_seq = \u0026#39;\u0026#39;.join([NUM_TO_LETTER[num] for num in sample_array]) results[\u0026#39;samples\u0026#39;].append(designed_seq) # Calculate recovery rate recovery = (sample_array == native_array).mean() results[\u0026#39;recovery_rates\u0026#39;].append(recovery) # Calculate edit distance using gRNAde\u0026#39;s function edit_dist = edit_distance(designed_seq, native_seq) results[\u0026#39;edit_distances\u0026#39;].append(edit_dist) print(f\u0026#39;Sample {i+1}: Recovery={recovery:.4f}, Edit_dist={edit_dist}\u0026#39;) # Calculate self-consistency scores using gRNAde\u0026#39;s EternaFold evaluator print(\u0026#34;\\nCalculating self-consistency scores with EternaFold...\u0026#34;) try: sc_scores = self_consistency_score_eternafold( sample_arrays, raw_data[\u0026#39;sec_struct_list\u0026#39;], mask_coords ) results[\u0026#39;sc_scores_eternafold\u0026#39;] = sc_scores.tolist() for i, sc_score in enumerate(sc_scores): print(f\u0026#39;Sample {i+1}: SC_score={sc_score:.4f}\u0026#39;) except Exception as e: print(f\u0026#34;Warning: Could not calculate SC scores: {e}\u0026#34;) print(\u0026#34;This might be due to EternaFold not being properly installed or configured.\u0026#34;) results[\u0026#39;sc_scores_eternafold\u0026#39;] = [0.0] * len(samples) # Save results if save_results and output_dir: os.makedirs(output_dir, exist_ok=True) # Save as FASTA file compatible with gRNAde format sequences = [] # First record: input sequence with metadata sequences.append(SeqRecord( Seq(native_seq), id=\u0026#34;input_sequence,\u0026#34;, description=f\u0026#34;pdb_id={pdb_id}, ribodiffusion_evaluation\u0026#34; )) # Remaining records: designed sequences with metrics for i, (seq, recovery, edit_dist, sc_score) in enumerate(zip( results[\u0026#39;samples\u0026#39;], results[\u0026#39;recovery_rates\u0026#39;], results[\u0026#39;edit_distances\u0026#39;], results[\u0026#39;sc_scores_eternafold\u0026#39;] )): sequences.append(SeqRecord( Seq(seq), id=f\u0026#34;sample={i},\u0026#34;, description=f\u0026#34;recovery={recovery:.4f}, edit_dist={edit_dist}, sc_score={sc_score:.4f}\u0026#34; )) # Save FASTA fasta_path = os.path.join(output_dir, f\u0026#34;{pdb_id}_ribodiffusion_designs.fasta\u0026#34;) SeqIO.write(sequences, fasta_path, \u0026#34;fasta\u0026#34;) print(f\u0026#34;\\nResults saved to: {fasta_path}\u0026#34;) # Print summary statistics print(f\u0026#34;\\n{\u0026#39;=\u0026#39;*50}\u0026#34;) print(f\u0026#34;Summary for {pdb_id}:\u0026#34;) print(f\u0026#34;Native sequence length: {len(native_seq)}\u0026#34;) print(f\u0026#34;Number of samples: {len(samples)}\u0026#34;) print(f\u0026#34;Mean Recovery: {np.mean(results[\u0026#39;recovery_rates\u0026#39;]):.4f} ± {np.std(results[\u0026#39;recovery_rates\u0026#39;]):.4f}\u0026#34;) print(f\u0026#34;Mean Edit Distance: {np.mean(results[\u0026#39;edit_distances\u0026#39;]):.2f} ± {np.std(results[\u0026#39;edit_distances\u0026#39;]):.2f}\u0026#34;) if results[\u0026#39;sc_scores_eternafold\u0026#39;][0] != 0.0: print(f\u0026#34;Mean SC Score (EternaFold): {np.mean(results[\u0026#39;sc_scores_eternafold\u0026#39;]):.4f} ± {np.std(results[\u0026#39;sc_scores_eternafold\u0026#39;]):.4f}\u0026#34;) else: print(\u0026#34;SC Scores not calculated (EternaFold unavailable)\u0026#34;) print(f\u0026#34;{\u0026#39;=\u0026#39;*50}\u0026#34;) return results # Initialize model model = create_model(config) ema = ExponentialMovingAverage(model.parameters(), decay=config.model.ema_decay) params = model.parameters() optimizer = get_optimizer(config, model.parameters()) state = dict(optimizer=optimizer, model=model, ema=ema, step=0) model_size = sum(p.numel() for p in model.parameters()) * 4 / 2 ** 20 print(\u0026#39;Model size: {:.1f}MB\u0026#39;.format(model_size)) # Load checkpoint state = restore_checkpoint(checkpoint_path, state, device=config.device) ema.copy_to(model.parameters()) # Initialize noise scheduler noise_scheduler = NoiseScheduleVP(config.sde.schedule, continuous_beta_0=config.sde.continuous_beta_0, continuous_beta_1=config.sde.continuous_beta_1) # Obtain data scalar and inverse scalar inverse_scaler = get_data_inverse_scaler(config) # Setup sampling function test_sampling_fn = get_sampling_fn(config, noise_scheduler, config.eval.sampling_steps, inverse_scaler) pdb2data = functools.partial(du.PDBtoData, num_posenc=config.data.num_posenc, num_rbf=config.data.num_rbf, knn_num=config.data.knn_num) # Run inference pdb_file = \u0026#39;/home/remote1/geometric-rna-design/data/raw/7PIC_1_5.pdb\u0026#39; pdb_id = os.path.basename(pdb_file).replace(\u0026#39;.pdb\u0026#39;, \u0026#39;\u0026#39;) # Configure sampling config.eval.dynamic_threshold = True config.eval.cond_scale = 0.4 config.eval.n_samples = 16 # Generate samples print(f\u0026#39;Processing PDB: {pdb_file}\u0026#39;) test_sampling_fn = get_sampling_fn(config, noise_scheduler, config.eval.sampling_steps, inverse_scaler) struct_data = pdb2data(pdb_file) struct_data = tree.map_structure( lambda x: x.unsqueeze(0).repeat_interleave(config.eval.n_samples, dim=0).to(config.device), struct_data ) samples = test_sampling_fn(model, struct_data) # Get native sequence native_seq = \u0026#39;\u0026#39;.join(list(NUM_TO_LETTER[struct_data[\u0026#39;seq\u0026#39;][0].cpu().numpy()])) print(f\u0026#39;Native sequence: {native_seq}\u0026#39;) # Create output directory current_time = datetime.now().strftime(\u0026#34;%Y%m%d_%H%M%S\u0026#34;) output_dir = f\u0026#34;ribodiffusion_grnade_eval_{current_time}\u0026#34; # Evaluate with gRNAde\u0026#39;s self-consistency framework results = evaluate_ribodiffusion_with_grnade_sc( samples=samples, native_seq=native_seq, pdb_file=pdb_file, pdb_id=pdb_id, output_dir=output_dir, save_results=True ) Well, let\u0026rsquo; s run it with almost 13000 data to test the actually performance, and we got:\nJust do it! result and result（after log） Oh, actually, diffusion model is like buying a lottery ticket. Well, well, well, let\u0026rsquo;s move on towards geometirc or Large Language Model.\nSimple method for gRNAde One simple attention layer added to the model Since multi-layer attention mechanisms can help capture long sequences, I began by using MultiheadAttention and added a simple layer after the encoder and before the pooling set in the whole model for training. The original released code had some bugs, especially in the masking logic of the mask_coords function — it often caused dimension mismatches with the samples and threw errors. I’ve pasted my modified evaluator.py here. Also, to prevent GPU memory overflow, you’ll need to set the batch size properly.\nA simple attention，models.py is here:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #code before, Encoder layers (supports multiple conformations) self.encoder_layers = nn.ModuleList( MultiGVPConvLayer(self.node_h_dim, self.edge_h_dim, activations=activations, vector_gate=True, drop_rate=drop_rate, norm_first=True) for _ in range(num_layers)) ####################################### # Simple self-attention on pooled scalar node features, our changes self.attn = nn.MultiheadAttention( embed_dim=self.node_h_dim[0], num_heads=4, dropout=drop_rate, batch_first=True ) self.attn_ln = nn.LayerNorm(self.node_h_dim[0]) ########################################### # Decoder layers In the forward, we add:\n1 2 3 4 5 6 7 8 9 10 # edges: (n_edges, d_se), (n_edges, d_ve, 3) h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) ########################################################### # Apply simple self-attention over nodes (sequence length = n_nodes) x = h_V[0].unsqueeze(0) # (1, n_nodes, d_s) attn_out, _ = self.attn(x, x, x, need_weights=False) x = self.attn_ln(x + attn_out) h_V = (x.squeeze(0), h_V[1]) ########################################################## encoder_embeddings = h_V and in the sample forward,\n1 2 3 4 5 6 7 8 9 # edges: (n_edges, d_se), (n_edges, d_ve, 3) h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) ############################################# # Apply simple self-attention over nodes (sequence length = n_nodes) x = h_V[0].unsqueeze(0) # (1, n_nodes, d_s) attn_out, _ = self.attn(x, x, x, need_weights=False) x = self.attn_ln(x + attn_out) ############################################ h_V = (x.squeeze(0), h_V[1]) Since it’s usually hard to access wandb from local servers, and the original code relies heavily on wandb-related paths, I had to change the paths in trainer.py,\n1 2 3 4 5 6 7 8 if device.type == \u0026#39;xpu\u0026#39;: import intel_extension_for_pytorch as ipex model, optimizer = ipex.optimize(model, optimizer=optimizer) #=======same as before======= # Initialise save directory save_dir = os.path.join(os.path.dirname(__file__), \u0026#34;..\u0026#34;, \u0026#34;mymodel\u0026#34;) #save_dir = os.path.abspath(save_dir) or you can use this os.makedirs(save_dir, exist_ok=True) And we need to replace wandb.run.dir path to save_dir. Besides, we use autoaggresive as default.\nWhen we wanna train it, let\u0026rsquo; s keep it moving:\n1 2 3 nohup python main.py --no_wandb \u0026gt; main.log 2\u0026gt;\u0026amp;1 \u0026amp; tail -f main.log nvidia-smi Get our first model We finally getbest_checkpoint.h5 as the trained model，then, following the gRNAde.py，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 CHECKPOINT_PATH = { \u0026#39;all\u0026#39;: { 1: os.path.join(PROJECT_PATH, \u0026#34;mymodel/best_checkpoint.h5\u0026#34;),#change 2: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_2state_all.h5\u0026#34;), 3: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_3state_all.h5\u0026#34;), 5: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_5state_all.h5\u0026#34;), }, \u0026#39;das\u0026#39;: { 1: os.path.join(PROJECT_PATH, \u0026#34;mymodel/best_checkpoint.h5\u0026#34;),#change 2: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_2state_das.h5\u0026#34;), 3: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_3state_das.h5\u0026#34;), 5: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_5state_das.h5\u0026#34;), }, \u0026#39;multi\u0026#39;: { 1: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_1state_multi.h5\u0026#34;), 2: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_2state_multi.h5\u0026#34;), 3: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_3state_multi.h5\u0026#34;), 5: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_5state_multi.h5\u0026#34;), } } Attention, sometimes sec_struct_utils.py will automatically remove something before we process it，so we just remove the code:\n1 2 3 4 5 6 7 8 9 10 output = subprocess.run(cmd, check=True, capture_output=True).stdout.decode(\u0026#34;utf-8\u0026#34;) # Delete temporary files这三行注释掉 # if sequence is not None: # os.remove(fasta_file_path) if n_samples \u0026gt; 1: return output.split(\u0026#34;\\n\u0026#34;)[:-1] else: return [output.split(\u0026#34;\\n\u0026#34;)[-2]] then,\n1 2 3 4 5 current_datetime = datetime.now().strftime(\u0026#34;%Y%m%d_%H%M%S\u0026#34;) try: fasta_file_path = os.path.join(wandb.run.dir, f\u0026#34;temp_{current_datetime}.fasta\u0026#34;) except AttributeError: fasta_file_path = os.path.join(PROJECT_PATH, \u0026#34;temp\u0026#34;, f\u0026#34;temp_{current_datetime}.fasta\u0026#34;)#改这里路径后是这样，我一般不用wandb It works, I test it just as epoch = 1 but the performance is bull shit.\nOMG, such epoch1 Then we improve it to 50epochs, like the author\u0026rsquo;s default, it performed well. However, in the trainer.py, the author train it without early stopping method. So I add it to the script.\nepoch=50 test it well Then I am thinking about multi-scale attention, from Atlas: Multi-Scale Attention Improves Long Context Image Modeling. So, I added 3 layers of attention, but not multi-scale.\nMulti-layer attention or multihead attention? Test it on the 500 dataset, grnade is：\nLength Range Statistic Perplexity Recovery Edit Distance SC Score Sample Count 0-100 Median 1.4128 0.56445 17.5972 0.62315 234 100-200 Median 1.26885 0.73585 30.2634 0.61495 72 200+ Median 1.3132 0.64605 226.875 0.42835 138 0-100 Mean 1.47854274 0.569795726 18.55480427 0.634651282 234 100-200 Mean 1.34514583 0.685369444 37.49759306 0.569984722 72 200+ Mean 1.41243696 0.639286232 211.1403986 0.421031884 138 multiheadattention:\nLength Range Statistic Perplexity Recovery Edit Distance SC Score Sample Count 0-100 Median 1.30675 0.5532 17.21875 0.62105 234 100-200 Median 1.17275 0.7444 29.75 0.62205 72 200+ Median 1.30785 0.70485 183.84375 0.41915 140 0-100 Mean 1.36121325 0.572758974 18.34507564 0.638930342 234 100-200 Mean 1.24458056 0.704726389 35.62898472 0.605529167 72 200+ Mean 1.36845071 0.685876429 180.6455357 0.430865714 140 Multi-layer attention：\nLength Range Statistic Perplexity Recovery Edit Distance SC Score Sample Count 0-100 Median 1.3358 0.5592 17.6667 0.6396 235 100-200 Median 1.2226 0.71205 33.78125 0.6771 72 200+ Median 1.32545 0.68665 193.0625 0.42465 140 0-100 Mean 1.36930766 0.557402979 18.98926 0.660275319 235 100-200 Mean 1.28780417 0.682672222 37.93399306 0.624776389 72 200+ Mean 1.40186786 0.673555 187.8361607 0.434380714 140 We then compare the SCC，Recovery.\n加了注意力的两个模型实际在表现上都比原版好，现在就要取舍了 After that, we test grnade and multi-layer attention on 12000+data,\nSCC improved Data leakage issue came up The author had over 13,000 samples (from RNAsolo 2023), with about 12,000 usable ones. Among them, the training split has 12,000+, the validation set has about 500+, and only around 200 are in the test set.\nWhat’s that supposed to mean? 12,000 : 500 : 200 — that’s nowhere near the usual 8:1:1 split! And it gets even worse — those 200 test samples don’t even go beyond a length of 200. Like, what am I supposed to test with that?\ntest set,0-160 length range So we need to optimize this split method. My idea is to first split the clusters based on length ranges — 0–100, 100–200, and 200+. Then, within each range, I’ll select clusters using an 8:1:1 ratio. Finally, I’ll extract the PDB indices contained in each cluster. That way, it should make our experiments more balanced and meaningful.\ntrain split， range, 12000+data validation split, range,450data test split, almost 660 data Meanwhile, RNASolo has 3000+ more data in 2025 than 2023. Maybe we can use this? But let\u0026rsquo;s focus on our primary 2023 dataset to train and test equally.\n2025 RNASolo Optimization methods and experiments After I evenly split the test set, I first retrained and tested the gRNAde model. The results were a bit unstable, but it wasn’t a big deal. Simple plot below:\nresult in test split After that, I trained gRNAde with 100 epochs and early stopping methods with seed = 0, 1, 2 . Besides, I found that the previous attention approach helped capture long-range interactions, so I’m planning to optimize the GVP layer next.\nGraph-Mamba Mamba Different from RNN, the SSM(State Space Model) will \u0026ldquo;remember\u0026rdquo; the graph or text interactions better.\nFor it promising performance,\nSSMs have rich theoretical properties but suffer from high computational cost and numerical instability. So HiPPO(High-order Polynomial Projection Operator) is proposed to optimize the SSM, as follows,\nNotably, it is developed into the S4 model, which shows its strong capability to capture long-range dependencies.\nGraph-Mamba: Towards Long-Range Graph Sequence Modeling withSelective State Spaces From the Graph-Mamba, we will know that,\nSSM：State Space Models, linear.\nBased on GraphGPS.\nGraph-Mamba-Block(GMB) replaced Attention.\nMPNN (Message Passing Neural Network)—GatedGCN.\nGraphGPS(MPNN)+GMB = Graph-Mamba.\nOptimization: Node Prioritization; Permutation-based Training.\nIn the same way, we just follow the algorithm and replace the gvpcov layer to our graphgpsMamba.\nThe code will load GatedGCN as MPNN and combine it with graphGPS then add mamba block. However, graphGPS, or more specifically, gatedGCN, is not proposed to model our RNA molecules. So in our test, it performs worse than baseline.\nLong-Short Transformer and Graph Long-Short Transformer We will learn from the article:\nLong-Short Transformer: Effcient Transformers for Language and Vision\nIt shows a promising way to combine the different attention output. Actually, for the detail,\nLong-Short Transformer\nShort-term attention via sliding window\nLong-range attention via dynamic projection\nAggregating Long-range and Short-term Attentions\u0026amp;DualLN\nWe then develop it into Long-short Graph Transformer, using MultiGVPConv for message passing, and sliding window for short attention, dynamic projection for long range. In the code, we mixed the edge features to node and processed the node scalar features, because the author\u0026rsquo; s model from long-short transformer is 1D information for sentence modeling.\nUnluckily, although it outperformed than any others in 200+ nts long range rna modeling, the short and medium range would drop down.\nGAT-GVP long-short graph transformer According to Graph Attention Networks, we changed the short range to the GAT to capture the neighbor interactions, maintain the 3D information.\nExperiments Learning rate We use our graph transformer which just adds a transformer into the layer, training with different lr, and finds out that when lr=0.0002, it will be the best.\nComparison We trained with seed0, and,\nAfter that, we chose some of them to train 3 seeds as seed=0,1,2, then:\nOther Paper Reading Off-topic story — So last week I was totally slacking off when I suddenly saw an email from my advisor’s Zoom: “xxx invites you\u0026hellip; at 11:30.” I thought maybe the meeting time had changed. But then I noticed the original 9:30 link was still active, not canceled.\nSo on the meeting day, the theory slides in my PPT still weren’t done, and I figured I’d hop into the 9:30 call just to check—you know, just in case they actually started then. And sure enough, I heard: “Alright, let’s get started.”\n…And that’s how I GAME OVER\u0026hellip;\nThis week I’m not doing anything fancy — just reading papers properly. Gotta figure out how to tell a story in the next report, right? Anyway, I’ll jot down a few things here that I think are somewhat insightful.\nA Hyperbolic Discrete Diffusion 3D RNA Inverse Folding Model for functional RNA design。 Abstract\u0026amp;Introduction Background: revolutionary opportunities for diverse RNA-based biotechnologies and biomedical applications. primary goal of RNA inverse folding is to design nucleotide sequences that can fold into a given RNA secondary or tertiary structure.\nChallenge: limited availability of experimentally derived 3D structural data and unique characteristics of RNA 3D structures; existing tools either focus on 2D inverse folding tasks or learn limited 3D structural features from experimentally determined and predicted 3D structure datasets for the 3D inverse folding problem. High-resolution RNA structures are significantly less common compared to those of proteins. RIdiffusion, a hyperbolic denoising diffusion generative RNA inverse folding model.\nRIdiffusion efficiently recovers the distribution of nucleotides for targeted RNA 3D structures based on limited training samples using a discrete diffusion model.\nHonestly, the background part is usually just the same old recycled stuff, but different papers tend to focus on different challenges. What really stood out to me in this one is how it combines geometric deep learning with a discrete diffusion model — plus, the evaluation is super thorough. You can tell they really put in the work on this one.\nstructure based. the promise of creating novel functional RNAs. The “low-sample” challenge underscores the importance of learning efficiency in generative models in 3D RNA inverse folding tasks.\nDiffusion: modeling inverse folding problem as diffusio ndenoising.\nRNA structures exhibit inherent hierarchical organization (e.g., from base-paring, secondary structure elements and remote pseudoknots to complex 3D shapes)——hyperbolic space.\nWhy? Because:\ncompared to Euclidean space, a hyperbolic distance better captures molecular differences and improve performance in small molecule generation tasks\nIn other words, this HB metric captures structural features better than the Euclidean one — or you could say it reflects the more subtle structural differences more effectively. The authors illustrate this with the following figure:\nWe can mainly focus on panels A and D. Panel A shows a schematic of the transformation, while panel D visualizes how the RNA structures are distributed in the HB space. Simply put, different chains (or backbones) are separated more clearly there — which means the features can be captured more effectively.\nabout the model hyperbolic equivariant graph neural networks (HEGNNs) to parameterize the discrete diffusion model, and effectively capture the 3D structural characteristics by incorporating RNA’s geometric features and topological properties into the generation process.\nframework：\nMATERIALS AND METHODS I would just understand the Hyperbolic Equivariant Graph Denoising Network, Normally, models tend to lose some information when dealing with this kind of 3D data.\nSo the author utilizes SE(3) equivariant neural layers from Equivariant Graph Neural Networks (EGNN).49 This approach allows the model to preserve SO(3) rotational equivariance and E(3) translational invariance when updating the representations of nodes and edges, thereby retaining geometric information and ensuring the robustness and effectiveness of the hidden representations, to enable the EGNN network to update edge.其实就是加了个层。\nDatasets and Setup So how the author split？\nWe followed the same splitting methods outlined in RDesign to ensure a high-quality dataset with only experimentally derived structures, which consists of 1773 structures for training, 221 structures for validation, and 223 structures for testing in an 8:1:1 ratio.\nAccording 8:1:1.\nIn addition, to evaluate the generalization ability, the authors used PSI-CD-HIT to cluster the data based on nucleotide similarity, setting similarity thresholds of 0.8, 0.9, and 1.0。\nEvaluation Metrics Besides Recovery Rate, there is Macro-F1 score,\nThe Macro-F1 score is used to assess the accuracy and comprehensiveness of the model in predicting protein or RNA sequences, particularly in cases where the distribution of different letter residues is imbalanced, by evaluating the Macro-F1 score for each residue in the sequence.\n$$ NoveltyScore(x_i) = 1.0 - RecoveryRate(x_i,x_{mostsimilar}) $$There’s also a GSN (Global-scale Novelty) metric, where 100 samples are drawn using RR-based stratified sampling.\nAnd there’s another one, quite similar to SCC — it’s based on structures folded with RhoFold, then structural similarity is computed using US-align, which gives the RMSD value.\nBaselines 1 RNA tertiary structure inverse folding model, 3 protein tertiary structure inverse folding baselines, 1 Transformer-based graph GNN network for graph structure: gRNAde, PiFold, StructGNN, GVP-GNN, and GraphTrans.\nAuthor of RiboDiffusion\u0026amp;RhoDesign provided no training scripts in the released code.\nResults we divided the dataset of seq-0.8 into short (\u0026lt;50nt), medium (50~100nt), and long (\u0026gt;100nt)\nEmm\u0026hellip; is that long???\nThe novelty of generative designs can be viewed from two perspectives: sequence novelty and physicochemical properties novelty. In our case, sequence and physicochemical properties (derived from sequence) novelty are important in RNA 3D inverse folding. Indeed, sequence features such as physicochemical properties play a crucial role in functional RNA design. For instance, it was reported that natural RNAs show privileged physicochemical property space which is related to their biological functions. Other sequence features like GC content are important properties related to developability of RNA molecules. Given the nature of one-to-many relationship between the targeted RNA structure and its possible sequences, the novelty and diversity of generated sequences are important considerations for improving physicochemical properties of the designed sequences.\nphysicochemical properties space, evaluation,\nBeyond RNA Structure Alone: Complex-Aware Feature Fusion for Tertiary Structure-based RNA Design Abstract important in synthetic biology and therapeutics limitation: overlook the role of complex-level information In fact, the authors argue that analyzing RNA alone isn’t enough — you also have to look at the related complexes.\nour method incorporates protein features extracted by protein language model (e.g., ESM-2)\nTo address bio-logical complexity of protein-RNA interactions, propose a distance-aware filtering for local features from protein representation.\na high-affinity design framework that combines our CARD with an affinity evaluation model.\nFinal goal: produce high-affinity RNA sequences\nIntroduction more sophisticated computational approaches that can model the complex relationship between RNA sequences and their structures. The authors also mentioned LEARNA and Meta-LEARNA, which are RL-based, as well as RDesign and RhoDesign, which are based on 3D graph representations.\ndifferent from protein folding which roughly follows Anfinsen’s rule, RNAs are highly flexible and rely on interactions with proteins, DNA, and other biomolecules to achieve folding. RNAs may adopt various conformations by interacting with distinct macromolecules at different stages of functioning. At different stages, they also bind with macromolecules. This paper considers both RNA structural features and protein–RNA interactions to generate sequences comprehensively.\nThe outline, as shown in Figure 1, includes a pre-trained protein language model (PLM) to handle proteins, and a Complex-Aware Transformer (CAFormer) to integrate RNA and protein features. They use distance-aware filtering to select relevant regions while ignoring unrelated areas. This setup allows the model to capture the context of protein–RNA complexes and focus on the interaction regions within the complex.\nOverall, the authors iteratively screen the generated RNA sequences: candidate sequences are first filtered based on predicted affinity scores, and then structural compatibility is validated using multiple folding models.\nAffinity Evaluation\nRegarding “affinity”, the authors mention predictive models like PNAB, DeePNAP, PredPRBA, PRdeltaGPred, and PRA-Pred.\nHowever, structural data for RNA–protein complexes is very limited. CoPRA integrates PRA310 and introduces a multi-modal model, which combines RNA and protein language models with comprehensive structural information.\nAdditionally, the authors also discuss the development and background of inverse folding:\nRNA design aims to generate sequences that fold into predefined structures. Early methods focused on secondary structure optimization, using thermodynamic parameters and energy minimization. Tools like RNAfold predict RNA secondary structures based on the minimum free energy principle. As understanding of RNA structure has advanced, focus has shifted to complex tertiary structure-based design due to RNA’s high conformational flexibility, which challenges traditional thermodynamic methods. Recent deep learning approaches for RNA tertiary structure design include gRNAde, which utilizes geometric deep learning and graph neural networks to generate RNA sequences; RiboDiffusion, a diffusion model for inverse folding leveraging RNA backbone structures; RDesign, which employs a data-efficient learning framework with contrastive learning for tertiary structures; and Rhodesign, focusing on RNA aptamer design by guiding sequence generation through structural predictions.\nMethods It’s actually quite comprehensive. Regarding that filter, since a single protein can bind RNA through multiple sites (different conformations), and a single RNA can also interact with different proteins, the authors provide the following figure to illustrate this:\nCOMPLEX-AWARE ATTENTION BLOCK\nFor this part, the authors first concatenate the RNA and the filtered protein information, and then use N multi-head self-attention layers to enhance the model’s ability to capture contextual interactions.\nAfter that, they use a Transformer-based decoder, similar to RhoDesign, to generate RNA sequences. The decoding is framed as a next-token prediction task, where the decoder predicts the next nucleotide based on the previously generated ones. During training, the model is optimized using cross-entropy loss.\nHigh-Affinity RNA Design Framework\nstructure-to-sequence design model and evaluation tools. First, RNA sequences are designed using the authors’ model to meet complex-specific constraints (like binding sites and interactions). The sequences are then evaluated with ensemble regression models trained on the PRA201 dataset, while their structures are folded using AlphaFold3, RhoFold, and RoseTTAFold2NA to calculate RMSD. In each iteration, the top 10%–20% of sequences are selected, and the process is repeated until an optimal set is obtained.\nExperiments Dataset and Implementation Details\nDatasets: PRI30K and PRA201. The latter is used for the blind test, while the former removes structurally similar cases for training and testing. After processing, the dataset contains 21,050 protein–RNA pairs and 2,309 RNA sequences. Clustering is also done using CD-HIT with an 80% similarity threshold. 200 epochs, batch size of 48 (24 per GPU), attention blocks N = 6, local filtered size K = 64, ESM-2 650M. Comparison: SeqRNN and SeqLSTM, StructGNN, GraphTrans, RDesign, and RhoDesign. Ablation Studies\nkey components of method, including the number of attention blocks in CAFormer, the fashion of filtering the protein representation, and the choices of protein representation model.\nRiboDiffusion: tertiary structure-based RNA inverse folding with generative diffusion models Abstract:\nscarcity of data, nonunique structure-sequence mapping, flexibility of RNA conformation.\ngrowing applications in synthetic biology and therapeutics;\ninverse folding problem;\nIntroduction:\nRNA-based biotechnology;\nearly methods for focus on folding into RNA secondary structures; Some use efficient local search strategies guided by the energy function; Or globally by modeling the sequence distribution or directly manipulating diverse candidates;\nDAS physically based approach, but still constrained by local design strategy and computational efficiency.\n\u0026ndash; only 2D, without considering 3D structures of RNA\nMethods:\nDiffusion;\nThree-atom coarse-grained representation including the atom coordinates of C4’, C1’, N1 (pyrimidine) or N9 (purine) for every nucleotide;\nConsider the RNA inverse folding problem as modeling the conditional distribution p(S|X);\nBased on the GVP-GNN architecture;\nResults\nDataset: predicting structures with RhoFold. The structures predicted from RNAcentral sequences are filtered by pLDDT to keep only high-quality predictions, resulting in 17 000 structures.\nCluster: sequence\u0026ndash;PSI-CD-HIT to cluster sequences based on nucleotide similarity. We set the threshold at 0:8=0:6=0:4 and obtain 1252=1157=1114 clusters; structure similarity clustering, calculate the TM-score matrix using US-align and apply scipy, achieve 2036=1659=1302 clusters with TM-score thresholds of 0:6=0:5=0:4. 15% for testing, 10% for validation.\nEvaluation Metrics:\nRR, F1 Score(RNAfold and RMSD)\nLearning to Design RNA Abstract:\nBased on deep reinforcement learning;\nJointly optimize over a rich space of architectures for the policy network, the hyperparameters of the training procedure and the formulation of the decision process.\nIntroduction:\nLEARNA: deep RL algorithm for RNA Design. Given a target secondary structure, can predict the entire RNA sequence. After generating an RNA sequence, then folds this and locally adapts it, and uses the distance of the resulting structure to the target structure as an error signal for the RL agent;\nMeta-LEARNA: learns a single policy across many RNA Design tasks directly applicable to new RNA Design tasks;\nThe first application of architecture search (AS) to RL;\nNew benchmark dataset with an explicit training, validation and test split;\nFaster.\nTHE RNA DESIGN PROBLEM:\nFolding algorithm: zuker;\nLoss Function: Hamming diastance.\nLEARNING TO DESIGN RNA:\nAction Space, State Space, Transition Function, Reward Function;\nOne problem of current deep reinforcement learning methods is that their performance can be very sensitive to choices regarding the architecture of the policy network, the training hyperparameters, and the formulation of the problem as a decision process.\nRL often yield noisy or unreliable outcomes in single optimization runs, (a) The number of unsolved sequences, (b) the sum of mean distances, (c) the sum of minimum distances to the target structure.\nRNA-DCGen: Dual Constrained RNA Sequence Generation with LLM-Attack Challenge: recent diffusion and flowmatching-based models face two key limitations: specialization for fixed constraint types, such as tertiary structures, and lack of flexibility in imposing additional conditions beyond the primary property of interest.\nGenerative \u0026ndash;\u0026gt; Search;\nDual Constrained: on the sequence itself and on specified constraints.\n","date":"2025-06-04T15:36:23+08:00","permalink":"https://mosfish.github.io/p/deep-learning-attempt-for-rna-inverse-design./","title":"Deep Learning attempt for RNA inverse design."},{"content":"Free5GC核心网部署与接入测试手册 写在开篇： 建议养成备份（例如git工具）的好习惯。本文包含Linux开发基础内容、free5gc核心网\n考虑到版权问题，本文章如需要转载请进行声明。\n起初使用VMware虚拟机装的Ubuntu桌面版系统，但是实际工程需要板载Linux系统（但是学习初期直接上手，容易因为不熟悉操作而出现诸如网卡连不了网，没声音，拼音输入法等软硬件不适配类似的各种问题）可以先基于虚拟机进行学习，入门后再基于板载Linux系统开发核心网。\n5G核心网介绍 参考 【5G】5G 核心网（5GC）基本架构_5g核心网架构-CSDN博客\n核心网具体结构 5G 核心网是支持 5G 移动通信的核心基础设施，在 5G 通信中负责实现数据处理、控制、管理、路由和连接等功能。5G 核心网的架构采用了基于服务的设计 (Service-based Architecture, SBA)，支持网络切片、NFV、软件定义网络 (Software-Defined Networking, SDN) 等先进技术，提供更高的灵活性和可扩展性。\n5G网络架构与接口 5G核心网网元功能 AMF (Access and Mobility Management Function) 接入与移动性管理功能，主要负责：\n接入控制、用户移动性管理 会话管理和用户身份管理 处理UE与网络的接入请求 管理用户接入状态及移动性支持 SMF (Session Management Function)\n会话管理功能，负责：\n会话的建立、修改和释放 数据流管理（路由、QoS保证、IP分配） UPF (User Plane Function) 用户面功能，负责：\n转发用户数据流 与SMF协作管理数据路径、路由和转发 NSSF (Network Slice Selection Function) 网络切片选择功能，负责：\n根据业务需求动态选择网络切片 管理不同业务场景的切片资源 PCF (Policy Control Function) 策略控制功能，负责：\n制定网络策略（QoS、计费、流量管理、安全性） 为AMF/SMF/UPF提供策略决策服务 UDM (Unified Data Management) 统一数据管理功能，负责：\n存储用户订阅信息（SUPI）、服务配置、认证信息 与AUSF/AMF等网元协作提供数据服务 5G网络组成 架构组成：5GC（核心网） + RAN（无线接入网） + UE（用户设备） 核心网功能：服务管理、数据传输、安全性 RAN功能：无线连接、数据传输与控制信号传递 UE角色：用户交互终端，通过RAN与核心网通信 无线接入网与核心网接口 主要接口及功能：\nN1接口\n连接：UE ↔ AMF 功能：信令传递（UE接入管理、认证） N2接口\n连接：AMF ↔ SMF 功能：会话管理协作（处理用户会话任务） N3接口\n连接：无线接入网 ↔ UPF 功能：用户数据传输（数据面主要路径） N4接口\n连接：SMF ↔ UPF 功能：用户数据路径选择与管理（SMF控制UPF流量） N6接口\n连接：UPF ↔ 数据网络（DN） 功能：用户数据外发至互联网/外部网络 linux开发基本配置与指令 一、常用指令 1.基本指令部分 复制 Ctrl+Shift+C\n粘贴 Ctrl+Shift+V\n更新包列表\n1 sudo apt update 查网卡ip\n1 2 Ifconfig # 记得提前安装ifconfig网络工具 sudo apt install net-tools 或者\n1 ip a 显示当前路径\n1 pwd 进入文件夹\n1 2 3 cd \u0026lt;文件夹名\u0026gt; # 例如 cd free5gc-compose # 或者进入多个内嵌文件夹如 cd ~/free5gc-compose/config 查询网站ip地址\n1 nslookup baidu.com 查各网元运行情况、地址\n1 2 docker-compose ps 或看全部网元： docker ps -a 停止容器网元运行\n1 docker-compose down 启动容器网元并后台挂机\n1 docker-compose up -d 查看运行日志（以amf为例，可换成smf、upf等）\n1 docker logs \u0026lt;id\u0026gt; 例如：docker logs amf\n查找单个网元信息（例如amf）\n1 docker inspect amf 进入容器进行交互\n1 docker exec -it \u0026lt;容器ID或容器名(如amf)\u0026gt; sh 或者docker exec -it \u0026lt;容器ID或容器名(如amf)\u0026gt; bash\n按照格式查找所有网元的ip地址（没有启动或者出错就不显示）\n1 docker inspect -f \u0026#39;{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; $(docker ps -aq) 查询所有网元mac地址\n1 docker inspect -f \u0026#39;{{.Name}} - {{range .NetworkSettings.Networks}}{{.MacAddress}}{{end}}\u0026#39; $(docker ps -aq) 我的地址（每个人的都不同，根据实际情况可以修改）： /home/wxy/free5gc-compose/config/amfcfg.yaml\n查看端口情况\n1 etstat -tuln | grep 38412 看防火墙设置\n1 sudo iptables -L 看网桥\n1 brctl show 查看网桥连接的ip地址\n1 brctl showmacs \u0026lt;网桥名\u0026gt; 看路由\n1 ip route show 添加网桥\n1 sudo brctl addif \u0026lt;bridge\u0026gt; \u0026lt;网卡名\u0026gt; Linux启动wireshark抓包——需要安装linux的wireshark然后启动：\tsudo wireshark\n2.网络空间测试 一般是用docker inspect amf或者docker logs amf来对amf、smf、upf之类的进行测试，\n也可以进入网络空间进行进一步测试例如：\n1 docker exec -it upf bash 不过最好是利用pid方法进入网络空间进行ifconfig或者ping操作\n1 2 3 ps aux | grep upf #获取进程（可以得到各种进程id） pgrep -f upf #获取pid： sudo nsenter -t \u0026lt;pid\u0026gt; -n #进入网络空间 其余指令还有kill之类的可以终止进程。\n除此之外有的时候ping网站可能会遇到name resolution的报错，可以换成ip地址，指令如下，以百度为例：\n1 nslookup baidu.com 不过ping命令有的时候会被网页阻拦，用curl命令比较合适，例如：\n1 curl www.baidu.com 3.启动环境部分 启动核心网和ueransim环境分以下三步，根据实际情况可以更改代码：\n启动核心网环境\n1 2 cd ~/free5gc-compose sudo docker-compose up -d 启动UERANSIM的gnb\n1 2 cd ~/UERANSIM/build ./nr-gnb -c ../config/free5gc-gnb.yaml 启动ue\n1 2 cd ~/UERANSIM/build sudo ./nr-ue -c ../config/free5gc-ue.yaml 虚拟网卡测试方法\n1 2 ping www.baidu.com -I uesimtun0 # 或者 curl www.baidu.com -I uesimtun0 二、虚拟机Linux设置vpn方法 途径1.主机共享vpn 参考：VMware Ubuntu虚拟机 使用主机VPN 配置（简单、可行）-CSDN博客\n（图片资源没保存，所以只有文字了）\n一定要记得开篇说的资源备份啊呜呜呜 打开虚拟网络编辑器，可以看到虚拟机开启的NAT模式是VMnet8。\n主机windows+R输入cmd打开命令行输入ipconfig ，得到VMnet8的ip：192.168.154.1。之后查询vpn端口，以clash为例，右上角记下vpn的端口，之后允许局域网记得打开。\n在虚拟机的设置中打开proxy代理，选择manual，\n虚拟机网络地址与VPN Port需要根据本机确定，左边四个填VMnet4的ip，右边四个填vpn的端口即可。\n问题1：如果我的vpn没有用clash，不知道vpn端口怎么办？ 参考：在windows中如何查看代理的地址和端口_怎么查看自己电脑的代理地址-CSDN博客\nWindows+R输入control打开主机的控制面板，点击「网络和Internet」，\n在「internet选项」中点击「连接」的「局域网设置」\n启用代理服务器，不用打勾，保持默认就行，右下角端口即为vpn端口。\n问题2：为什么我的vpn它通不了外网？ 一般而言，如果浏览器可以打开谷歌就证明梯子没问题。那么问题就出在——实际命令行终端走代理需要进一步配置端口\n已知代理端口（例如7890），之后就set一下http、https两个协议走这个端口，类似于\n1 set http_proxy=http://127.0.0.1:7890 \u0026amp; set https_proxy=http://127.0.0.1:7890 但是具体还是得看具体的设备，这种事情可以问一下ai。\n途径2.linux里下载Clash csdn里有相关内容，请自行搜索，配置起来较麻烦，\nLinux下Clash翻墙完整使用指南 | Clash中文教程网。\n这个教程写的不是很清楚，可以拓展一下。\n三、主机与虚拟机共享剪贴板 参考：主机和VMware虚拟机间共享剪贴板方法_vmware workstation 中启用了剪贴板共享功能-CSDN博客\n为啥主机复制过去虚拟机粘贴不了？有的时候可以复制粘贴有的时候不能？一般来说，先设置虚拟机，需要保证客户机隔离里面是启用复制粘贴的（默认启用）\n如果这个时候还不行，可以需要安装虚拟机增强工具包open-vm-tools和open-vm-tools-desktop，安装命令：\n1 2 sudo apt-get install open-vm-tools sudo apt-get install open-vm-tools-desktop 然后重启系统：\n1 sudo reboot 四、虚拟机关机与挂起 虚拟机关机就是不再占用资源，但是再次开启需要重启；挂起就是虚拟机保存挂起的状态，打开不需要重启。\n最重要的事情是记得自己虚拟机的路径.\n问题1：关机后找不到虚拟机咋办 点击扫描虚拟机，找到.vmx文件打开就行。\n五、虚拟机分盘扩容操作 有的时候给虚拟机分盘内存太小，当盘快满的时候会弹出警告，一定要提前采取措施来分盘，否则会启动不了虚拟机。\n1 lsblk #检查磁盘情况 1 df -h\t#检查磁盘占用情况 可以看到我主要的盘sda3占用了53%。\n分盘主要是两个操作\n划出内存\n编辑虚拟机设置-硬盘-拓展，然后选择扩展后的硬盘空间\n分配空间\n考虑两种方法可以，有一个是类似于windows的图形化分盘，下载并启动gparted，这个方法在VMware虚拟机扩容磁盘，有很详细图文_虚拟机硬盘空间-CSDN博客有提及。\n1 2 3 sudo apt-get update sudo apt install gparted sudo gparted 在图形界面中可以选择 /dev/sda3 分区，然后调整它的大小。\n或者利用fdisk（推荐），输入命令：\n1 sudo fdisk /dev/sda 再输入p得到分区列表预览：\nfdisk 中执行命令汇总如下\n输入 p 显示当前的分区列表，记下 /dev/sda3 的起始位置（例如 1054720）。\n输入 d 删除 /dev/sda3，并确保输入分区号（3）。注意，这不会删除数据，分区号只是从分区表中移除。\n输入 n 创建新分区，选择 3 为分区号，使用 1054720为起始扇区（与之前一致），并设置结束扇区为新的磁盘容量（可以选择最大可用空间）。\n输入 w 保存更改并退出 fdisk。\n可能遇见这种情况，但是如果默认系统是linux一般用不上：输入 t 设置分区类型，选择 83（Linux 文件系统）。\n问题1：虚拟机退出后再启动会卡在初始页面 类似于这样，进不去虚拟机，一直卡在这个地方：\n因为没注意虚拟机盘满了，卡在初始界面进不去，所以要在盘快满的时候提前分盘。\nVMware卡在命令行/dev/sda3 clean\u0026hellip; 界面不动的解决办法：\n参考https://blog.csdn.net/SunshineLiy/article/details/134372529\n显示分区已满，需要找到这个分区一些没用的文件删掉，先进去虚拟机能分盘再说。\n进入 grub 模式，重启虚拟机，在显示到这个界面的时候\n快速按下 Shift 不要松，直到出现grub界面（这一步需要反应快一些，多试几遍就可以，如果操作正确的话，grub界面很快就会出来，如果按Shift三秒没有出现，大概率是操作错误了。此方法如果进不去，也可以参考网上也有其他进入grub的方法）\n进入ubuntu(高级模式)，选择第一个advanced\n之后选择第二个\n选择root\n选择root后会弹出两行提示，直接回车就可以\n输入以下指令看到磁盘使用情况\n1 df -h sda3确实100%满了，所以我们需要在命令行来清理一些文件。\n查看根目录下所有文件和目录的磁盘使用情况\n1 du -sh /* 找到自己占用空间较多的文件目录\n1 rm -f /文件名/ 删除那些暂时没有用的大文件，reboot 重启。然后赶紧增加空间！！！\n六、Ubuntu桌面相关问题 1.桌面字标大小 调整分辨率和缩放就行。缩放到200%左右比较合适，在setting里面设置。有的时候会有桌面越来越小的bug，在设置里面改就可以。\n2.桌面卡死 输入以下命令重启桌面等一会就行：\n1 sudo restart lightdm 七、虚拟机没网设置网络图标消失 如果只是网络连接处显示问号，可以在终端输入：\n1 sudo vim /var/lib/NetworkManager/NetworkManager.state 保证这里是true。重启虚拟机，检查情况。如果还没有反应可以重启主机，再排查\n参考彻底解决VM ubuntu在虚拟机找不到网卡无法上网的问题 - 知乎\n还是不行在setting里面找network关了重开，而如果第一行开启按钮没有显示的话可能需要用以下方式：\n1 2 3 sudo service NetworkManager stop sudo rm /var/lib/NetworkManager/NetworkManager.state sudo service NetworkManager start 最后的办法是还原默认设置后再看看有没有，没有的话再进行上一步输入命令。\nFree5gc部署、UERANSIM安装 主要参考：Free5gc+UERANSIM模拟5G网络环境搭建及基本使用 - FreeBuf网络安全行业门户\n此处采用docker容器化部署，其他独立化部署可以看教程，根据实际情况同时参考教程和本文档。\n考虑实际情况，建议这里采用镜像源代理的方式，暂且不用vpn\n一、Free5gc部署 准备工作 使用命令：\n1 uname -a 此命令用于确认安装的虚拟机内核版本，要求的内核版本为5.0.0-23-generic或5.4.0及之后的版本,\n若当前虚拟机内核版本不符合要求，则需要更换内核，使用以下命令安装5.0.0-23-generic的内核，注意用sudo是管理员权限的操作，需要输入密码，为了安全，密码一般是不显示的但是可以输入，并不是bug，输入后按回车。\n1 2 sudo apt install \u0026#39;linux-image-5.0.0-23-generic\u0026#39; sudo apt install \u0026#39;linux-headers-5.0.0-23-generic 安装完成后，需要重启虚拟机，并在启动时连按shift键，进入grub引导页，更换启动的Linux内核。\n之后安装基本组件：\n1 2 3 4 5 6 7 8 9 10 sudo apt install git-all sudo apt-get install curl sudo apt install make sudo apt -y install gcc sudo apt -y install g++ sudo apt -y install autoconf sudo apt -y install libtool sudo apt -y install pkg-config sudo apt -y install libmnl-dev sudo apt -y install libyaml-dev 如果人为设置了vpn就不用按照教程里替换源了。之后安装go语言环境，注意安装go语言环境时必须为普通用户安装，否则会导致后续安装出现问题。输入命令：\n1 go 以确认是否存在其他版本的go，若存在，则通过以下命令删除\n1 sudo rm -rf /usr/local/go 之后安装最新版本的go：\n1 2 3 cd ~ wget https://dl.google.com/go/go1.20.4.linux-amd64.tar.gz sudo tar -C /usr/local -zxvf go1.20.4.linux-amd64.tar.gz 安装完成后，需要通过以下命令配置环境变量（此过程按回车不会有输出）\n1 2 3 4 5 mkdir -p ~/go/{bin,pkg,src} echo \u0026#39;export GOPATH=$HOME/go\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export GOROOT=/usr/local/go\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export PATH=$PATH:$GOPATH/bin:$GOROOT/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 之后输入命令：\n1 go #有版本号等输出就行。安装成功，再补充安装以下模块，该模块为free5gc独立部署的日志模块，容器化部署也可以安装：\n1 #go get -u github.com/sirupsen/logrus 通过官方安装脚本安装docker，\n1 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun 安装完成后，运行命令docker \u0026ndash;version验证。之后需要安装docker-compose，通过以下命令完成：\n1 sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose 重启docker服务即可完成docker的部署：\n1 systemctl restart docker 完成后，需要将当前普通用户加入docker用户组，docker用户组在上述安装时自动创建，无需手动创建：\n1 2 sudo gpasswd -a $USER docker #将当前普通用户加入docker用户组 newgrp docker #更新docker用户组 此步目的在于防止后续free5gc容器化部署时，到make base步骤，出现permission denied。\n更换镜像源 这是很重要的一步，原理是部分镜像源起到类似代理的作用。在网上搜索最新docker镜像源，参考\n国内能用的Docker镜像源【2025最新持续更新】_docker 镜像-CSDN博客\n国内仍然可用docker镜像源汇总，长期维护，定期更新（2025年3月21日）_docker 国内镜像源-CSDN博客\nDocker换源加速(更换镜像源)详细教程（2025.3最新可用镜像，全网最详细） - 知乎\n然后输入类似于\n1 sudo nano /etc/docker/daemon.json 来创造或修改配置文件，在里面写入代理网站，但是下面这个是一开始的，现在被ban掉了好多，不推荐，\n1 2 3 4 5 6 7 8 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://mirror.baidubce.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://mirror.ccs.tencentyun.com\u0026#34; ] } 推荐这些，或者去我给的参考链接找最新存活的：\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker-0.unsee.tech\u0026#34;, \u0026#34;https://docker-cf.registry.cyou\u0026#34;, \u0026#34;https://docker.1panel.live\u0026#34;, \u0026#34;https://docker.xuanyuan.me\u0026#34;, \u0026#34;https://1ms.run\u0026#34;, \u0026#34;https://hub.fast360.xyz\u0026#34;, \u0026#34;https://hub.littlediary.cn\u0026#34; ] } 然后保存离开，输入以下命令清除缓存、重启docker\n1 2 sudo systemctl daemon-reload sudo systemctl restart docker 然后可以查看docker源是否更改：\n1 docker info | grep -A 1 \u0026#34;Registry Mirrors\u0026#34; 最后可以尝试验证：\n1 docker pull hello-world 拉取成功即可以。\n继续安装其他组件 安装cmake：\n1 sudo snap install cmake –classic 安装mongodb\n1 2 3 sudo apt -y update sudo apt -y install mongodb wget git sudo systemctl start mongodb 此时可能会报错Package 'mongodb' has no installation candidate，有可能因为ubuntu没更新找不到安装包，可以试试导入mongodb的公钥，运行以下命令：\n1 wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo gpg --dearmor -o /usr/share/keyrings/mongodb-archive-keyring.gpg 并添加其到apt，\n1 echo \u0026#34;deb [signed-by=/usr/share/keyrings/mongodb-archive-keyring.gpg] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/6.0 multiverse\u0026#34; | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list 更新包列表：\n1 sudo apt-get update 安装mongodb：\n1 sudo apt-get install mongodb-org 之后可以选择性安装yarn**（独立化部署的话则是必须）**\n1 2 3 curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo \u0026#34;deb https://dl.yarnpkg.com/debian/ stable main\u0026#34; | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt update \u0026amp;\u0026amp; sudo apt install yarn 构建GTP5G模块 注意构建GTP5G模块时，需要用普通用户构建，否则后续使用时会出错\n1 2 3 cd~ git clone https://github.com/free5gc/gtp5g.git cd gtp5g 编译\n1 2 make sudo make install 此时可能会遇到报错，如果是类似于warning: the compiler differs from the one used to build the kernel等，原因是找不到那几个编译器。\n需要安装：\n1 2 sudo apt update sudo apt install gcc-12 然后重新编译：\n1 2 3 make clean make sudo make install 容器化部署free5gc模拟核心网 首先，通过git clone下载项目代码（注意这里和教程文章的不太一样，教程里面那个链接好像用不了）：\n1 2 3 cd ~ git clone https://github.com/free5gc/free5gc-compose.git cd free5gc-compose 如果有vpn就不用按照文章里说的改代理\n编译代码：\n1 2 3 cd ~/free5gc-compose make base docker-compose build 其中docker-compose build一步可能报版本错误，原因在于上面安装docker、docker-compose时可能安装了较低版本的docker、docker-compose，此时可以选择重装docker、docker-compose或者修改docker-compose.yaml文件中第一行版本号3.8为当前版本，建议优先选择重装docker、docker-compose，以回避未知错误，可以通过命令docker \u0026ndash;version命令查询当前版本。（不过如果是按教程里的话一般没问题）\n交叉编译 交叉编译指利用不同的平台进行处理，并把安装或编译好的东西转移到所需平台。这种思想不仅仅用于以下案例，可以用到很多地方。\n如果不按照刚才说法设置镜像代理的话，编译过程中也有可能遇到golang bullseye报错，\n=\u0026gt; ERROR [internal] load metadata for docker.io/library/golang: 21.5s ------ \u0026gt; [internal] load metadata for docker.io/library/golang:1.21.8-bullseye\n如果实在不打算使用镜像源，可以利用交叉编译在主机windows挂梯子下载docker desktop软件，下载好后在上面搜索golang，选1.24.0-bullseye（我用的1.23.6-bullseye，这俩都行），点击pull，即可把包下载到电脑里。\n然后在主机打开命令行，输入\n1 docker images ，可以看到包已经下载到了主机里，然后输入命令：\n1 docker save -o E:\\go\\golang_1.24.0-bullseye.tar golang:1.24.0-bullseye 可以把包保存到电脑路径里\n从主机把压缩包拷贝到虚拟机：\n在虚拟机输入\n1 2 cd ~/free5gc-compose/base vim Dockerfile 进入配置文件，\n输入I，进入修改，把版本号改成所用版本(1.24.0之类的)，然后Esc，输入命令:wq（带冒号）即可保存退出。\n然后虚拟机终端输入：\n1 sudo docker load \u0026lt; /home/golang_1.24.0-bullseye 之后重复原本步骤编译就可以。\n至此，free5gc容器化部署完成\n二、安装UERANSIM 1 2 cd ~ git clone https://github.com/aligungr/UERANSIM 然后：\n1 2 3 4 5 6 7 sudo apt update sudo apt upgrade sudo apt install make sudo apt install g++ sudo apt install libsctp-dev sudo apt install lksctp-tools sudo apt install iproute2 编译代码：\n1 2 cd ~/UERANSIM make 编译完成就安装成功了。\n三、启动free5gc和ueransim环境 Free5gc，启动\n1 2 cd ~/free5gc-compose docker-compose up -d 之后\n1 ifconfig 这是查看网卡地址，启动free5gc后，会在本地虚拟化出一系列的网卡，这一步需要关注的是原先虚拟机自带的网卡，通常这类网卡的ip地址均处于192.168.*.*网段，网卡名类似ens33，eth0之类，可以以此特征区分出来\n接下来需要查看并记录amf网元的ip地址：\n1 docker inspect amf 找到上面记录有\u0026quot;IPAddress\u0026quot;: 的一行，后面记录的即是amf的ip地址\n记录下这两个ip地址后，就可以完成UERANSIM中gnb的配置了，通过修改free5gc-gnb.yaml配置文件完成这一步操作：\n1 2 cd ~/UERANSIM/config/ vim free5gc-gnb.yaml 输入I进行修改模式，需要修改其中的ngapIp、gtpIp为本机ip\n修改其中的amfconfig一项下的address为amf的ip，然后Esc，再输入 :wq\n表示保存并退出，类似还有 :q :qa :exit :^X之类的。\n每次重启机器后，amf地址可能改变，注意更改。\n至此，UERANSIM的基站配置完成，接下来需要在free5gc中注册UERANSIM的UE部分：\n访问地址 http://localhost:5000/可进入到free5gc的webui处，登录：\n用户名：admin 密码：free5gc\n之后通过free5gc的webui新增一个ue的注册信息（否则会报错说无法注册），此处配置的UE信息原则上需要和~/UERANSIM/config/free5gc-ue.yaml中的信息一致，但由于此处UERANSIM的代码作者已经设置好，所以实际上无需做任何更改，直接拉到最下面点create就ok\n之后启动UERANSIM模拟设备\n1 2 cd ~/UERANSIM/build#启动一个shell，执行启动gnb的流程 ./nr-gnb -c ../config/free5gc-gnb.yaml#通过nr-gnb程序，指定使用的gnb配置文件，启动模拟基站 另起一个shell，执行启动UE的流程\n1 2 cd ~/UERANSIM/build #通过nr-ue程序，指定使用的ue配置文件，启动模拟用户设备 sudo ./nr-ue -c ../config/free5gc-ue.yaml #此处因为需要虚拟出一张ue的网卡，所以需要root权限执行 启动的两个shell不可关闭，可以后台执行，但建议前台执行方便实时查看状态信息。启动完成后，执行ifconfig可以看到多了一张名为uesimtun0的网卡；另外，在free5gc的webui处，查看REALTIME STATUS可以看到有一个UE处于连接状态，此时即证明UERANSIM的环境启动成功：\n之后测试一下\n1 ping www.baidu.com -I uesimtun0 能通就ok，在free5gc官网也可以看到connect信息。\n核心网的基站层测试 一、以基站配置测试ueransim软件层 修改PLMN值，IP不变，进行测试、注册\n先改文件里PLMN的mcc、mnc分别为001、01，按照自己的目录（我的涉及/home/wxy/free5gc-compose/config/amfcfg.yaml、gnbcfg.yaml、smfcfg.yaml、uecfg.yaml，/home/wxy/UERANSIM/config/free5gc-gnb.yaml、free5gc-ue.yaml，包括修改ue里supi的前五位为00101：\n还有nrfcfg文件的mcc、mnc\n改完save，然后关闭docker\n1 docker-compose down 再重新跑核心网流程wireshark抓包查看闪退原因\n启动核心网环境\n1 2 cd ~/free5gc-compose sudo docker-compose up -d 启动UERANSIM的gnb\n1 2 cd ~/UERANSIM/build ./nr-gnb -c ../config/free5gc-gnb.yaml 启动ue\n1 2 cd ~/UERANSIM/build sudo ./nr-ue -c ../config/free5gc-ue.yaml 并重新create，修改mcc、mnc、id后注册\n然后根据一般ueransim测试流程启动，查询有无报错，并用uesimtun0来ping一下百度。\n虚拟网卡测试\n1 ping www.baidu.com -I uesimtun0 之后需要修改ip： 分别在docker-compose文件里面添加upf的ip、设置amf的ip，还有network的范围\n地址范围subnet要可以覆盖，改为192.168.0.0/16\n由于n3iwue、free5gc-n3iwf两部分此时用不到，可以把这两段代码分别注释掉。我们要修改amf为192.168.2.198，smf内的upf与N3口192.168.8.198。\n在amfcfg将ngapip改为基站的192.168.2.198\nsmfcfg的upf ip改为基站upf 192.168.8.198\n以及upfcfg的N3 ip为192.168.8.198\n改完保存，把docker给down掉关闭，然后重复一般测试流程，成功标志为\n1 ping baidu.com -I uesimtun0 可以收到包。\n问题1：遇到ue、gnb成功连接且核心网成功配置但是不通网 这个问题比较随机，因为每个人虚拟机、或者linux子系统的默认配置不同。诊断方法可以考虑进入upf的网络空间进行调整；或者docker logs amf那几个看一下日志；或根据路由、配合抓包进行诊断。具体命令在本文档第二部分。\n查询UERANSIM内的UE和GNB有没有建立PDU连接：\n如图可见二者连接正常，说明ue与gnb连接正常。之后可以分别查看一下amf、upf有没有正常启动、报错，用以下命令检查日志：\n1 docker logs amf 1 docker logs upf 如果没有报错，排查gnb通过N3口连接upf：\n1 ping 192.168.8.198 -I uesimtun0 还是通的话，可以进入upf的网络空间，看upf联网如何，\n利用pid方法进入网络空间进行ifconfig或者ping操作\n1 pgrep -f upf #获取pid： 1 sudo nsenter -t \u0026lt;pid\u0026gt; -n #进入网络空间 之后，在里面输入：\n1 ifconfig 查找网络工具，若有upfgtp或者eth0就可以，之后输入ping命令\n1 ping 8.8.8.8 如果有包收发，那就证明upf可以上网，所以整个网络连接没有问题。\n问题出在哪里？IP 转发 和 NAT 配置\n我们需要启用 IPv4 数据包转发功能（将接收到的 IP 数据包从一个网络接口转发到另一个网络接口），以及开启NAT模式（数据包离开当前网络时进行源地址转换），并允许转发的数据包通过防火墙。ICMP\n把docker-compose文件的upf部分的command修改添加以下代码\n1 2 3 4 5 6 7 8 command: - /bin/bash - -c - | sysctl -w net.ipv4.ip_forward=1 iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -I FORWARD 1 -j ACCEPT ./upf -c /free5gc/config/upfcfg.yaml 修改后save，然后把ueransim关掉，docker给down掉，重新跑一遍启动流程，之后测试\n1 ping www.baidu.com -I uesimtun0 跑通成功。\n二、连接实体基站进行测试 用SSH来连接基站，需要下载ssh工具\n1 2 sudo apt update sudo apt install openssh-client 启动基站流程：\n分别打开三个终端shell\n在第一个shell输入\n1 ssh root@192.168.2.246 输入密码，然后输入\n1 cd /root/yzmm/rel/GNB/phy/bin ./run.sh 挂在后台，别关闭。\n在第二个shell输入\n1 ssh root@192.168.2.246 输入密码，然后输入\n1 cd /root/yzmm/rel/GNB/cu/cu/nr_hl_cu/build ./run.sh 在第三个shell输入\n1 ssh root@192.168.2.246 再\n1 cd /root/yzmm/rel/GNB/du/ran/DU/build/intel/du_bin/bin ./run.sh 需要关掉ueransim，防止ip干扰冲突。\n然后添加端口映射38412，此处对amf修改无严格要求。\n添加upf网口2152:2152 /udp，经测试ue无法注册信号，产生mac相关报错，原因可能是free5gc默认和ueransim的对齐导致一些文件配置有问题。于是采用修改的核心网，工程文件来源于师兄distributed-core-network-control-plane。\n写在结尾 特别鸣谢：两位师兄和两位老师\n后期工作：\n梳理流程思路，抓包分析\n深入理解代码、规范、抓包工具；实体基站和软件ueransim的不同\n步进调试环境搭建、分布式核心网改进\n本文只涉及基础的Free5GC内容，由于传统核心网的集中性、易受攻击的问题，可以采取分布式架构进行优化，下篇文章阐述何为分布式核心网、市场空间、技术路线等。\n","date":"2025-05-23T12:45:04+08:00","permalink":"https://mosfish.github.io/p/5g%E6%A0%B8%E5%BF%83%E7%BD%91%E9%83%A8%E7%BD%B2%E5%8F%8A%E6%B5%8B%E8%AF%95%E6%89%8B%E5%86%8C/","title":"5G核心网部署及测试手册"},{"content":"水木清华的深冬与初春 喜欢摄影，随快门声记录下那些难忘的时刻，一点一点凑成碎片，从流年中捞起那不变的永恒。\n记得刚到这里的时候，也才将将春节之后。京城的街道充满了三种氛围：节日的热闹，假期的冷清，外来旅客潮的繁忙——巧了，我也是这众多旅人中的一员，听着火车站大哥亲切的京腔，似乎很久都没回家了，今年有幸在华北（家的地方），感受到了发自内心的愉悦与放松。\n喜欢家里的饺子，喜欢家里的炒饭炒面，喜欢门外吆喝的小商小贩，喜欢小区门口的庆丰包子铺，喜欢地铁口的煎饼果子、加俩蛋。\n人间烟火气，最抚凡人心\n或者说，最抚凡人心的，是熟悉的、家乡的烟火气。五道口这里，是恩师的母校，也是故友求学的地方，虽时运不济恰恰错峰难以见一面故友，不过我相信，来日方长，未来我应该还会来五道口访问交流，希望到那时我们都摆脱了升学的压力，在饺子铺一叙过往吧。\n热情好客的寒潮与零下十度的京城 回想起来，倒是第一次因为学业而来到京城，不想麻烦亲朋好友，便匆匆租了个房了事。好巧不巧，海淀的初春也遭遇了难得一遇的大寒潮，尽管我裹得严严实实（但是面部、双耳没裹上），也是被冻得丢了半条命，肚子痛得打滚，点了一份疙瘩汤当晚饭（还有美团送药），草草了事。\n在清华忙碌的日子 相较于康乐园，似乎清华园也差不多大，当然如果康乐园的家属区感觉占比大一点。冬日（或者初春）的清华简直就像人民公园，一眼看上去和我家门口的公园感觉差不多，寥寥几个学生、打卡的游客，以及我这种无业游民哈哈（旅客？学生？应该算visiting student吧）。在这里的时候，每天上午9点多来，忙到晚上9点多10点回去，我摸鱼的时候学了Latex、Markdown，真的挺有意思；除此之外其实主线任务是学习Linux开发、Arm开发、free5GC核心网架构，其实第一次开会我真的啥也没听懂，倒是快走人的时候才知道——哦，原来我是这么干的啊！\n在FIT楼，五道口的一些碎片 在FIT楼，还挺热，在平常应该穿毛衣的情况下，我在里面基本都是短袖，以至于每次楼下取外卖的时候，门口大爷会一脸错愕的盯着我：“小伙子，火力是旺！”此外，我在知乎还看到了一个问题：《为什么FIT楼这么热》，倒是个性化推送了。\n学校大，食堂离我在的FIT楼远，因此我吃了20多天外卖（海淀的饭真的有点贵的\u0026hellip;）。其实有的生活过的挺苦逼的，比如：\nCase1 早上没有报备入校咋办？ 很正常，系统甚至有延迟，五道口旁边有一个咖啡店，也有喜茶、霸王茶姬，“欢迎来chang”是我这段时间听到最多的话，因为好喝、便宜、人少、离学校和车站近，所以奶茶店学习挺常见的哈\u0026hellip;\n奶茶办公让我想起了“学术酒吧” Case2 苦逼生活？咋个苦法？ 其实只是相对的苦不是绝对的苦，只是不如母校的生活罢了。压力大，任务繁重，好几次连外卖都没顾得上吃，楼道里自动售货机的面包、泡面凑合凑合\u0026hellip;\u0026hellip;\n和呆了一个月的地方合个影吧 但是好景不长，之后就被发配到基站屋子里，和一堆服务器大铁疙瘩待一块去了，一看就是我们臭理工男的工作室，鉴于FIT楼建成已久，所以基建也没那么新。\n我的工位与发配楼上的工位 吃啥 探店+食堂，海淀的饭不便宜。这几天主要还是在照澜园吃饭了，毕竟离得近；有些窗口很便宜，但是教职工自选那里确实是有点贵了（也有可能是我挑贵的拿了）\n饮食记录 还去了方砖厂炸酱面，免费续面条真不错，就是面一般般，可能网红属性就这样。\n有点腻的方砖厂炸酱面，一碗30块钱 结束与告别 用UERANSIM和free5GC、分布式核心网搭建了通信系统，学到了很多，写了个文档，我想想，等之后有时间我写给文档（脱敏版）上传github，就叫《全网最详细版free5gc和ueransim核心网通信链路部署搭建手册》啦！（如果我不懒没有鸽的话）\n汇报 清华的老师：恭喜你完成了一个有意义的short stay，希望你学到了东西！\n时间过得好快，中午被实验室的师兄们拉去清华的某个餐厅点菜聚餐（倒是挺贵，不过师兄们未来应该不会差钱的），当天晚上最后和师兄在金谷园吃了顿饺子，本来打算请师兄吃饭的，没想到师兄提前把单买好了。\n没有回眸，转身离开 告别 收拾东西，感慨诸多，感谢这一段经历，感谢老师、师兄的帮助。其实很多时候人都会被他人灌输的“高价值”事物洗脑，以为某些东西看上去“高级”或者“先进”，实际上祛魅的最好方式是真正去体验一下，去感受一下，才能知道是否和他人描述的相符，这也有点像小马过河吧。\n没有调查就没有发言权。\n这次实地的“考察”让我感觉到，清华，我国工科教育当之无愧的殿堂，事实上里面的学生和大家也没什么不同，只是一批更聚焦、更能坚守的同龄人，在深耕的领域有自己的taste，在朋辈身旁有更大的peer pressure（这可能有优有劣）；清华的设施也没想象中那么新规或者那么老旧，一切都是刚刚好，甚至和母校都很像。\n每一次来到这里，都能唤起许多美好的回忆，又能够发现许多新鲜的东西，心情不同，时间不同，景色也不同。\n说了这么多，权当老夫痴言乱语罢了，前途似海，来日方长。\n","date":"2025-03-12T10:52:03+08:00","image":"https://mosfish.github.io/p/days-in-tsinghua./10_hu_1d5a0457f3f45ccc.webp","permalink":"https://mosfish.github.io/p/days-in-tsinghua./","title":"Days in Tsinghua."},{"content":"突然告知要代表学院出差 事情是这样的，突然接到通知让我和其他几位（我是某校级科创协会会长，其他一块的有类似学生会主席、研会主席、团副这种）去西安交流，于是我请了假、上了车，去了机场。一切是那么的突然，似乎没给我纠结的时间，但是我听到“出差补贴”四个字的时候直接蹦了起来，学生群体真好拿捏呀。\n出行前的轶事：刚好DSP课点名了，我给李教授说请假了，结果教授说：\n不相信，班里总有不少人假冒假条，一点到名就说请假。\n我倒是没翘过课被抓，老师讲的比较带感，唯独我对此不感兴趣，甚至不知道自己是不是真正喜欢现在的EE专业。不过这次“请假”确实是真的，毕竟我都上飞机了。于是，迎着落日余晖和熟悉的干燥的冷气，我到了西安，感受到熟悉却又陌生的气息。\n西安的东西和我家很像，街角的包子铺、肉夹馍，面条饺子之类的，却都是我喜欢吃的东西。酒店有健身房，倒也是不错。\n且向长安那畔行 西电交流 首日，由于我们是第一批，天大寒、砚冰坚，雨夹雪实在有些冷人了。起初进不去，找了迪卡侬取取暖。首日去了西电的研究生校区进行交流，后来发现我们的交流具有现实意义——学硕、专硕与工程博士的区分设立。这里的冬天夹杂了一份秋的踪影，地上金黄色的落叶飞舞着。这段时光或许是人生里很惬意的时光之一了，跟着老师和师兄们，在迪卡侬躲避寒风，品尝泡馍\u0026hellip;\u0026hellip;\n不许动！说了不许动！ 也是有资格上桌交流了 西交交流——创新港，偷摸鱼 好吧这次就没资格上桌了，于是校园走走，嗯，还挺好的其实。\n创新港与电信楼 瞎逛 其实这个地方来了很多次，所以也不知道逛啥，会了会老友（我们仨），然后和大部队一块去了趟省博物馆、大唐不夜城啥的，买了一堆纪念币，除此之外就\u0026hellip;就只是吃吃吃了。\n结语 好吃，太冷，太干，下次不来了。\n当初若选大学的时候来了西交，那可能会有些不一样；不过我不后悔来我的母校，因为我这次访问的时候，他们说西交压力太大了。\n","date":"2024-12-17T18:29:48+08:00","image":"https://mosfish.github.io/p/%E8%A5%BF%E5%AE%89%E4%B8%A4%E4%B8%89%E4%BA%8B/1_hu_efcab814805c5b8e.webp","permalink":"https://mosfish.github.io/p/%E8%A5%BF%E5%AE%89%E4%B8%A4%E4%B8%89%E4%BA%8B/","title":"西安两三事"},{"content":"记一次话剧主角的经历 之前一直喜欢看剧，话剧、舞台剧、音乐剧、交响音乐会都有涉及，不过个人更喜欢音乐剧一些，从法红与黑、法扎、莫里哀（明年订票去），到德扎、伊丽莎白、蝴蝶梦，几乎看了个遍，自学了一丢丢德语、在学校学了法语，不为别的，就为了看剧；相反，话剧的兴趣可能没那么大。\n最近的汇总，被门票爆金币了 偶然进入话剧社 进入这个地方的原因多种多样，有为了扩(ren)大(shi)社(mei)交(zi)的，有喜欢搞剧的，像我这种陪好兄弟去玩结果兄弟被刷了、我莫名其妙的被录进去的还是蛮少见的。\n主角竟是我？ 作为完全零经验小白，被迫出演《控方证人》的威尔爵士，链接https://www.bilibili.com/video/BV1Rb4y1371f\n一直想着结束后说点啥感想，恰逢一周年吧，又看了看自己演的话剧的录播，其实感触颇多。记得学姐说：\n“你可以出师啦”\n是吗？或许是吧，一个没有经验的小白，坐排天天挨骂、念台词被各种批斗；为了话剧放弃了许多，不过想了想也还不错，虽然错过了很多机会，想来现在很多比赛费半天劲也就当玩玩，不如去试试research，这段经历倒是给我启发。\n排练的碎片 多少日夜，多少排练，多少绕口令，回忆涌上心头，可惜曾经的朋友慢慢四散而去。\n学好声韵辨四声，阴阳上去要分明， 部位方法须找准，开齐合撮属口形。 双唇班抱必百波，抵舌当地斗点钉， 舌根高狗工耕故，舌面机结教坚精， 翘舌主争真志照，平舌资责早在增。 擦音发翻飞分复，送气查柴产彻称。 合口忽午枯胡鼓，开口河坡哥安争。 嘴撮虚学寻徐剧，齐齿衣优摇业英。 抵颚恩音烟弯稳，穿鼻昂迎中拥生。 咬紧字头归字尾，不难达到纯和清。\n仍记得定妆照那天赶完ddl就睡了三个小时；早上穿着戏服上课当显眼包，下午提前做完实验赶到现场，依稀记得女搭档们忙前忙后给我化妆的样子，仍然记得学姐顶着发烧来给我们拍照\u0026hellip;\u0026hellip;太多太多瞬间。\n这是哪位呢？我不认识 临门一脚 前几天晚上最后一次联排，相当于演习。整体很仓促，大家都容易着急，演话剧最容易出现的问题就是急或者快；期待最后一天能超常发挥。\n啊？ 早上8点半到活动中心搬东西，我一个“老头”还得一步一步从那里把门扛到新传剧场，倒也不能说激动，主要还是紧张\u0026hellip;\n忙到乱成一团了 在聚光灯下 演出很顺利，起初面对聚光灯手抖的不行，逐渐到泰然自若、享受掌声，这是这段时间的成长。剧组每个人都很辛苦，在这短短的一个月每个人都舍弃了不少，但是收获的更多。\n从此，面对台下熙熙攘攘的观众，你是否有挺胸谈吐的决心？反正我还是害怕，嘿嘿。\n场照、合影与谢幕 岁月不居，时节如流。\n所以，青春永不落幕，对吗？\n在聚光灯下 ","date":"2024-12-08T12:25:21+08:00","image":"https://mosfish.github.io/p/%E5%9C%A8%E8%81%9A%E5%85%89%E7%81%AF%E4%B8%8B/12_hu_3f2c33362a33220b.jpg","permalink":"https://mosfish.github.io/p/%E5%9C%A8%E8%81%9A%E5%85%89%E7%81%AF%E4%B8%8B/","title":"在聚光灯下"}]