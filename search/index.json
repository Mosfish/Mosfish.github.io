[{"content":"如何优化rna逆合成模型 这是一个很challenging的问题，别问，问就是啥都不会。\n考虑到gRNAde: Geometric Deep Learning for 3D RNA Inverse Design论文的细节、思路非常清晰，且作者release了非常具象的source code，因而以gRNAde等state-of-the-art模型入手分析问题，作者代码开源地址：https://github.com/chaitjo/geometric-rna-design\n服务器需要的一些设置 在这个过程中，注意一些tricks。由于租用的国内服务器与英国的环境不太一样，因此需要多使用数据盘而非系统盘。本项目最大的问题是作者的环境变量设置。\nPython 3.10.12 and CUDA 11.8, numpy \u0026lt;2.0\n镜像网站加速 1 git clone https://ghfast.top/github.com/chaitjo/geometric-rna-design 1 cd /root/autodl-tmp/geometric-rna-design 此外，在数据盘配环境 1 mamba create -p /root/autodl-tmp/rna python=3.10 如果需要激活环境，请注意相对路径：\n1 mamba activate /root/autodl-tmp/rna 配置租用服务器版本环境变量，下方命令只对当前shell有效。建议写入bashrc或者利用作者给出的.env文件，记得source ~/.bashrc 1 2 3 4 5 6 export PROJECT_PATH=\u0026#39;/root/autodl-tmp/geometric-rna-design/\u0026#39; export ETERNAFOLD=\u0026#39;/root/autodl-tmp/geometric-rna-design/tools/EternaFold\u0026#39; export X3DNA=\u0026#39;/root/autodl-tmp/geometric-rna-design/tools/x3dna-v2.4\u0026#39; export PATH=\u0026#34;/root/autodl-tmp/geometric-rna-design/tools/x3dna-v2.4/bin:$PATH\u0026#34; export PATH=\u0026#34;/root/autodl-tmp/cdhit:$PATH\u0026#34; PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 而相应的，英国项目版本则按照如下，放到.env文件里。\n1 2 3 4 5 6 7 8 9 export PROJECT_PATH=\u0026#39;/home/remote1/geometric-rna-design/\u0026#39; export DATA_PATH=\u0026#39;/home/remote1/geometric-rna-design/data/\u0026#39; export WANDB_PROJECT=\u0026#39;rna\u0026#39; export WANDB_ENTITY=\u0026#39;wenxy59-sun-yat-sen-university\u0026#39; export WANDB_DIR=\u0026#39;/home/remote1/geometric-rna-design/\u0026#39; export ETERNAFOLD=\u0026#39;/home/remote1/geometric-rna-design/tools/EternaFold\u0026#39; export X3DNA=\u0026#39;/home/remote1/geometric-rna-design/tools/x3dna-v2.4\u0026#39; export PATH=\u0026#34;/home/remote1/geometric-rna-design/tools/x3dna-v2.4/bin:$PATH\u0026#34; PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 训练模型的注意事项 数据预处理 注意运行main脚本前，需要处理数据生成process.pt文件，并生成相应的das_split.pt文件。在这个过程中，需要安装USalign与qTMclust工具。一般而言，安装后者的同时会自动安装前者。\n1 2 3 git clone https://github.com/pylelab/USalign.git cd USalign g++ -O3 -o qTMclust qTMclust.cpp -lm 之后就可以使用USalign -h与qTMclust -h命令来验证安装，记得查一下路径。在训练脚本里的相应位置路径用的相对路径可能报错，例如src/data/clustering_units.py记得修改。\n处理数据成功 数据处理大概是把14000+条raw数据处理为3910条可用数据，因为raw数据有残缺的处理后筛查出去了，因而数据不多，正常。\n有问题的数据 然后用notebook里的代码生成split文件。\n模型使用与实验 作者给出.py脚本进行启动，或者用命令行如下：\n1 python gRNAde.py --pdb_filepath data/raw/6J6G_1_L-E.pdb --output_filepath tutorial/lnc/po/114.fasta --split das --max_num_conformers 1 --n_samples 16 --temperature 0.5 使用效果如图：\n利用gRNAde预测结构与序列 模型在长rna序列（100+nts）的时候性能会下降，虽然recovery保持良好但是二级结构自洽性得分SC Score呈现明显线性下降。\n四个参数直观对比 SC Score(左)与Recovery(右)随Sequence的变化 另一个生成范式：RiboDiffusion 至于RiboDiffusion，其他部署方式一致但是环境设置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 conda create -n rna2 python=3.10 -y conda activate rna2 pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116 pip install absl-py==0.15.0 pip install biopython==1.80 pip install dm_tree==0.1.7 pip install fair-esm==2.0.0 pip install ml_collections==0.1.1 pip install numpy==1.24.3 pip install scipy\u0026gt;=1.10.0 pip install tqdm==4.64.1 pip install torch-cluster==1.6.1+pt113cu116 -f https://data.pyg.org/whl/torch-1.13.0+cu116.html pip install torch-scatter==2.1.1+pt113cu116 -f https://data.pyg.org/whl/torch-1.13.0+cu116.html pip install torch-geometric==2.3.1 根据脚本运行，输出会出现一些问题，例如：\nRiboDiffusion输出 RiboDiffusion模型的Recovery在对数坐标(上)与常数坐标(下)表示下随Sequence的变化 初步改进——attention 考虑到多层注意力机制会有利于长序列的捕捉，我先使用MultiheadAttention，加了一层简单的代码，进行训练。其中原本release的代码有bug，尤其是这个函数里的mask_coords掩码逻辑有问题，经常与sample的维度不匹配报错。我把修改过的evaluator.py贴在这里，同时防止显存爆炸，需要设置一些batch。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 import os import copy import shutil from datetime import datetime import numpy as np import pandas as pd from tqdm import tqdm import wandb import torch import torch.nn.functional as F from torchmetrics.functional.classification import binary_matthews_corrcoef from Bio import SeqIO from Bio.Seq import Seq from Bio.SeqRecord import SeqRecord from MDAnalysis.analysis.align import rotation_matrix from MDAnalysis.analysis.rms import rmsd as get_rmsd from src.data.data_utils import pdb_to_tensor, get_c4p_coords from src.data.sec_struct_utils import ( predict_sec_struct, dotbracket_to_paired, dotbracket_to_adjacency ) from src.constants import ( NUM_TO_LETTER, PROJECT_PATH, RMSD_THRESHOLD, TM_THRESHOLD, GDT_THRESHOLD ) def evaluate( model, dataset, n_samples, temperature, device, model_name=\u0026#34;eval\u0026#34;, metrics=[ \u0026#39;recovery\u0026#39;, \u0026#39;perplexity\u0026#39;, \u0026#39;sc_score_eternafold\u0026#39;, \u0026#39;sc_score_ribonanzanet\u0026#39;, \u0026#39;sc_score_rhofold\u0026#39; ], save_designs=False ): \u0026#34;\u0026#34;\u0026#34; Run evaluation suite for trained RNA inverse folding model on a dataset. The following metrics can be computed along with metadata per sample per residue: 1. (recovery) Sequence recovery per residue (taking mean gives per sample recovery) 2. (perplexity) Perplexity per sample 3. (sc_score_eternafold) Secondary structure self-consistency score per sample, using EternaFold for secondary structure prediction and computing MCC between the predicted and groundtruth 2D structures as adjacency matrices. 4. (sc_score_ribonanzanet) Chemical modification self-consistency score per sample, using RibonanzaNet for chemical modification prediction of the groundtruth and designed sequences, and measuring MAE between them. 5. (sc_score_rhofold) Tertiary structure self-consistency scores per sample, using RhoFold for tertiary structure prediction and measuring RMSD, TM-score, and GDT_TS between the predicted and groundtruth C4\u0026#39; 3D coordinates. 6. (rmsd_within_thresh) Percentage of samples with RMSD within threshold (\u0026lt;=2.0A) 7. (tm_within_thresh) Percentage of samples with TM-score within threshold (\u0026gt;=0.45) 8. (gddt_within_thresh) Percentage of samples with GDT_TS within threshold (\u0026gt;=0.50) Args: model: trained RNA inverse folding model dataset: dataset to evaluate on n_samples: number of predicted samples/sequences per data point temperature: sampling temperature device: device to run evaluation on model_name: name of model/dataset for plotting (default: \u0026#39;eval\u0026#39;) metrics: list of metrics to compute save_designs: whether to save designs as fasta with metrics Returns: Dictionary with the following keys: df: DataFrame with metrics and metadata per residue per sample for analysis and plotting samples_list: list of tensors of shape (n_samples, seq_len) per data point recovery_list: list of mean recovery per data point perplexity_list: list of mean perplexity per data point sc_score_eternafold_list: list of 2D self-consistency scores per data point sc_score_ribonanzanet_list: list of 1D self-consistency scores per data point sc_score_rmsd_list: list of 3D self-consistency RMSDs per data point sc_score_tm_list: list of 3D self-consistency TM-scores per data point sc_score_gddt_list: list of 3D self-consistency GDTs per data point rmsd_within_thresh_list: list of % scRMSDs within threshold per data point tm_within_thresh_list: list of % scTMs within threshold per data point gddt_within_thresh_list: list of % scGDDTs within threshold per data point \u0026#34;\u0026#34;\u0026#34; assert \u0026#39;recovery\u0026#39; in metrics, \u0026#39;Sequence recovery must be computed for evaluation\u0026#39; ####################################################################### # Optionally initialise other models used for self-consistency scoring ####################################################################### if \u0026#39;sc_score_ribonanzanet\u0026#39; in metrics: from tools.ribonanzanet.network import RibonanzaNet # Initialise RibonanzaNet for self-consistency score ribonanza_net = RibonanzaNet( os.path.join(PROJECT_PATH, \u0026#39;tools/ribonanzanet/config.yaml\u0026#39;), os.path.join(PROJECT_PATH, \u0026#39;tools/ribonanzanet/ribonanzanet.pt\u0026#39;), device ) # Transfer model to device in eval mode ribonanza_net = ribonanza_net.to(device) ribonanza_net.eval() if \u0026#39;sc_score_rhofold\u0026#39; in metrics: from tools.rhofold.rf import RhoFold from tools.rhofold.config import rhofold_config # Initialise RhoFold for 3D self-consistency score rhofold = RhoFold(rhofold_config, device) rhofold_path = os.path.join(PROJECT_PATH, \u0026#34;tools/rhofold/model_20221010_params.pt\u0026#34;) print(f\u0026#34;Loading RhoFold checkpoint: {rhofold_path}\u0026#34;) rhofold.load_state_dict(torch.load(rhofold_path, map_location=torch.device(\u0026#39;cpu\u0026#39;))[\u0026#39;model\u0026#39;]) # Transfer model to device in eval mode rhofold = rhofold.to(device) rhofold.eval() current_datetime = datetime.now().strftime(\u0026#34;%Y%m%d_%H%M%S\u0026#34;) #################################################### # Evaluation loop over each data point sequentially #################################################### # per sample metric lists for storing evaluation results samples_list = [] # list of tensors of shape (n_samples, seq_len) per data point recovery_list = [] # list of mean recovery per data point perplexity_list = [] # list of mean perplexity per data point sc_score_ribonanzanet_list = [] # list of 1D self-consistency scores per data point sc_score_eternafold_list = [] # list of 2D self-consistency scores per data point sc_score_rmsd_list = [] # list of 3D self-consistency RMSDs per data point rmsd_within_thresh_list = [] # list of % scRMSDs within threshold per data point sc_score_tm_list = [] # list of 3D self-consistency TM-scores per data point tm_within_thresh_list = [] # list of % scTMs within threshold per data point sc_score_gddt_list = [] # list of 3D self-consistency GDTs per data point gddt_within_thresh_list = [] # list of % scGDDTs within threshold per data point # DataFrame to store metrics and metadata per residue per sample for analysis and plotting df = pd.DataFrame(columns=[\u0026#39;idx\u0026#39;, \u0026#39;recovery\u0026#39;, \u0026#39;sasa\u0026#39;, \u0026#39;paired\u0026#39;, \u0026#39;rmsds\u0026#39;, \u0026#39;model_name\u0026#39;]) model.eval() if device.type == \u0026#39;xpu\u0026#39;: import intel_extension_for_pytorch as ipex model = ipex.optimize(model) if \u0026#39;sc_score_ribonanzanet\u0026#39; in metrics: ribonanza_net = ipex.optimize(ribonanza_net) if \u0026#39;sc_score_rhofold\u0026#39; in metrics: rhofold = ipex.optimize(rhofold) with torch.no_grad(): for idx, raw_data in tqdm( enumerate(dataset.data_list), total=len(dataset.data_list) ): # featurise raw data data = dataset.featurizer(raw_data).to(device) # sample n_samples from model for single data point: n_samples x seq_len samples, logits = model.sample(data, n_samples, temperature, return_logits=True) samples_list.append(samples.cpu().numpy()) # perplexity per sample: n_samples x 1 n_nodes = logits.shape[1] perplexity = torch.exp(F.cross_entropy( logits.view(n_samples * n_nodes, model.out_dim), samples.view(n_samples * n_nodes).long(), reduction=\u0026#34;none\u0026#34; ).view(n_samples, n_nodes).mean(dim=1)).cpu().numpy() perplexity_list.append(perplexity.mean()) ########### # Metadata ########### # per residue average SASA: seq_len x 1 mask_coords = data.mask_coords.cpu().numpy() sasa = np.mean(raw_data[\u0026#39;sasa_list\u0026#39;], axis=0)[mask_coords] # per residue indicator for paired/unpaired: seq_len x 1 paired = np.mean( [dotbracket_to_paired(sec_struct) for sec_struct in raw_data[\u0026#39;sec_struct_list\u0026#39;]], axis=0 )[mask_coords] # per residue average RMSD: seq_len x 1 if len(raw_data[\u0026#34;coords_list\u0026#34;]) == 1: rmsds = np.zeros_like(sasa) else: rmsds = [] for i in range(len(raw_data[\u0026#34;coords_list\u0026#34;])): for j in range(i+1, len(raw_data[\u0026#34;coords_list\u0026#34;])): coords_i = get_c4p_coords(raw_data[\u0026#34;coords_list\u0026#34;][i]) coords_j = get_c4p_coords(raw_data[\u0026#34;coords_list\u0026#34;][j]) rmsds.append(torch.sqrt(torch.sum((coords_i - coords_j)**2, dim=1)).cpu().numpy()) rmsds = np.stack(rmsds).mean(axis=0)[mask_coords] ########## # Metrics ########## # sequence recovery per residue across all samples: n_samples x seq_len recovery = samples.eq(data.seq).float().cpu().numpy() recovery_list.append(recovery.mean()) # update per residue per sample dataframe df = pd.concat([ df, pd.DataFrame({ \u0026#39;idx\u0026#39;: [idx] * len(recovery.mean(axis=0)), \u0026#39;recovery\u0026#39;: recovery.mean(axis=0), \u0026#39;sasa\u0026#39;: sasa, \u0026#39;paired\u0026#39;: paired, \u0026#39;rmsds\u0026#39;: rmsds, \u0026#39;model_name\u0026#39;: [model_name] * len(recovery.mean(axis=0)) }) ], ignore_index=True) # global 2D self consistency score per sample: n_samples x 1 if \u0026#39;sc_score_eternafold\u0026#39; in metrics: sc_score_eternafold, pred_sec_structs = self_consistency_score_eternafold( samples.cpu().numpy(), raw_data[\u0026#39;sec_struct_list\u0026#39;], mask_coords, return_sec_structs = True ) sc_score_eternafold_list.append(sc_score_eternafold.mean()) # global 1D self consistency score per sample: n_samples x 1 if \u0026#39;sc_score_ribonanzanet\u0026#39; in metrics: sc_score_ribonanzanet, pred_chem_mods = self_consistency_score_ribonanzanet( samples.cpu().numpy(), raw_data[\u0026#39;sequence\u0026#39;], mask_coords, ribonanza_net, return_chem_mods = True ) sc_score_ribonanzanet_list.append(sc_score_ribonanzanet.mean()) # global 3D self consistency scores per sample: n_samples x 1, each if \u0026#39;sc_score_rhofold\u0026#39; in metrics: try: output_dir = os.path.join( wandb.run.dir, f\u0026#34;designs_{model_name}/{current_datetime}/sample{idx}/\u0026#34;) except AttributeError: output_dir = os.path.join( PROJECT_PATH, f\u0026#34;designs_{model_name}/{current_datetime}/sample{idx}/\u0026#34;) sc_score_rmsd, sc_score_tm, sc_score_gdt = self_consistency_score_rhofold( samples.cpu().numpy(), raw_data, mask_coords, rhofold, output_dir, save_designs = save_designs ) sc_score_rmsd_list.append(sc_score_rmsd.mean()) sc_score_tm_list.append(sc_score_tm.mean()) sc_score_gddt_list.append(sc_score_gdt.mean()) rmsd_within_thresh_list.append((sc_score_rmsd \u0026lt;= RMSD_THRESHOLD).sum() / n_samples) tm_within_thresh_list.append((sc_score_tm \u0026gt;= TM_THRESHOLD).sum() / n_samples) gddt_within_thresh_list.append((sc_score_gdt \u0026gt;= GDT_THRESHOLD).sum() / n_samples) if save_designs: # collate designed sequences in fasta format sequences = [SeqRecord( Seq(raw_data[\u0026#34;sequence\u0026#34;]), id=f\u0026#34;input_sequence,\u0026#34;, description=f\u0026#34;pdb_id={raw_data[\u0026#39;id_list\u0026#39;][0]} rfam={raw_data[\u0026#39;rfam_list\u0026#39;][0]} eq_class={raw_data[\u0026#39;eq_class_list\u0026#39;][0]} cluster={raw_data[\u0026#39;cluster_structsim0.45\u0026#39;]}\u0026#34; )] for idx, zipped in enumerate(zip( samples.cpu().numpy(), perplexity, recovery.mean(axis=1), sc_score_eternafold, pred_sec_structs, sc_score_ribonanzanet, pred_chem_mods, sc_score_rmsd, sc_score_tm, sc_score_gdt )): seq, perp, rec, sc, pred_ss, sc_ribo, pred_cm, sc_rmsd, sc_tm, sc_gdt = zipped seq = \u0026#34;\u0026#34;.join([NUM_TO_LETTER[num] for num in seq]) edit_dist = edit_distance(seq, raw_data[\u0026#39;sequence\u0026#39;]) sequences.append(SeqRecord( Seq(seq), id=f\u0026#34;sample={idx},\u0026#34;, description=f\u0026#34;temperature={temperature} perplexity={perp:.4f} recovery={rec:.4f} edit_dist={edit_dist} sc_score={sc:.4f} sc_score_ribonanzanet={sc_ribo:.4f} sc_score_rmsd={sc_rmsd:.4f} sc_score_tm={sc_tm:.4f} sc_score_gdt={sc_gdt:.4f}\u0026#34; )) # write all designed sequences to output filepath SeqIO.write(sequences, os.path.join(output_dir, \u0026#34;all_designs.fasta\u0026#34;), \u0026#34;fasta\u0026#34;) out = { \u0026#39;df\u0026#39;: df, \u0026#39;samples_list\u0026#39;: samples_list, \u0026#39;recovery_list\u0026#39;: recovery_list, \u0026#39;perplexity_list\u0026#39;: perplexity_list } if \u0026#39;sc_score_eternafold\u0026#39; in metrics: out[\u0026#39;sc_score_eternafold\u0026#39;] = sc_score_eternafold_list if \u0026#39;sc_score_ribonanzanet\u0026#39; in metrics: out[\u0026#39;sc_score_ribonanzanet\u0026#39;] = sc_score_ribonanzanet_list if \u0026#39;sc_score_rhofold\u0026#39; in metrics: out[\u0026#39;sc_score_rmsd\u0026#39;] = sc_score_rmsd_list out[\u0026#39;sc_score_tm\u0026#39;] = sc_score_tm_list out[\u0026#39;sc_score_gddt\u0026#39;] = sc_score_gddt_list out[\u0026#39;rmsd_within_thresh\u0026#39;] = rmsd_within_thresh_list out[\u0026#39;tm_within_thresh\u0026#39;] = tm_within_thresh_list out[\u0026#39;gddt_within_thresh\u0026#39;] = gddt_within_thresh_list # ========================================================= import gc if \u0026#39;ribonanza_net\u0026#39; in locals(): del ribonanza_net if \u0026#39;rhofold\u0026#39; in locals(): del rhofold gc.collect() if torch.cuda.is_available(): torch.cuda.empty_cache() # ========================================================= return out def self_consistency_score_eternafold( samples, true_sec_struct_list, mask_coords, n_samples_ss = 1, num_to_letter = NUM_TO_LETTER, return_sec_structs = False ): \u0026#34;\u0026#34;\u0026#34; Compute self consistency score for an RNA, given its true secondary structure(s) and a list of designed sequences. EternaFold is used to \u0026#39;forward fold\u0026#39; the designs. Args: samples: designed sequences of shape (n_samples, seq_len) true_sec_struct_list: list of true secondary structures (n_true_ss, seq_len) mask_coords: mask for missing sequence coordinates to be ignored during evaluation n_samples_ss: number of predicted secondary structures per designed sample num_to_letter: lookup table mapping integers to nucleotides return_sec_structs: whether to return the predicted secondary structures Workflow: Input: For a given RNA molecule, we are given: - Designed sequences of shape (n_samples, seq_len) - True secondary structure(s) of shape (n_true_ss, seq_len) For each designed sequence: - Predict n_sample_ss secondary structures using EternaFold - For each pair of true and predicted secondary structures: - Compute MCC score between their adjacency matrix representations - Take the average MCC score across all n_sample_ss predicted structures Take the average MCC score across all n_samples designed sequences \u0026#34;\u0026#34;\u0026#34; n_true_ss = len(true_sec_struct_list) sequence_length = mask_coords.sum() # map all entries from dotbracket to numerical representation true_sec_struct_list = np.array([dotbracket_to_adjacency(ss) for ss in true_sec_struct_list]) # mask out missing sequence coordinates true_sec_struct_list = true_sec_struct_list[:, mask_coords][:, :, mask_coords] # reshape to (n_true_ss * n_samples_ss, seq_len, seq_len) true_sec_struct_list = torch.tensor( true_sec_struct_list ).unsqueeze(1).repeat(1, n_samples_ss, 1, 1).reshape(-1, sequence_length, sequence_length) mcc_scores = [] pred_sec_structs = [] for _sample in samples: # convert sample to string pred_seq = \u0026#39;\u0026#39;.join([num_to_letter[num] for num in _sample]) # predict secondary structure(s) for each sample pred_sec_struct_list = predict_sec_struct(pred_seq, n_samples=n_samples_ss) if return_sec_structs: pred_sec_structs.append(copy.copy(pred_sec_struct_list)) # map all entries from dotbracket to numerical representation pred_sec_struct_list = np.array([dotbracket_to_adjacency(ss) for ss in pred_sec_struct_list]) # reshape to (n_samples_ss * n_true_ss, seq_len, seq_len) pred_sec_struct_list = torch.tensor( pred_sec_struct_list ).unsqueeze(0).repeat(n_true_ss, 1, 1, 1).reshape(-1, sequence_length, sequence_length) # compute mean MCC score between pairs of true and predicted secondary structures mcc_scores.append( binary_matthews_corrcoef( pred_sec_struct_list, true_sec_struct_list, ).float().mean() ) if return_sec_structs: return np.array(mcc_scores), pred_sec_structs else: return np.array(mcc_scores) def self_consistency_score_ribonanzanet( samples, true_sequence, mask_seq, ribonanza_net, num_to_letter=NUM_TO_LETTER, return_chem_mods=False, ): \u0026#34;\u0026#34;\u0026#34;Compute self consistency score for an RNA, given the (predicted) chemical modifications for the original RNA and a list of designed sequences. RibonanzaNet is used to \u0026#39;forward fold\u0026#39; the designs. Args: samples: designed sequences of shape (n_samples, seq_len) true_sequence: true RNA sequence used to predict chemical modifications mask_seq: mask for missing sequence coordinates to be ignored during evaluation ribonanza_net: RibonanzaNet model num_to_letter: lookup table mapping integers to nucleotides return_chem_mods: whether to return the predicted chemical modifications Workflow: Input: For a given RNA molecule, we are given: - Designed sequences of shape (n_samples, seq_len) - Predicted chemical modifications for original sequence, of shape (n_samples, seq_len, 2), predicted via RibonanzaNet, of which we take the index 0 from the last channal --\u0026gt; 2A3/SHAPE. For each designed sequence: - Predict chemical modifications using RibonanzaNet - Compute mean absolute error between prediction and chemical modifications for the original sequence Take the average mean absolute error across all n_samples designed sequences \u0026#34;\u0026#34;\u0026#34; # Compute original sequence\u0026#39;s chemical modifications using RibonanzaNet true_sequence = np.array([char for char in true_sequence]) true_sequence = \u0026#34;\u0026#34;.join(true_sequence[mask_seq]) true_chem_mod = ribonanza_net.predict(true_sequence).unsqueeze(0).cpu().numpy()[:,:,0] # 2. 【核心修改】分批处理模型生成的样本 _samples_char = np.array([[num_to_letter[num] for num in seq] for seq in samples]) batch_size = 1 # 定义一个小的批次大小，可以根据显存调整（比如4, 8, 16） all_preds = [] # 用于收集所有批次的预测结果 for i in range(0, len(_samples_char), batch_size): # 取出一个小批次 batch_samples = _samples_char[i:i+batch_size] # 对这个小批次进行预测 batch_pred = ribonanza_net.predict(batch_samples).cpu().numpy()[:,:,0] all_preds.append(batch_pred) # 将所有批次的结果拼接起来 pred_chem_mod = np.concatenate(all_preds, axis=0) # _samples_char = np.array([[num_to_letter[num] for num in seq] for seq in samples]) # _samples = np.array([[num_to_letter[num] for num in seq] for seq in samples]) #pred_chem_mod = ribonanza_net.predict(_samples_char).cpu().numpy()[:,:,0] ##pred_chem_mod = ribonanza_net.predict(_samples[:, mask_seq]).cpu().numpy()[:,:,0] if return_chem_mods: return (np.abs(pred_chem_mod - true_chem_mod).mean(1)), pred_chem_mod else: return np.abs(pred_chem_mod - true_chem_mod).mean(1) def self_consistency_score_ribonanzanet_sec_struct( samples, true_sec_struct, mask_coords, ribonanza_net_ss, num_to_letter = NUM_TO_LETTER, return_sec_structs = False ): # map from dotbracket to numerical representation true_sec_struct = np.array(dotbracket_to_adjacency(true_sec_struct, keep_pseudoknots=True)) # mask out missing sequence coordinates true_sec_struct = true_sec_struct[mask_coords][:, mask_coords] # (n_samples, seq_len, seq_len) true_sec_struct = torch.tensor(true_sec_struct) _samples = np.array([[num_to_letter[num] for num in seq] for seq in samples]) _, pred_sec_structs = ribonanza_net_ss.predict(_samples) # (n_samples, seq_len, seq_len) mcc_scores = [] for pred_sec_struct in pred_sec_structs: # map from dotbracket to numerical representation pred_sec_struct = torch.tensor(dotbracket_to_adjacency(pred_sec_struct, keep_pseudoknots=True)) # compute mean MCC score between pairs of true and predicted secondary structures mcc_scores.append( binary_matthews_corrcoef( pred_sec_struct, true_sec_struct, ).float().mean() ) if return_sec_structs: return np.array(mcc_scores), pred_sec_structs else: return np.array(mcc_scores) def self_consistency_score_rhofold( samples, true_raw_data, mask_coords, rhofold, output_dir, num_to_letter = NUM_TO_LETTER, save_designs = False, save_pdbs = False, use_relax = False, ): \u0026#34;\u0026#34;\u0026#34; Compute self consistency score for an RNA, given its true 3D structure(s) for the original RNA and a list of designed sequences. RhoFold is used to \u0026#39;forward fold\u0026#39; the designs. Credit: adapted from Rishabh Anand Args: samples: designed sequences of shape (n_samples, seq_len) true_raw_data: Original RNA raw data with 3D structure(s) in `coords_list` mask_coords: mask for missing sequence coordinates to be ignored during evaluation rhofold: RhoFold model output_dir: directory to save designed sequences and structures num_to_letter: lookup table mapping integers to nucleotides save_designs: whether to save designs as fasta to output directory save_pdbs: whether to save PDBs of forward-folded designs to output directory use_relax: whether to perform Amber relaxation on designed structures Workflow: Input: For a given RNA molecule, we are given: - Designed sequences of shape (n_samples, seq_len) - True 3D structure(s) of shape (n_true_structs, seq_len, 3) For each designed sequence: - Predict the tertiary structure using RhoFold - For each pair of true and predicted 3D structures: - Compute RMSD, TM-score \u0026amp; GDT between their C4\u0026#39; coordinates Take the average self-consistency scores across all n_samples designed sequences Returns: sc_rmsds: array of RMSD scores per sample sc_tms: array of TM-score scores per sample sc_gddts: array of GDT scores per sample \u0026#34;\u0026#34;\u0026#34; os.makedirs(output_dir, exist_ok=True) # Collate designed sequences in fasta format # first record: input sequence and model metadata input_seq = SeqRecord( Seq(true_raw_data[\u0026#34;sequence\u0026#34;]), id=f\u0026#34;input_sequence,\u0026#34;, description=f\u0026#34;input_sequence\u0026#34; ) # SeqIO.write(input_seq, os.path.join(output_dir, \u0026#34;input_seq.fasta\u0026#34;), \u0026#34;fasta\u0026#34;) sequences = [input_seq] # remaining records: designed sequences and metrics sc_rmsds = [] sc_tms = [] sc_gddts = [] for idx, seq in enumerate(samples): # Save designed sequence to fasta file (temporary) seq = SeqRecord( Seq(\u0026#34;\u0026#34;.join([num_to_letter[num] for num in seq])), id=f\u0026#34;sample={idx},\u0026#34;, description=f\u0026#34;sample={idx}\u0026#34; ) sequences.append(seq) design_fasta_path = os.path.join(output_dir, f\u0026#34;design{idx}.fasta\u0026#34;) SeqIO.write(seq, design_fasta_path, \u0026#34;fasta\u0026#34;) # Forward fold designed sequence using RhoFold design_pdb_path = os.path.join(output_dir, f\u0026#34;design{idx}.pdb\u0026#34;) rhofold.predict(design_fasta_path, design_pdb_path, use_relax) # Load C4\u0026#39; coordinates of designed structure _, coords, _, _ = pdb_to_tensor( design_pdb_path, return_sec_struct=False, return_sasa=False, keep_insertions=False, ) coords = get_c4p_coords(coords) # zero-center coordinates coords = coords - coords.mean(dim=0) # Compute self-consistency between designed and groundtruth structures _sc_rmsds = [] _sc_tms = [] _sc_gddts = [] for other_coords in true_raw_data[\u0026#34;coords_list\u0026#34;]: _other = get_c4p_coords(other_coords)[mask_coords, :] # zero-center other coordinates _other = _other - _other.mean(dim=0) # globally align coordinates R_hat = rotation_matrix( _other, # mobile set coords # reference set )[0] _other = _other @ R_hat.T # compute metrics _sc_rmsds.append(get_rmsd( coords, _other, superposition=True, center=True)) _sc_tms.append(get_tmscore(coords, _other)) _sc_gddts.append(get_gddt(coords, _other)) sc_rmsds.append(np.mean(_sc_rmsds)) sc_tms.append(np.mean(_sc_tms)) sc_gddts.append(np.mean(_sc_gddts)) # remove temporary files os.unlink(design_fasta_path) if save_pdbs is False: os.unlink(design_pdb_path) if save_designs is False: # remove output directory shutil.rmtree(output_dir) else: # write all designed sequences to output filepath SeqIO.write(sequences, os.path.join(output_dir, \u0026#34;all_designs.fasta\u0026#34;), \u0026#34;fasta\u0026#34;) return np.array(sc_rmsds), np.array(sc_tms), np.array(sc_gddts) def get_tmscore(y_hat: torch.Tensor, y: torch.Tensor) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;Template Modelling score (TM-score). Credit: Arian Jamasb, graphein (https://github.com/a-r-j/graphein) https://en.wikipedia.org/wiki/Template_modeling_score TM-score is a measure of similarity between two protein structures. The TM-score is intended as a more accurate measure of the global similarity of full-length protein structures than the often used RMSD measure. The TM-score indicates the similarity between two structures by a score between ``[0, 1]``, where 1 indicates a perfect match between two structures (thus the higher the better). Generally scores below 0.20 corresponds to randomly chosen unrelated proteins whereas structures with a score higher than 0.5 assume roughly the same fold. A quantitative study shows that proteins of TM-score = 0.5 have a posterior probability of 37% in the same CATH topology family and of 13% in the same SCOP fold family. The probabilities increase rapidly when TM-score \u0026gt; 0.5. The TM-score is designed to be independent of protein lengths. We have adapted the implementation to RNA (TM-score threshold = 0.45). Requires aligned C4\u0026#39; coordinates as input. \u0026#34;\u0026#34;\u0026#34; l_target = y.shape[0] d0_l_target = 1.24 * np.power(l_target - 15, 1 / 3) - 1.8 di = torch.pairwise_distance(y_hat, y) out = torch.sum(1 / (1 + (di / d0_l_target) ** 2)) / l_target if torch.isnan(out): return torch.tensor(0.0) return out def get_gddt(y_hat: torch.Tensor, y: torch.Tensor) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34;Global Distance Deviation Test metric (GDDT). Credit: Arian Jamasb, graphein (https://github.com/a-r-j/graphein) https://en.wikipedia.org/wiki/Global_distance_test The GDT score is calculated as the largest set of amino acid residues\u0026#39; alpha carbon atoms in the model structure falling within a defined distance cutoff of their position in the experimental structure, after iteratively superimposing the two structures. By the original design the GDT algorithm calculates 20 GDT scores, i.e. for each of 20 consecutive distance cutoffs (``0.5 Å, 1.0 Å, 1.5 Å, ... 10.0 Å``). For structure similarity assessment it is intended to use the GDT scores from several cutoff distances, and scores generally increase with increasing cutoff. A plateau in this increase may indicate an extreme divergence between the experimental and predicted structures, such that no additional atoms are included in any cutoff of a reasonable distance. The conventional GDT_TS total score in CASP is the average result of cutoffs at ``1``, ``2``, ``4``, and ``8`` Å. Random predictions give around 20; getting the gross topology right gets one to ~50; accurate topology is usually around 70; and when all the little bits and pieces, including side-chain conformations, are correct, GDT_TS begins to climb above 90. We have adapted the implementation to RNA. Requires aligned C4\u0026#39; coordinates as input. \u0026#34;\u0026#34;\u0026#34; # Get distance between points dist = torch.norm(y - y_hat, dim=1) # Return mean fraction of distances below cutoff for each cutoff (1, 2, 4, 8) count_1 = (dist \u0026lt; 1).sum() / dist.numel() count_2 = (dist \u0026lt; 2).sum() / dist.numel() count_4 = (dist \u0026lt; 4).sum() / dist.numel() count_8 = (dist \u0026lt; 8).sum() / dist.numel() out = torch.mean(torch.tensor([count_1, count_2, count_4, count_8])) if torch.isnan(out): return torch.tensor(0.0) return out def edit_distance(s: str, t: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; A Space efficient Dynamic Programming based Python3 program to find minimum number operations to convert str1 to str2 Source: https://www.geeksforgeeks.org/edit-distance-dp-5/ \u0026#34;\u0026#34;\u0026#34; n = len(s) m = len(t) prev = [j for j in range(m+1)] curr = [0] * (m+1) for i in range(1, n+1): curr[0] = i for j in range(1, m+1): if s[i-1] == t[j-1]: curr[j] = prev[j-1] else: mn = min(1 + prev[j], 1 + curr[j-1]) curr[j] = min(mn, 1 + prev[j-1]) prev = curr.copy() return prev[m] 除此之外，需要加一层简单的attention，models.py代码如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 ################################################################ # Generalisation of Geometric Vector Perceptron, Jing et al. # for explicit multi-state biomolecule representation learning. # Original repository: https://github.com/drorlab/gvp-pytorch ################################################################ from typing import Optional import torch from torch import nn import torch.nn.functional as F from torch.distributions import Categorical import torch_geometric from src.layers import * class AutoregressiveMultiGNNv1(torch.nn.Module): \u0026#39;\u0026#39;\u0026#39; Autoregressive GVP-GNN for **multiple** structure-conditioned RNA design. Takes in RNA structure graphs of type `torch_geometric.data.Data` or `torch_geometric.data.Batch` and returns a categorical distribution over 4 bases at each position in a `torch.Tensor` of shape [n_nodes, 4]. The standard forward pass requires sequence information as input and should be used for training or evaluating likelihood. For sampling or design, use `self.sample`. Args: node_in_dim (tuple): node dimensions in input graph node_h_dim (tuple): node dimensions to use in GVP-GNN layers node_in_dim (tuple): edge dimensions in input graph edge_h_dim (tuple): edge dimensions to embed in GVP-GNN layers num_layers (int): number of GVP-GNN layers in encoder/decoder drop_rate (float): rate to use in all dropout layers out_dim (int): output dimension (4 bases) \u0026#39;\u0026#39;\u0026#39; def __init__( self, node_in_dim = (64, 4), node_h_dim = (128, 16), edge_in_dim = (32, 1), edge_h_dim = (32, 1), num_layers = 3, drop_rate = 0.1, out_dim = 4, ): super().__init__() self.node_in_dim = node_in_dim self.node_h_dim = node_h_dim self.edge_in_dim = edge_in_dim self.edge_h_dim = edge_h_dim self.num_layers = num_layers self.out_dim = out_dim activations = (F.silu, None) # Node input embedding self.W_v = torch.nn.Sequential( LayerNorm(self.node_in_dim), GVP(self.node_in_dim, self.node_h_dim, activations=(None, None), vector_gate=True) ) # Edge input embedding self.W_e = torch.nn.Sequential( LayerNorm(self.edge_in_dim), GVP(self.edge_in_dim, self.edge_h_dim, activations=(None, None), vector_gate=True) ) # Encoder layers (supports multiple conformations) self.encoder_layers = nn.ModuleList( MultiGVPConvLayer(self.node_h_dim, self.edge_h_dim, activations=activations, vector_gate=True, drop_rate=drop_rate, norm_first=True) for _ in range(num_layers)) # Simple self-attention on pooled scalar node features self.attn = nn.MultiheadAttention( embed_dim=self.node_h_dim[0], num_heads=4, dropout=drop_rate, batch_first=True ) self.attn_ln = nn.LayerNorm(self.node_h_dim[0]) # Decoder layers self.W_s = nn.Embedding(self.out_dim, self.out_dim) self.edge_h_dim = (self.edge_h_dim[0] + self.out_dim, self.edge_h_dim[1]) self.decoder_layers = nn.ModuleList( GVPConvLayer(self.node_h_dim, self.edge_h_dim, activations=activations, vector_gate=True, drop_rate=drop_rate, autoregressive=True, norm_first=True) for _ in range(num_layers)) # Output self.W_out = GVP(self.node_h_dim, (self.out_dim, 0), activations=(None, None)) def forward(self, batch): h_V = (batch.node_s, batch.node_v) h_E = (batch.edge_s, batch.edge_v) edge_index = batch.edge_index seq = batch.seq h_V = self.W_v(h_V) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) h_E = self.W_e(h_E) # (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3) for layer in self.encoder_layers: h_V = layer(h_V, edge_index, h_E) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) # Pool multi-conformation features: # nodes: (n_nodes, d_s), (n_nodes, d_v, 3) # edges: (n_edges, d_se), (n_edges, d_ve, 3) h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) # Apply simple self-attention over nodes (sequence length = n_nodes) x = h_V[0].unsqueeze(0) # (1, n_nodes, d_s) attn_out, _ = self.attn(x, x, x, need_weights=False) x = self.attn_ln(x + attn_out) h_V = (x.squeeze(0), h_V[1]) encoder_embeddings = h_V h_S = self.W_s(seq) h_S = h_S[edge_index[0]] h_S[edge_index[0] \u0026gt;= edge_index[1]] = 0 h_E = (torch.cat([h_E[0], h_S], dim=-1), h_E[1]) for layer in self.decoder_layers: h_V = layer(h_V, edge_index, h_E, autoregressive_x = encoder_embeddings) logits = self.W_out(h_V) return logits @torch.no_grad() def sample( self, batch, n_samples, temperature: Optional[float] = 0.1, logit_bias: Optional[torch.Tensor] = None, return_logits: Optional[bool] = False ): \u0026#39;\u0026#39;\u0026#39; Samples sequences autoregressively from the distribution learned by the model. Args: batch (torch_geometric.data.Data): mini-batch containing one RNA backbone to design sequences for n_samples (int): number of samples temperature (float): temperature to use in softmax over the categorical distribution logit_bias (torch.Tensor): bias to add to logits during sampling to manually fix or control nucleotides in designed sequences, of shape [n_nodes, 4] return_logits (bool): whether to return logits or not Returns: seq (torch.Tensor): int tensor of shape [n_samples, n_nodes] based on the residue-to-int mapping of the original training data logits (torch.Tensor): logits of shape [n_samples, n_nodes, 4] (only if return_logits is True) \u0026#39;\u0026#39;\u0026#39; h_V = (batch.node_s, batch.node_v) h_E = (batch.edge_s, batch.edge_v) edge_index = batch.edge_index device = edge_index.device num_nodes = h_V[0].shape[0] h_V = self.W_v(h_V) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) h_E = self.W_e(h_E) # (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3) for layer in self.encoder_layers: h_V = layer(h_V, edge_index, h_E) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) # Pool multi-conformation features # nodes: (n_nodes, d_s), (n_nodes, d_v, 3) # edges: (n_edges, d_se), (n_edges, d_ve, 3) h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) # Apply simple self-attention over nodes (sequence length = n_nodes) x = h_V[0].unsqueeze(0) # (1, n_nodes, d_s) attn_out, _ = self.attn(x, x, x, need_weights=False) x = self.attn_ln(x + attn_out) h_V = (x.squeeze(0), h_V[1]) # Repeat features for sampling n_samples times h_V = (h_V[0].repeat(n_samples, 1), h_V[1].repeat(n_samples, 1, 1)) h_E = (h_E[0].repeat(n_samples, 1), h_E[1].repeat(n_samples, 1, 1)) # Expand edge index for autoregressive decoding edge_index = edge_index.expand(n_samples, -1, -1) offset = num_nodes * torch.arange(n_samples, device=device).view(-1, 1, 1) edge_index = torch.cat(tuple(edge_index + offset), dim=-1) # This is akin to \u0026#39;batching\u0026#39; (in PyG style) n_samples copies of the graph seq = torch.zeros(n_samples * num_nodes, device=device, dtype=torch.int) h_S = torch.zeros(n_samples * num_nodes, self.out_dim, device=device) logits = torch.zeros(n_samples * num_nodes, self.out_dim, device=device) h_V_cache = [(h_V[0].clone(), h_V[1].clone()) for _ in self.decoder_layers] # Decode one token at a time for i in range(num_nodes): h_S_ = h_S[edge_index[0]] h_S_[edge_index[0] \u0026gt;= edge_index[1]] = 0 h_E_ = (torch.cat([h_E[0], h_S_], dim=-1), h_E[1]) edge_mask = edge_index[1] % num_nodes == i # True for all edges where dst is node i edge_index_ = edge_index[:, edge_mask] # subset all incoming edges to node i h_E_ = tuple_index(h_E_, edge_mask) node_mask = torch.zeros(n_samples * num_nodes, device=device, dtype=torch.bool) node_mask[i::num_nodes] = True # True for all nodes i and its repeats for j, layer in enumerate(self.decoder_layers): out = layer(h_V_cache[j], edge_index_, h_E_, autoregressive_x=h_V_cache[0], node_mask=node_mask) out = tuple_index(out, node_mask) # subset out to only node i and its repeats if j \u0026lt; len(self.decoder_layers)-1: h_V_cache[j+1][0][i::num_nodes] = out[0] h_V_cache[j+1][1][i::num_nodes] = out[1] lgts = self.W_out(out) # Add logit bias if provided to fix or bias positions if logit_bias is not None: lgts += logit_bias[i] # Sample from logits seq[i::num_nodes] = Categorical(logits=lgts / temperature).sample() h_S[i::num_nodes] = self.W_s(seq[i::num_nodes]) logits[i::num_nodes] = lgts if return_logits: return seq.view(n_samples, num_nodes), logits.view(n_samples, num_nodes, self.out_dim) else: return seq.view(n_samples, num_nodes) def pool_multi_conf(self, h_V, h_E, mask_confs, edge_index): if mask_confs.size(1) == 1: # Number of conformations is 1, no need to pool return (h_V[0][:, 0], h_V[1][:, 0]), (h_E[0][:, 0], h_E[1][:, 0]) # True num_conf for masked mean pooling n_conf_true = mask_confs.sum(1, keepdim=True) # (n_nodes, 1) # Mask scalar features mask = mask_confs.unsqueeze(2) # (n_nodes, n_conf, 1) h_V0 = h_V[0] * mask h_E0 = h_E[0] * mask[edge_index[0]] # Mask vector features mask = mask.unsqueeze(3) # (n_nodes, n_conf, 1, 1) h_V1 = h_V[1] * mask h_E1 = h_E[1] * mask[edge_index[0]] # Average pooling multi-conformation features h_V = (h_V0.sum(dim=1) / n_conf_true, # (n_nodes, d_s) h_V1.sum(dim=1) / n_conf_true.unsqueeze(2)) # (n_nodes, d_v, 3) h_E = (h_E0.sum(dim=1) / n_conf_true[edge_index[0]], # (n_edges, d_se) h_E1.sum(dim=1) / n_conf_true[edge_index[0]].unsqueeze(2)) # (n_edges, d_ve, 3) return h_V, h_E class NonAutoregressiveMultiGNNv1(torch.nn.Module): \u0026#39;\u0026#39;\u0026#39; Non-Autoregressive GVP-GNN for **multiple** structure-conditioned RNA design. Takes in RNA structure graphs of type `torch_geometric.data.Data` or `torch_geometric.data.Batch` and returns a categorical distribution over 4 bases at each position in a `torch.Tensor` of shape [n_nodes, 4]. The standard forward pass requires sequence information as input and should be used for training or evaluating likelihood. For sampling or design, use `self.sample`. Args: node_in_dim (tuple): node dimensions in input graph node_h_dim (tuple): node dimensions to use in GVP-GNN layers node_in_dim (tuple): edge dimensions in input graph edge_h_dim (tuple): edge dimensions to embed in GVP-GNN layers num_layers (int): number of GVP-GNN layers in encoder/decoder drop_rate (float): rate to use in all dropout layers out_dim (int): output dimension (4 bases) \u0026#39;\u0026#39;\u0026#39; def __init__( self, node_in_dim = (64, 4), node_h_dim = (128, 16), edge_in_dim = (32, 1), edge_h_dim = (32, 1), num_layers = 3, drop_rate = 0.1, out_dim = 4, ): super().__init__() self.node_in_dim = node_in_dim self.node_h_dim = node_h_dim self.edge_in_dim = edge_in_dim self.edge_h_dim = edge_h_dim self.num_layers = num_layers self.out_dim = out_dim activations = (F.silu, None) # Node input embedding self.W_v = torch.nn.Sequential( LayerNorm(self.node_in_dim), GVP(self.node_in_dim, self.node_h_dim, activations=(None, None), vector_gate=True) ) # Edge input embedding self.W_e = torch.nn.Sequential( LayerNorm(self.edge_in_dim), GVP(self.edge_in_dim, self.edge_h_dim, activations=(None, None), vector_gate=True) ) # Encoder layers (supports multiple conformations) self.encoder_layers = nn.ModuleList( MultiGVPConvLayer(self.node_h_dim, self.edge_h_dim, activations=activations, vector_gate=True, drop_rate=drop_rate, norm_first=True) for _ in range(num_layers)) # Simple self-attention on pooled scalar node features self.attn = nn.MultiheadAttention( embed_dim=self.node_h_dim[0], num_heads=4, dropout=drop_rate, batch_first=True ) self.attn_ln = nn.LayerNorm(self.node_h_dim[0]) # Output self.W_out = torch.nn.Sequential( LayerNorm(self.node_h_dim), GVP(self.node_h_dim, self.node_h_dim, activations=(None, None), vector_gate=True), GVP(self.node_h_dim, (self.out_dim, 0), activations=(None, None)) ) def forward(self, batch): h_V = (batch.node_s, batch.node_v) h_E = (batch.edge_s, batch.edge_v) edge_index = batch.edge_index h_V = self.W_v(h_V) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) h_E = self.W_e(h_E) # (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3) for layer in self.encoder_layers: h_V = layer(h_V, edge_index, h_E) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) # Pool multi-conformation features: # nodes: (n_nodes, d_s), (n_nodes, d_v, 3) # edges: (n_edges, d_se), (n_edges, d_ve, 3) # h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) h_V = (h_V[0].mean(dim=1), h_V[1].mean(dim=1)) # Apply simple self-attention over nodes (sequence length = n_nodes) x = h_V[0].unsqueeze(0) # (1, n_nodes, d_s) attn_out, _ = self.attn(x, x, x, need_weights=False) x = self.attn_ln(x + attn_out) h_V = (x.squeeze(0), h_V[1]) logits = self.W_out(h_V) # (n_nodes, out_dim) return logits def sample(self, batch, n_samples, temperature=0.1, return_logits=False): with torch.no_grad(): h_V = (batch.node_s, batch.node_v) h_E = (batch.edge_s, batch.edge_v) edge_index = batch.edge_index h_V = self.W_v(h_V) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) h_E = self.W_e(h_E) # (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3) for layer in self.encoder_layers: h_V = layer(h_V, edge_index, h_E) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) # Pool multi-conformation features # h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) h_V = (h_V[0].mean(dim=1), h_V[1].mean(dim=1)) logits = self.W_out(h_V) # (n_nodes, out_dim) probs = F.softmax(logits / temperature, dim=-1) seq = torch.multinomial(probs, n_samples, replacement=True) # (n_nodes, n_samples) if return_logits: return seq.permute(1, 0).contiguous(), logits.unsqueeze(0).repeat(n_samples, 1, 1) else: return seq.permute(1, 0).contiguous() def pool_multi_conf(self, h_V, h_E, mask_confs, edge_index): if mask_confs.size(1) == 1: # Number of conformations is 1, no need to pool return (h_V[0][:, 0], h_V[1][:, 0]), (h_E[0][:, 0], h_E[1][:, 0]) # True num_conf for masked mean pooling n_conf_true = mask_confs.sum(1, keepdim=True) # (n_nodes, 1) # Mask scalar features mask = mask_confs.unsqueeze(2) # (n_nodes, n_conf, 1) h_V0 = h_V[0] * mask h_E0 = h_E[0] * mask[edge_index[0]] # Mask vector features mask = mask.unsqueeze(3) # (n_nodes, n_conf, 1, 1) h_V1 = h_V[1] * mask h_E1 = h_E[1] * mask[edge_index[0]] # Average pooling multi-conformation features h_V = (h_V0.sum(dim=1) / n_conf_true, # (n_nodes, d_s) h_V1.sum(dim=1) / n_conf_true.unsqueeze(2)) # (n_nodes, d_v, 3) h_E = (h_E0.sum(dim=1) / n_conf_true[edge_index[0]], # (n_edges, d_se) h_E1.sum(dim=1) / n_conf_true[edge_index[0]].unsqueeze(2)) # (n_edges, d_ve, 3) return h_V, h_E 而由于本地服务器一般很难登上wandb，而作者原版代码在路径上用了很多相关的，因而在trainer.py更改路径，\n1 2 3 4 5 6 7 8 if device.type == \u0026#39;xpu\u0026#39;: import intel_extension_for_pytorch as ipex model, optimizer = ipex.optimize(model, optimizer=optimizer) #=======上面的和作者的一样======= # Initialise save directory save_dir = os.path.join(os.path.dirname(__file__), \u0026#34;..\u0026#34;, \u0026#34;mymodel\u0026#34;) #save_dir = os.path.abspath(save_dir) 这是换成绝对路径，可以不需要；模型保存在主项目目录的mymodel文件夹。 os.makedirs(save_dir, exist_ok=True) 之后把wandb.run.dir路径改为save_dir路径，防止训练模型后没有文件；模型默认选择autoaggresive。\n注意，训练模型最好挂在后台运行，具体命令如下：\n1 nohup python main.py --no_wandb \u0026gt; main.log 2\u0026gt;\u0026amp;1 \u0026amp; 然后可以查看log日志，\n1 tail -f main.log 以及及时选择查看gpu的运行情况，例如，\n1 nvidia-smi 第一次测试训练的模型 模型以best_checkpoint.h5形式得出，然后根据作者的gRNAde.py脚本，只需修改以下的加载路径和其他没啥用的print字符串就可以，由于多态生成我没有用，因此我没替换那个；其余俩都得替换。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 CHECKPOINT_PATH = { \u0026#39;all\u0026#39;: { 1: os.path.join(PROJECT_PATH, \u0026#34;mymodel/best_checkpoint.h5\u0026#34;),#修改为mymodel里的checkpoint 2: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_2state_all.h5\u0026#34;), 3: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_3state_all.h5\u0026#34;), 5: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_5state_all.h5\u0026#34;), }, \u0026#39;das\u0026#39;: { 1: os.path.join(PROJECT_PATH, \u0026#34;mymodel/best_checkpoint.h5\u0026#34;),#修改为mymodel里的checkpoint 2: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_2state_das.h5\u0026#34;), 3: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_3state_das.h5\u0026#34;), 5: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_5state_das.h5\u0026#34;), }, \u0026#39;multi\u0026#39;: { 1: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_1state_multi.h5\u0026#34;), 2: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_2state_multi.h5\u0026#34;), 3: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_3state_multi.h5\u0026#34;), 5: os.path.join(PROJECT_PATH, \u0026#34;checkpoints/gRNAde_ARv1_5state_multi.h5\u0026#34;), } } 除此之外，通过命令行人工测试太费劲了，需要写一个自动化脚本。具体逻辑就是，我在data/raw里面取一些名称作为索引，保存在.txt文件中；然后用脚本把原先命令行的命令包含进去，input文件路径下改为按索引名称.pdb递增，输出.fasta文件同理，注意models.py文件要保持同步！！！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 #这是自动化测试脚本 import os import subprocess import sys from pathlib import Path def read_test_index(index_file=\u0026#34;test.txt\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Read test_index.txt file to get the list of PDB files \u0026#34;\u0026#34;\u0026#34; try: with open(index_file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: lines = f.readlines() # Filter out empty lines and comment lines, remove newline characters pdb_files = [] for line in lines: line = line.strip() if line and not line.startswith(\u0026#39;#\u0026#39;): pdb_files.append(line) return pdb_files except FileNotFoundError: print(f\u0026#34;Error: File {index_file} not found\u0026#34;) return [] except Exception as e: print(f\u0026#34;Error reading file: {e}\u0026#34;) return [] def extract_filename_without_extension(pdb_filename): \u0026#34;\u0026#34;\u0026#34; Extract the filename without extension from a PDB filename Example: 100D_1_A-B.pdb -\u0026gt; 100D_1_A-B \u0026#34;\u0026#34;\u0026#34; return Path(pdb_filename).stem def run_command(pdb_file): \u0026#34;\u0026#34;\u0026#34; Execute the run.py command for the specified PDB file \u0026#34;\u0026#34;\u0026#34; # Extract filename (without extension) for output filename output_name = extract_filename_without_extension(pdb_file) # Build command cmd = [ \u0026#34;python\u0026#34;, \u0026#34;run.py\u0026#34;, \u0026#34;--pdb_filepath\u0026#34;, f\u0026#34;data/raw/{pdb_file}\u0026#34;, \u0026#34;--output_filepath\u0026#34;, f\u0026#34;testmyrna/{output_name}.fasta\u0026#34;, \u0026#34;--split\u0026#34;, \u0026#34;das\u0026#34;, \u0026#34;--max_num_conformers\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;--n_samples\u0026#34;, \u0026#34;16\u0026#34;, \u0026#34;--temperature\u0026#34;, \u0026#34;0.5\u0026#34; ] print(f\u0026#34;Processing: {pdb_file}\u0026#34;) print(f\u0026#34;Executing command: {\u0026#39; \u0026#39;.join(cmd)}\u0026#34;) try: # Execute command result = subprocess.run(cmd, capture_output=True, text=True, check=True) print(f\u0026#34;✓ Successfully processed {pdb_file}\u0026#34;) print(f\u0026#34;Output file: testmyrna/{output_name}.fasta\u0026#34;) # Display part of the output if available if result.stdout: print(\u0026#34;Standard output:\u0026#34;) print(result.stdout[:200] + (\u0026#34;...\u0026#34; if len(result.stdout) \u0026gt; 200 else \u0026#34;\u0026#34;)) return True except subprocess.CalledProcessError as e: print(f\u0026#34;✗ Failed to process {pdb_file}\u0026#34;) print(f\u0026#34;Error code: {e.returncode}\u0026#34;) if e.stderr: print(f\u0026#34;Error message: {e.stderr}\u0026#34;) return False except Exception as e: print(f\u0026#34;✗ Unexpected error occurred while processing {pdb_file}: {e}\u0026#34;) return False def main(): \u0026#34;\u0026#34;\u0026#34; Main function \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;=== RNA Test Script Started ===\u0026#34;) # Check for required files and directories if not os.path.exists(\u0026#34;run.py\u0026#34;): print(\u0026#34;Error: run.py file not found in the current directory\u0026#34;) sys.exit(1) if not os.path.exists(\u0026#34;test.txt\u0026#34;): print(\u0026#34;Error: test.txt file not found in the current directory\u0026#34;) sys.exit(1) # Ensure output directory exists output_dir = \u0026#34;testmyrna\u0026#34; os.makedirs(output_dir, exist_ok=True) print(f\u0026#34;Output directory: {output_dir}\u0026#34;) # Read test index file pdb_files = read_test_index() if not pdb_files: print(\u0026#34;Warning: test.txt file is empty or failed to read\u0026#34;) sys.exit(1) print(f\u0026#34;Found {len(pdb_files)} PDB files to process:\u0026#34;) for i, pdb_file in enumerate(pdb_files, 1): print(f\u0026#34; {i}. {pdb_file}\u0026#34;) print(\u0026#34;\\nStarting processing...\u0026#34;) # Statistics success_count = 0 failed_files = [] # Process each PDB file for i, pdb_file in enumerate(pdb_files, 1): print(f\u0026#34;\\n[{i}/{len(pdb_files)}] \u0026#34; + \u0026#34;=\u0026#34;*50) # Check if input file exists input_path = f\u0026#34;data/raw/{pdb_file}\u0026#34; if not os.path.exists(input_path): print(f\u0026#34;Warning: Input file {input_path} does not exist, skipping...\u0026#34;) failed_files.append(pdb_file) continue # Execute command if run_command(pdb_file): success_count += 1 else: failed_files.append(pdb_file) # Print summary print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*60) print(\u0026#34;=== Processing Completed ===\u0026#34;) print(f\u0026#34;Total: {len(pdb_files)} files\u0026#34;) print(f\u0026#34;Success: {success_count} files\u0026#34;) print(f\u0026#34;Failed: {len(failed_files)} files\u0026#34;) if failed_files: print(\u0026#34;\\nFailed files:\u0026#34;) for failed_file in failed_files: print(f\u0026#34; - {failed_file}\u0026#34;) print(f\u0026#34;\\nOutput files saved in: {output_dir}/\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() 注意，有时sec_struct_utils.py文件清除缓存速度太快了，所以我进行修改，首先把这部分清除的注释了：\n1 2 3 4 5 6 7 8 9 10 output = subprocess.run(cmd, check=True, capture_output=True).stdout.decode(\u0026#34;utf-8\u0026#34;) # Delete temporary files这三行注释掉 # if sequence is not None: # os.remove(fasta_file_path) if n_samples \u0026gt; 1: return output.split(\u0026#34;\\n\u0026#34;)[:-1] else: return [output.split(\u0026#34;\\n\u0026#34;)[-2]] 之后把路径改了，\n1 2 3 4 5 current_datetime = datetime.now().strftime(\u0026#34;%Y%m%d_%H%M%S\u0026#34;) try: fasta_file_path = os.path.join(wandb.run.dir, f\u0026#34;temp_{current_datetime}.fasta\u0026#34;) except AttributeError: fasta_file_path = os.path.join(PROJECT_PATH, \u0026#34;temp\u0026#34;, f\u0026#34;temp_{current_datetime}.fasta\u0026#34;)#改这里路径后是这样，我一般不用wandb 生成在某文件夹后，需要用新的脚本处理数据，由于我的sample为16，因而需要分别计算length, perplexity, recovery, edit distance, SC Score的平均值，并按照列的顺序保存为.txt文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 import os import re import glob from pathlib import Path def parse_fasta_file(fasta_path): \u0026#34;\u0026#34;\u0026#34; Parse a single FASTA file to extract the input sequence length and metrics for 16 samples Returns: - input_length: Length of the input sequence - avg_metrics: Average metrics for 16 samples {perplexity, recovery, edit_dist, sc_score} \u0026#34;\u0026#34;\u0026#34; try: with open(fasta_path, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: content = f.read() # Split different sequence blocks sequences = content.strip().split(\u0026#39;\u0026gt;\u0026#39;) sequences = [seq.strip() for seq in sequences if seq.strip()] input_length = 0 sample_metrics = [] for seq_block in sequences: lines = seq_block.strip().split(\u0026#39;\\n\u0026#39;) if not lines: continue header = lines[0] sequence_lines = lines[1:] # Process input sequence if \u0026#39;input_sequence\u0026#39; in header: # Combine all sequence lines and calculate length sequence = \u0026#39;\u0026#39;.join(sequence_lines).replace(\u0026#39; \u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) input_length = len(sequence) print(f\u0026#34; Input sequence length: {input_length}\u0026#34;) # Process sample sequence elif \u0026#39;sample=\u0026#39; in header: # Extract metrics using regular expressions perplexity_match = re.search(r\u0026#39;perplexity=([0-9.]+)\u0026#39;, header) recovery_match = re.search(r\u0026#39;recovery=([0-9.]+)\u0026#39;, header) edit_dist_match = re.search(r\u0026#39;edit_dist=([0-9.]+)\u0026#39;, header) sc_score_match = re.search(r\u0026#39;sc_score=([0-9.]+)\u0026#39;, header) if all([perplexity_match, recovery_match, edit_dist_match, sc_score_match]): metrics = { \u0026#39;perplexity\u0026#39;: float(perplexity_match.group(1)), \u0026#39;recovery\u0026#39;: float(recovery_match.group(1)), \u0026#39;edit_dist\u0026#39;: float(edit_dist_match.group(1)), \u0026#39;sc_score\u0026#39;: float(sc_score_match.group(1)) } sample_metrics.append(metrics) # Calculate averages if sample_metrics: avg_metrics = { \u0026#39;perplexity\u0026#39;: sum(m[\u0026#39;perplexity\u0026#39;] for m in sample_metrics) / len(sample_metrics), \u0026#39;recovery\u0026#39;: sum(m[\u0026#39;recovery\u0026#39;] for m in sample_metrics) / len(sample_metrics), \u0026#39;edit_dist\u0026#39;: sum(m[\u0026#39;edit_dist\u0026#39;] for m in sample_metrics) / len(sample_metrics), \u0026#39;sc_score\u0026#39;: sum(m[\u0026#39;sc_score\u0026#39;] for m in sample_metrics) / len(sample_metrics) } print(f\u0026#34; Found {len(sample_metrics)} samples\u0026#34;) print(f\u0026#34; Average metrics: perplexity={avg_metrics[\u0026#39;perplexity\u0026#39;]:.4f}, recovery={avg_metrics[\u0026#39;recovery\u0026#39;]:.4f}, edit_dist={avg_metrics[\u0026#39;edit_dist\u0026#39;]:.4f}, sc_score={avg_metrics[\u0026#39;sc_score\u0026#39;]:.4f}\u0026#34;) else: print(\u0026#34; Warning: No valid sample metrics found\u0026#34;) avg_metrics = None return input_length, avg_metrics except Exception as e: print(f\u0026#34; Error: Exception occurred while processing file: {e}\u0026#34;) return 0, None def process_all_fasta_files(input_dir=\u0026#34;tout\u0026#34;, output_file=\u0026#34;data/plotdata/plot.txt\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Process all FASTA files in the specified directory \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;=== FASTA File Processing Script ===\u0026#34;) # Check input directory if not os.path.exists(input_dir): print(f\u0026#34;Error: Input directory does not exist: {input_dir}\u0026#34;) return # Create output directory output_dir = os.path.dirname(output_file) if output_dir: os.makedirs(output_dir, exist_ok=True) print(f\u0026#34;Output directory: {output_dir}\u0026#34;) # Find all FASTA files fasta_pattern = os.path.join(input_dir, \u0026#34;*.fasta\u0026#34;) fasta_files = glob.glob(fasta_pattern) if not fasta_files: print(f\u0026#34;Warning: No .fasta files found in {input_dir}\u0026#34;) return print(f\u0026#34;Found {len(fasta_files)} FASTA files\u0026#34;) # Store processing results results = [] processed_count = 0 failed_count = 0 # Process each FASTA file for i, fasta_file in enumerate(sorted(fasta_files), 1): filename = os.path.basename(fasta_file) print(f\u0026#34;\\n[{i}/{len(fasta_files)}] Processing file: {filename}\u0026#34;) input_length, avg_metrics = parse_fasta_file(fasta_file) if input_length \u0026gt; 0 and avg_metrics is not None: # Format result result_line = f\u0026#34;{input_length} {avg_metrics[\u0026#39;perplexity\u0026#39;]:.4f} {avg_metrics[\u0026#39;recovery\u0026#39;]:.4f} {avg_metrics[\u0026#39;edit_dist\u0026#39;]:.4f} {avg_metrics[\u0026#39;sc_score\u0026#39;]:.4f}\u0026#34; results.append(result_line) processed_count += 1 print(f\u0026#34; ✓ Processed successfully\u0026#34;) else: print(f\u0026#34; ✗ Processing failed\u0026#34;) failed_count += 1 # Save results if results: try: with open(output_file, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: for result in results: f.write(result + \u0026#39;\\n\u0026#39;) print(f\u0026#34;\\n=== Processing Completed ===\u0026#34;) print(f\u0026#34;Total files: {len(fasta_files)}\u0026#34;) print(f\u0026#34;Successfully processed: {processed_count}\u0026#34;) print(f\u0026#34;Failed: {failed_count}\u0026#34;) print(f\u0026#34;Results saved to: {output_file}\u0026#34;) # Show preview of first few lines print(f\u0026#34;\\nResults preview (first 5 lines):\u0026#34;) for i, result in enumerate(results[:5]): print(f\u0026#34; {result}\u0026#34;) if len(results) \u0026gt; 5: print(f\u0026#34; ... (total {len(results)} lines)\u0026#34;) except Exception as e: print(f\u0026#34;Error saving file: {e}\u0026#34;) else: print(\u0026#34;No successfully processed data, unable to generate output file\u0026#34;) def validate_output_format(output_file): \u0026#34;\u0026#34;\u0026#34; Validate the format of the output file \u0026#34;\u0026#34;\u0026#34; if not os.path.exists(output_file): print(\u0026#34;Output file does not exist\u0026#34;) return print(f\u0026#34;\\n=== Validate Output Format ===\u0026#34;) try: with open(output_file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: lines = f.readlines() print(f\u0026#34;Total lines: {len(lines)}\u0026#34;) for i, line in enumerate(lines[:3], 1): # Check first 3 lines parts = line.strip().split() if len(parts) == 5: length = int(parts[0]) perplexity = float(parts[1]) recovery = float(parts[2]) edit_dist = float(parts[3]) sc_score = float(parts[4]) print(f\u0026#34;Line {i}: length={length}, perplexity={perplexity}, recovery={recovery}, edit_dist={edit_dist}, sc_score={sc_score}\u0026#34;) else: print(f\u0026#34;Line {i} format error: {line.strip()}\u0026#34;) except Exception as e: print(f\u0026#34;Error during validation: {e}\u0026#34;) def main(): \u0026#34;\u0026#34;\u0026#34; Main function \u0026#34;\u0026#34;\u0026#34; # Set input and output paths input_directory = \u0026#34;tout\u0026#34; output_filepath = \u0026#34;data/plotdata/plotgrna.txt\u0026#34; # Process all FASTA files process_all_fasta_files(input_directory, output_filepath) # Validate output format validate_output_format(output_filepath) if __name__ == \u0026#34;__main__\u0026#34;: main() 以及matlab的绘图代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 % Script to visualize RNA prediction metrics from else.txt % Load data data = load(\u0026#39;g.txt\u0026#39;); % Format: [length, perplexity, recovery, edit_dist, sc_score] lengths = data(:, 1); perplexity = data(:, 2); recovery = data(:, 3); edit_dist = data(:, 4); sc_score = data(:, 5); % Compute unique lengths and their average metrics [unique_lengths, ~, idx] = unique(lengths); n = length(unique_lengths); avg_perplexity = zeros(n,1); avg_recovery = zeros(n,1); avg_edit_dist = zeros(n,1); avg_sc_score = zeros(n,1); for i = 1:n avg_perplexity(i) = mean(perplexity(idx == i)); avg_recovery(i) = mean(recovery(idx == i)); avg_edit_dist(i) = mean(edit_dist(idx == i)); avg_sc_score(i) = mean(sc_score(idx == i)); end % Prepare 2x2 subplot figure(\u0026#39;Name\u0026#39;,\u0026#39;RNA Prediction Metrics\u0026#39;,\u0026#39;NumberTitle\u0026#39;,\u0026#39;off\u0026#39;); metrics = {avg_perplexity, avg_recovery, avg_edit_dist, avg_sc_score}; titles = { \u0026#39;Variation of Perplexity with RNA Sequence Length\u0026#39;, ... \u0026#39;Variation of Recovery Rate with RNA Sequence Length\u0026#39;, ... \u0026#39;Variation of Edit Distance with RNA Sequence Length\u0026#39;, ... \u0026#39;Variation of Structural Conservation Score with RNA Sequence Length\u0026#39; }; ylabels = {\u0026#39;Average Perplexity\u0026#39;, \u0026#39;Average Recovery Rate\u0026#39;, ... \u0026#39;Average Edit Distance\u0026#39;, \u0026#39;Average SC Score\u0026#39;}; % Set Times New Roman font for all text set(0,\u0026#39;defaultAxesFontName\u0026#39;,\u0026#39;Times New Roman\u0026#39;); set(0,\u0026#39;defaultTextFontName\u0026#39;,\u0026#39;Times New Roman\u0026#39;); markerSize = 10; markerColor = [0.9, 0.2, 0.2]; lineWidth = 1; for i = 1:4 subplot(2,2,i); scatter(unique_lengths, metrics{i}, markerSize, ... \u0026#39;MarkerEdgeColor\u0026#39;, markerColor, ... \u0026#39;MarkerFaceColor\u0026#39;, markerColor, ... \u0026#39;LineWidth\u0026#39;, lineWidth); xlabel(\u0026#39;RNA Sequence Length (nt)\u0026#39;,\u0026#39;FontName\u0026#39;,\u0026#39;Times New Roman\u0026#39;); ylabel(ylabels{i},\u0026#39;FontName\u0026#39;,\u0026#39;Times New Roman\u0026#39;); title(titles{i},\u0026#39;FontName\u0026#39;,\u0026#39;Times New Roman\u0026#39;); grid on; set(gca,\u0026#39;FontName\u0026#39;,\u0026#39;Times New Roman\u0026#39;); end % Adjust layout sgtitle(\u0026#39;Comparative Analysis of GRNADE Prediction Metrics\u0026#39;,\u0026#39;FontName\u0026#39;,\u0026#39;Times New Roman\u0026#39;); 经过实验与对比，起初的问题是，当epoch = 1的时候，模型简直非常差：\n很差的效果 但是当epoch提升到50，肉眼可见的好多了，和作者训练的模型几乎一致，因此代码是没问题的：\nepoch=50的效果 然而在我训练后发现我加了attention的模型与不加一模一样，后来发现，我在nonaggressive里面加的，而模型默认没选择这个。\n效果几乎一模一样，很奇怪 在之后，我重新修改了multiheadattention代码，训练好模型后如下，进行预测\n我的模型 使用情况正常 我随机抽取了500个datapoint后进行测试，结果如图：\n实际情况看来，貌似有效果但不大 因此我在考虑multi-scale attention的想法，根据论文Atlas: Multi-Scale Attention Improves Long Context Image Modeling的思路与AI的协助（实际上压力ai了），现在模型是如下这样，训练之后在进行测试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 ################################################################ # Generalisation of Geometric Vector Perceptron, Jing et al. # for explicit multi-state biomolecule representation learning. # Original repository: https://github.com/drorlab/gvp-pytorch ################################################################ from typing import Optional import torch from torch import nn import torch.nn.functional as F from torch.distributions import Categorical import torch_geometric from src.layers import * class MultiScaleAttention(nn.Module): \u0026#39;\u0026#39;\u0026#39; Multi-scale attention module to capture dependencies at different window sizes. \u0026#39;\u0026#39;\u0026#39; def __init__( self, embed_dim: int, num_heads: int = 8, window_sizes: list = [10, 50, 200, None], # None for global scale dropout: float = 0.1 ): super().__init__() self.window_sizes = window_sizes self.attentions = nn.ModuleList([ nn.MultiheadAttention( embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, batch_first=True ) for _ in window_sizes ]) def forward(self, x, mask=None): # x: (batch_size, seq_len, embed_dim) outputs = [] seq_len = x.size(1) for idx, (attn, window_size) in enumerate(zip(self.attentions, self.window_sizes)): if window_size is None: # Global attention attn_output, _ = attn(x, x, x, attn_mask=mask) else: # Local window attention: create sliding window mask or approximate # For simplicity, we can use full attention but in practice, implement windowing # Here, as a placeholder, apply full attention per scale (can be optimized with sparse masks) attn_output, _ = attn(x, x, x, attn_mask=mask) outputs.append(attn_output) # Fuse outputs: average across scales fused_output = torch.mean(torch.stack(outputs), dim=0) return fused_output class AutoregressiveMultiGNNv1(torch.nn.Module): \u0026#39;\u0026#39;\u0026#39; Autoregressive GVP-GNN for **multiple** structure-conditioned RNA design. Takes in RNA structure graphs of type `torch_geometric.data.Data` or `torch_geometric.data.Batch` and returns a categorical distribution over 4 bases at each position in a `torch.Tensor` of shape [n_nodes, 4]. The standard forward pass requires sequence information as input and should be used for training or evaluating likelihood. For sampling or design, use `self.sample`. Args: node_in_dim (tuple): node dimensions in input graph node_h_dim (tuple): node dimensions to use in GVP-GNN layers node_in_dim (tuple): edge dimensions in input graph edge_h_dim (tuple): edge dimensions to embed in GVP-GNN layers num_layers (int): number of GVP-GNN layers in encoder/decoder drop_rate (float): rate to use in all dropout layers out_dim (int): output dimension (4 bases) \u0026#39;\u0026#39;\u0026#39; def __init__( self, node_in_dim = (64, 4), node_h_dim = (128, 16), edge_in_dim = (32, 1), edge_h_dim = (32, 1), num_layers = 3, drop_rate = 0.1, out_dim = 4, num_attention_heads = 8, attention_window_sizes = [10, 50, 200, None], # Multi-scale windows ): super().__init__() self.node_in_dim = node_in_dim self.node_h_dim = node_h_dim self.edge_in_dim = edge_in_dim self.edge_h_dim = edge_h_dim self.num_layers = num_layers self.out_dim = out_dim activations = (F.silu, None) # Node input embedding self.W_v = torch.nn.Sequential( LayerNorm(self.node_in_dim), GVP(self.node_in_dim, self.node_h_dim, activations=(None, None), vector_gate=True) ) # Edge input embedding self.W_e = torch.nn.Sequential( LayerNorm(self.edge_in_dim), GVP(self.edge_in_dim, self.edge_h_dim, activations=(None, None), vector_gate=True) ) # Encoder layers (supports multiple conformations) self.encoder_layers = nn.ModuleList( MultiGVPConvLayer(self.node_h_dim, self.edge_h_dim, activations=activations, vector_gate=True, drop_rate=drop_rate, norm_first=True) for _ in range(num_layers)) # Multi-scale attention for capturing long-distance dependencies at different scales self.multi_scale_attention = MultiScaleAttention( embed_dim=self.node_h_dim[0], # Scalar dimension num_heads=num_attention_heads, window_sizes=attention_window_sizes, dropout=drop_rate ) # Decoder layers self.W_s = nn.Embedding(self.out_dim, self.out_dim) self.edge_h_dim = (self.edge_h_dim[0] + self.out_dim, self.edge_h_dim[1]) self.decoder_layers = nn.ModuleList( GVPConvLayer(self.node_h_dim, self.edge_h_dim, activations=activations, vector_gate=True, drop_rate=drop_rate, autoregressive=True, norm_first=True) for _ in range(num_layers)) # Output self.W_out = GVP(self.node_h_dim, (self.out_dim, 0), activations=(None, None)) def forward(self, batch): h_V = (batch.node_s, batch.node_v) h_E = (batch.edge_s, batch.edge_v) edge_index = batch.edge_index seq = batch.seq h_V = self.W_v(h_V) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) h_E = self.W_e(h_E) # (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3) for layer in self.encoder_layers: h_V = layer(h_V, edge_index, h_E) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) # Pool multi-conformation features: # nodes: (n_nodes, d_s), (n_nodes, d_v, 3) # edges: (n_edges, d_se), (n_edges, d_ve, 3) h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) # Apply multi-scale attention on pooled scalar features h_V_s = h_V[0].unsqueeze(0) # (1, n_nodes, d_s) attn_output = self.multi_scale_attention(h_V_s) h_V = (attn_output.squeeze(0) + h_V[0], h_V[1]) # Residual connection encoder_embeddings = h_V h_S = self.W_s(seq) h_S = h_S[edge_index[0]] h_S[edge_index[0] \u0026gt;= edge_index[1]] = 0 h_E = (torch.cat([h_E[0], h_S], dim=-1), h_E[1]) for layer in self.decoder_layers: h_V = layer(h_V, edge_index, h_E, autoregressive_x = encoder_embeddings) logits = self.W_out(h_V) return logits @torch.no_grad() def sample( self, batch, n_samples, temperature: Optional[float] = 0.1, logit_bias: Optional[torch.Tensor] = None, return_logits: Optional[bool] = False ): \u0026#39;\u0026#39;\u0026#39; Samples sequences autoregressively from the distribution learned by the model. Args: batch (torch_geometric.data.Data): mini-batch containing one RNA backbone to design sequences for n_samples (int): number of samples temperature (float): temperature to use in softmax over the categorical distribution logit_bias (torch.Tensor): bias to add to logits during sampling to manually fix or control nucleotides in designed sequences, of shape [n_nodes, 4] return_logits (bool): whether to return logits or not Returns: seq (torch.Tensor): int tensor of shape [n_samples, n_nodes] based on the residue-to-int mapping of the original training data logits (torch.Tensor): logits of shape [n_samples, n_nodes, 4] (only if return_logits is True) \u0026#39;\u0026#39;\u0026#39; h_V = (batch.node_s, batch.node_v) h_E = (batch.edge_s, batch.edge_v) edge_index = batch.edge_index device = edge_index.device num_nodes = h_V[0].shape[0] h_V = self.W_v(h_V) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) h_E = self.W_e(h_E) # (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3) for layer in self.encoder_layers: h_V = layer(h_V, edge_index, h_E) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) # Pool multi-conformation features # nodes: (n_nodes, d_s), (n_nodes, d_v, 3) # edges: (n_edges, d_se), (n_edges, d_ve, 3) h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) # Apply multi-scale attention on pooled scalar features h_V_s = h_V[0].unsqueeze(0) # (1, n_nodes, d_s) attn_output = self.multi_scale_attention(h_V_s) h_V = (attn_output.squeeze(0) + h_V[0], h_V[1]) # Residual connection # Repeat features for sampling n_samples times h_V = (h_V[0].repeat(n_samples, 1), h_V[1].repeat(n_samples, 1, 1)) h_E = (h_E[0].repeat(n_samples, 1), h_E[1].repeat(n_samples, 1, 1)) # Expand edge index for autoregressive decoding edge_index = edge_index.expand(n_samples, -1, -1) offset = num_nodes * torch.arange(n_samples, device=device).view(-1, 1, 1) edge_index = torch.cat(tuple(edge_index + offset), dim=-1) # This is akin to \u0026#39;batching\u0026#39; (in PyG style) n_samples copies of the graph seq = torch.zeros(n_samples * num_nodes, device=device, dtype=torch.int) h_S = torch.zeros(n_samples * num_nodes, self.out_dim, device=device) logits = torch.zeros(n_samples * num_nodes, self.out_dim, device=device) h_V_cache = [(h_V[0].clone(), h_V[1].clone()) for _ in self.decoder_layers] # Decode one token at a time for i in range(num_nodes): h_S_ = h_S[edge_index[0]] h_S_[edge_index[0] \u0026gt;= edge_index[1]] = 0 h_E_ = (torch.cat([h_E[0], h_S_], dim=-1), h_E[1]) edge_mask = edge_index[1] % num_nodes == i # True for all edges where dst is node i edge_index_ = edge_index[:, edge_mask] # subset all incoming edges to node i h_E_ = tuple_index(h_E_, edge_mask) node_mask = torch.zeros(n_samples * num_nodes, device=device, dtype=torch.bool) node_mask[i::num_nodes] = True # True for all nodes i and its repeats for j, layer in enumerate(self.decoder_layers): out = layer(h_V_cache[j], edge_index_, h_E_, autoregressive_x=h_V_cache[0], node_mask=node_mask) out = tuple_index(out, node_mask) # subset out to only node i and its repeats if j \u0026lt; len(self.decoder_layers)-1: h_V_cache[j+1][0][i::num_nodes] = out[0] h_V_cache[j+1][1][i::num_nodes] = out[1] lgts = self.W_out(out) # Add logit bias if provided to fix or bias positions if logit_bias is not None: lgts += logit_bias[i] # Sample from logits seq[i::num_nodes] = Categorical(logits=lgts / temperature).sample() h_S[i::num_nodes] = self.W_s(seq[i::num_nodes]) logits[i::num_nodes] = lgts if return_logits: return seq.view(n_samples, num_nodes), logits.view(n_samples, num_nodes, self.out_dim) else: return seq.view(n_samples, num_nodes) def pool_multi_conf(self, h_V, h_E, mask_confs, edge_index): if mask_confs.size(1) == 1: # Number of conformations is 1, no need to pool return (h_V[0][:, 0], h_V[1][:, 0]), (h_E[0][:, 0], h_E[1][:, 0]) # True num_conf for masked mean pooling n_conf_true = mask_confs.sum(1, keepdim=True) # (n_nodes, 1) # Mask scalar features mask = mask_confs.unsqueeze(2) # (n_nodes, n_conf, 1) h_V0 = h_V[0] * mask h_E0 = h_E[0] * mask[edge_index[0]] # Mask vector features mask = mask.unsqueeze(3) # (n_nodes, n_conf, 1, 1) h_V1 = h_V[1] * mask h_E1 = h_E[1] * mask[edge_index[0]] # Average pooling multi-conformation features h_V = (h_V0.sum(dim=1) / n_conf_true, # (n_nodes, d_s) h_V1.sum(dim=1) / n_conf_true.unsqueeze(2)) # (n_nodes, d_v, 3) h_E = (h_E0.sum(dim=1) / n_conf_true[edge_index[0]], # (n_edges, d_se) h_E1.sum(dim=1) / n_conf_true[edge_index[0]].unsqueeze(2)) # (n_edges, d_ve, 3) return h_V, h_E class NonAutoregressiveMultiGNNv1(torch.nn.Module): \u0026#39;\u0026#39;\u0026#39; Non-Autoregressive GVP-GNN for **multiple** structure-conditioned RNA design. Takes in RNA structure graphs of type `torch_geometric.data.Data` or `torch_geometric.data.Batch` and returns a categorical distribution over 4 bases at each position in a `torch.Tensor` of shape [n_nodes, 4]. The standard forward pass requires sequence information as input and should be used for training or evaluating likelihood. For sampling or design, use `self.sample`. Args: node_in_dim (tuple): node dimensions in input graph node_h_dim (tuple): node dimensions to use in GVP-GNN layers node_in_dim (tuple): edge dimensions in input graph edge_h_dim (tuple): edge dimensions to embed in GVP-GNN layers num_layers (int): number of GVP-GNN layers in encoder/decoder drop_rate (float): rate to use in all dropout layers out_dim (int): output dimension (4 bases) \u0026#39;\u0026#39;\u0026#39; def __init__( self, node_in_dim = (64, 4), node_h_dim = (128, 16), edge_in_dim = (32, 1), edge_h_dim = (32, 1), num_layers = 3, drop_rate = 0.1, out_dim = 4, ): super().__init__() self.node_in_dim = node_in_dim self.node_h_dim = node_h_dim self.edge_in_dim = edge_in_dim self.edge_h_dim = edge_h_dim self.num_layers = num_layers self.out_dim = out_dim activations = (F.silu, None) # Node input embedding self.W_v = torch.nn.Sequential( LayerNorm(self.node_in_dim), GVP(self.node_in_dim, self.node_h_dim, activations=(None, None), vector_gate=True) ) # Edge input embedding self.W_e = torch.nn.Sequential( LayerNorm(self.edge_in_dim), GVP(self.edge_in_dim, self.edge_h_dim, activations=(None, None), vector_gate=True) ) # Encoder layers (supports multiple conformations) self.encoder_layers = nn.ModuleList( MultiGVPConvLayer(self.node_h_dim, self.edge_h_dim, activations=activations, vector_gate=True, drop_rate=drop_rate, norm_first=True) for _ in range(num_layers)) # Output self.W_out = torch.nn.Sequential( LayerNorm(self.node_h_dim), GVP(self.node_h_dim, self.node_h_dim, activations=(None, None), vector_gate=True), GVP(self.node_h_dim, (self.out_dim, 0), activations=(None, None)) ) def forward(self, batch): h_V = (batch.node_s, batch.node_v) h_E = (batch.edge_s, batch.edge_v) edge_index = batch.edge_index h_V = self.W_v(h_V) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) h_E = self.W_e(h_E) # (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3) for layer in self.encoder_layers: h_V = layer(h_V, edge_index, h_E) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) # Pool multi-conformation features: # nodes: (n_nodes, d_s), (n_nodes, d_v, 3) # edges: (n_edges, d_se), (n_edges, d_ve, 3) # h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) h_V = (h_V[0].mean(dim=1), h_V[1].mean(dim=1)) logits = self.W_out(h_V) # (n_nodes, out_dim) return logits def sample(self, batch, n_samples, temperature=0.1, return_logits=False): with torch.no_grad(): h_V = (batch.node_s, batch.node_v) h_E = (batch.edge_s, batch.edge_v) edge_index = batch.edge_index h_V = self.W_v(h_V) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) h_E = self.W_e(h_E) # (n_edges, n_conf, d_se), (n_edges, n_conf, d_ve, 3) for layer in self.encoder_layers: h_V = layer(h_V, edge_index, h_E) # (n_nodes, n_conf, d_s), (n_nodes, n_conf, d_v, 3) # Pool multi-conformation features # h_V, h_E = self.pool_multi_conf(h_V, h_E, batch.mask_confs, edge_index) h_V = (h_V[0].mean(dim=1), h_V[1].mean(dim=1)) logits = self.W_out(h_V) # (n_nodes, out_dim) probs = F.softmax(logits / temperature, dim=-1) seq = torch.multinomial(probs, n_samples, replacement=True) # (n_nodes, n_samples) if return_logits: return seq.permute(1, 0).contiguous(), logits.unsqueeze(0).repeat(n_samples, 1, 1) else: return seq.permute(1, 0).contiguous() def pool_multi_conf(self, h_V, h_E, mask_confs, edge_index): if mask_confs.size(1) == 1: # Number of conformations is 1, no need to pool return (h_V[0][:, 0], h_V[1][:, 0]), (h_E[0][:, 0], h_E[1][:, 0]) # True num_conf for masked mean pooling n_conf_true = mask_confs.sum(1, keepdim=True) # (n_nodes, 1) # Mask scalar features mask = mask_confs.unsqueeze(2) # (n_nodes, n_conf, 1) h_V0 = h_V[0] * mask h_E0 = h_E[0] * mask[edge_index[0]] # Mask vector features mask = mask.unsqueeze(3) # (n_nodes, n_conf, 1, 1) h_V1 = h_V[1] * mask h_E1 = h_E[1] * mask[edge_index[0]] # Average pooling multi-conformation features h_V = (h_V0.sum(dim=1) / n_conf_true, # (n_nodes, d_s) h_V1.sum(dim=1) / n_conf_true.unsqueeze(2)) # (n_nodes, d_v, 3) h_E = (h_E0.sum(dim=1) / n_conf_true[edge_index[0]], # (n_edges, d_se) h_E1.sum(dim=1) / n_conf_true[edge_index[0]].unsqueeze(2)) # (n_edges, d_ve, 3) return h_V, h_E ","date":"2025-09-04T15:36:23+08:00","permalink":"https://mosfish.github.io/p/how-about-rna-inverse-folding/","title":"How about RNA inverse folding?"},{"content":"Free5GC核心网部署与接入测试手册 写在开篇： 建议养成备份（例如git工具）的好习惯。本文包含Linux开发基础内容、free5gc核心网\n考虑到版权问题，本文章如需要转载请进行声明。\n起初使用VMware虚拟机装的Ubuntu桌面版系统，但是实际工程需要板载Linux系统（但是学习初期直接上手，容易因为不熟悉操作而出现诸如网卡连不了网，没声音，拼音输入法等软硬件不适配类似的各种问题）可以先基于虚拟机进行学习，入门后再基于板载Linux系统开发核心网。\n5G核心网介绍 参考 【5G】5G 核心网（5GC）基本架构_5g核心网架构-CSDN博客\n核心网具体结构 5G 核心网是支持 5G 移动通信的核心基础设施，在 5G 通信中负责实现数据处理、控制、管理、路由和连接等功能。5G 核心网的架构采用了基于服务的设计 (Service-based Architecture, SBA)，支持网络切片、NFV、软件定义网络 (Software-Defined Networking, SDN) 等先进技术，提供更高的灵活性和可扩展性。\n5G网络架构与接口 5G核心网网元功能 AMF (Access and Mobility Management Function) 接入与移动性管理功能，主要负责：\n接入控制、用户移动性管理 会话管理和用户身份管理 处理UE与网络的接入请求 管理用户接入状态及移动性支持 SMF (Session Management Function)\n会话管理功能，负责：\n会话的建立、修改和释放 数据流管理（路由、QoS保证、IP分配） UPF (User Plane Function) 用户面功能，负责：\n转发用户数据流 与SMF协作管理数据路径、路由和转发 NSSF (Network Slice Selection Function) 网络切片选择功能，负责：\n根据业务需求动态选择网络切片 管理不同业务场景的切片资源 PCF (Policy Control Function) 策略控制功能，负责：\n制定网络策略（QoS、计费、流量管理、安全性） 为AMF/SMF/UPF提供策略决策服务 UDM (Unified Data Management) 统一数据管理功能，负责：\n存储用户订阅信息（SUPI）、服务配置、认证信息 与AUSF/AMF等网元协作提供数据服务 5G网络组成 架构组成：5GC（核心网） + RAN（无线接入网） + UE（用户设备） 核心网功能：服务管理、数据传输、安全性 RAN功能：无线连接、数据传输与控制信号传递 UE角色：用户交互终端，通过RAN与核心网通信 无线接入网与核心网接口 主要接口及功能：\nN1接口\n连接：UE ↔ AMF 功能：信令传递（UE接入管理、认证） N2接口\n连接：AMF ↔ SMF 功能：会话管理协作（处理用户会话任务） N3接口\n连接：无线接入网 ↔ UPF 功能：用户数据传输（数据面主要路径） N4接口\n连接：SMF ↔ UPF 功能：用户数据路径选择与管理（SMF控制UPF流量） N6接口\n连接：UPF ↔ 数据网络（DN） 功能：用户数据外发至互联网/外部网络 linux开发基本配置与指令 一、常用指令 1.基本指令部分 复制 Ctrl+Shift+C\n粘贴 Ctrl+Shift+V\n更新包列表\n1 sudo apt update 查网卡ip\n1 2 Ifconfig # 记得提前安装ifconfig网络工具 sudo apt install net-tools 或者\n1 ip a 显示当前路径\n1 pwd 进入文件夹\n1 2 3 cd \u0026lt;文件夹名\u0026gt; # 例如 cd free5gc-compose # 或者进入多个内嵌文件夹如 cd ~/free5gc-compose/config 查询网站ip地址\n1 nslookup baidu.com 查各网元运行情况、地址\n1 2 docker-compose ps 或看全部网元： docker ps -a 停止容器网元运行\n1 docker-compose down 启动容器网元并后台挂机\n1 docker-compose up -d 查看运行日志（以amf为例，可换成smf、upf等）\n1 docker logs \u0026lt;id\u0026gt; 例如：docker logs amf\n查找单个网元信息（例如amf）\n1 docker inspect amf 进入容器进行交互\n1 docker exec -it \u0026lt;容器ID或容器名(如amf)\u0026gt; sh 或者docker exec -it \u0026lt;容器ID或容器名(如amf)\u0026gt; bash\n按照格式查找所有网元的ip地址（没有启动或者出错就不显示）\n1 docker inspect -f \u0026#39;{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; $(docker ps -aq) 查询所有网元mac地址\n1 docker inspect -f \u0026#39;{{.Name}} - {{range .NetworkSettings.Networks}}{{.MacAddress}}{{end}}\u0026#39; $(docker ps -aq) 我的地址（每个人的都不同，根据实际情况可以修改）： /home/wxy/free5gc-compose/config/amfcfg.yaml\n查看端口情况\n1 etstat -tuln | grep 38412 看防火墙设置\n1 sudo iptables -L 看网桥\n1 brctl show 查看网桥连接的ip地址\n1 brctl showmacs \u0026lt;网桥名\u0026gt; 看路由\n1 ip route show 添加网桥\n1 sudo brctl addif \u0026lt;bridge\u0026gt; \u0026lt;网卡名\u0026gt; Linux启动wireshark抓包——需要安装linux的wireshark然后启动：\tsudo wireshark\n2.网络空间测试 一般是用docker inspect amf或者docker logs amf来对amf、smf、upf之类的进行测试，\n也可以进入网络空间进行进一步测试例如：\n1 docker exec -it upf bash 不过最好是利用pid方法进入网络空间进行ifconfig或者ping操作\n1 2 3 ps aux | grep upf #获取进程（可以得到各种进程id） pgrep -f upf #获取pid： sudo nsenter -t \u0026lt;pid\u0026gt; -n #进入网络空间 其余指令还有kill之类的可以终止进程。\n除此之外有的时候ping网站可能会遇到name resolution的报错，可以换成ip地址，指令如下，以百度为例：\n1 nslookup baidu.com 不过ping命令有的时候会被网页阻拦，用curl命令比较合适，例如：\n1 curl www.baidu.com 3.启动环境部分 启动核心网和ueransim环境分以下三步，根据实际情况可以更改代码：\n启动核心网环境\n1 2 cd ~/free5gc-compose sudo docker-compose up -d 启动UERANSIM的gnb\n1 2 cd ~/UERANSIM/build ./nr-gnb -c ../config/free5gc-gnb.yaml 启动ue\n1 2 cd ~/UERANSIM/build sudo ./nr-ue -c ../config/free5gc-ue.yaml 虚拟网卡测试方法\n1 2 ping www.baidu.com -I uesimtun0 # 或者 curl www.baidu.com -I uesimtun0 二、虚拟机Linux设置vpn方法 途径1.主机共享vpn 参考：VMware Ubuntu虚拟机 使用主机VPN 配置（简单、可行）-CSDN博客\n（图片资源没保存，所以只有文字了）\n一定要记得开篇说的资源备份啊呜呜呜 打开虚拟网络编辑器，可以看到虚拟机开启的NAT模式是VMnet8。\n主机windows+R输入cmd打开命令行输入ipconfig ，得到VMnet8的ip：192.168.154.1。之后查询vpn端口，以clash为例，右上角记下vpn的端口，之后允许局域网记得打开。\n在虚拟机的设置中打开proxy代理，选择manual，\n虚拟机网络地址与VPN Port需要根据本机确定，左边四个填VMnet4的ip，右边四个填vpn的端口即可。\n问题1：如果我的vpn没有用clash，不知道vpn端口怎么办？ 参考：在windows中如何查看代理的地址和端口_怎么查看自己电脑的代理地址-CSDN博客\nWindows+R输入control打开主机的控制面板，点击「网络和Internet」，\n在「internet选项」中点击「连接」的「局域网设置」\n启用代理服务器，不用打勾，保持默认就行，右下角端口即为vpn端口。\n问题2：为什么我的vpn它通不了外网？ 一般而言，如果浏览器可以打开谷歌就证明梯子没问题。那么问题就出在——实际命令行终端走代理需要进一步配置端口\n已知代理端口（例如7890），之后就set一下http、https两个协议走这个端口，类似于\n1 set http_proxy=http://127.0.0.1:7890 \u0026amp; set https_proxy=http://127.0.0.1:7890 但是具体还是得看具体的设备，这种事情可以问一下ai。\n途径2.linux里下载Clash csdn里有相关内容，请自行搜索，配置起来较麻烦，\nLinux下Clash翻墙完整使用指南 | Clash中文教程网。\n这个教程写的不是很清楚，可以拓展一下。\n三、主机与虚拟机共享剪贴板 参考：主机和VMware虚拟机间共享剪贴板方法_vmware workstation 中启用了剪贴板共享功能-CSDN博客\n为啥主机复制过去虚拟机粘贴不了？有的时候可以复制粘贴有的时候不能？一般来说，先设置虚拟机，需要保证客户机隔离里面是启用复制粘贴的（默认启用）\n如果这个时候还不行，可以需要安装虚拟机增强工具包open-vm-tools和open-vm-tools-desktop，安装命令：\n1 2 sudo apt-get install open-vm-tools sudo apt-get install open-vm-tools-desktop 然后重启系统：\n1 sudo reboot 四、虚拟机关机与挂起 虚拟机关机就是不再占用资源，但是再次开启需要重启；挂起就是虚拟机保存挂起的状态，打开不需要重启。\n最重要的事情是记得自己虚拟机的路径.\n问题1：关机后找不到虚拟机咋办 点击扫描虚拟机，找到.vmx文件打开就行。\n五、虚拟机分盘扩容操作 有的时候给虚拟机分盘内存太小，当盘快满的时候会弹出警告，一定要提前采取措施来分盘，否则会启动不了虚拟机。\n1 lsblk #检查磁盘情况 1 df -h\t#检查磁盘占用情况 可以看到我主要的盘sda3占用了53%。\n分盘主要是两个操作\n划出内存\n编辑虚拟机设置-硬盘-拓展，然后选择扩展后的硬盘空间\n分配空间\n考虑两种方法可以，有一个是类似于windows的图形化分盘，下载并启动gparted，这个方法在VMware虚拟机扩容磁盘，有很详细图文_虚拟机硬盘空间-CSDN博客有提及。\n1 2 3 sudo apt-get update sudo apt install gparted sudo gparted 在图形界面中可以选择 /dev/sda3 分区，然后调整它的大小。\n或者利用fdisk（推荐），输入命令：\n1 sudo fdisk /dev/sda 再输入p得到分区列表预览：\nfdisk 中执行命令汇总如下\n输入 p 显示当前的分区列表，记下 /dev/sda3 的起始位置（例如 1054720）。\n输入 d 删除 /dev/sda3，并确保输入分区号（3）。注意，这不会删除数据，分区号只是从分区表中移除。\n输入 n 创建新分区，选择 3 为分区号，使用 1054720为起始扇区（与之前一致），并设置结束扇区为新的磁盘容量（可以选择最大可用空间）。\n输入 w 保存更改并退出 fdisk。\n可能遇见这种情况，但是如果默认系统是linux一般用不上：输入 t 设置分区类型，选择 83（Linux 文件系统）。\n问题1：虚拟机退出后再启动会卡在初始页面 类似于这样，进不去虚拟机，一直卡在这个地方：\n因为没注意虚拟机盘满了，卡在初始界面进不去，所以要在盘快满的时候提前分盘。\nVMware卡在命令行/dev/sda3 clean\u0026hellip; 界面不动的解决办法：\n参考https://blog.csdn.net/SunshineLiy/article/details/134372529\n显示分区已满，需要找到这个分区一些没用的文件删掉，先进去虚拟机能分盘再说。\n进入 grub 模式，重启虚拟机，在显示到这个界面的时候\n快速按下 Shift 不要松，直到出现grub界面（这一步需要反应快一些，多试几遍就可以，如果操作正确的话，grub界面很快就会出来，如果按Shift三秒没有出现，大概率是操作错误了。此方法如果进不去，也可以参考网上也有其他进入grub的方法）\n进入ubuntu(高级模式)，选择第一个advanced\n之后选择第二个\n选择root\n选择root后会弹出两行提示，直接回车就可以\n输入以下指令看到磁盘使用情况\n1 df -h sda3确实100%满了，所以我们需要在命令行来清理一些文件。\n查看根目录下所有文件和目录的磁盘使用情况\n1 du -sh /* 找到自己占用空间较多的文件目录\n1 rm -f /文件名/ 删除那些暂时没有用的大文件，reboot 重启。然后赶紧增加空间！！！\n六、Ubuntu桌面相关问题 1.桌面字标大小 调整分辨率和缩放就行。缩放到200%左右比较合适，在setting里面设置。有的时候会有桌面越来越小的bug，在设置里面改就可以。\n2.桌面卡死 输入以下命令重启桌面等一会就行：\n1 sudo restart lightdm 七、虚拟机没网设置网络图标消失 如果只是网络连接处显示问号，可以在终端输入：\n1 sudo vim /var/lib/NetworkManager/NetworkManager.state 保证这里是true。重启虚拟机，检查情况。如果还没有反应可以重启主机，再排查\n参考彻底解决VM ubuntu在虚拟机找不到网卡无法上网的问题 - 知乎\n还是不行在setting里面找network关了重开，而如果第一行开启按钮没有显示的话可能需要用以下方式：\n1 2 3 sudo service NetworkManager stop sudo rm /var/lib/NetworkManager/NetworkManager.state sudo service NetworkManager start 最后的办法是还原默认设置后再看看有没有，没有的话再进行上一步输入命令。\nFree5gc部署、UERANSIM安装 主要参考：Free5gc+UERANSIM模拟5G网络环境搭建及基本使用 - FreeBuf网络安全行业门户\n此处采用docker容器化部署，其他独立化部署可以看教程，根据实际情况同时参考教程和本文档。\n考虑实际情况，建议这里采用镜像源代理的方式，暂且不用vpn\n一、Free5gc部署 准备工作 使用命令：\n1 uname -a 此命令用于确认安装的虚拟机内核版本，要求的内核版本为5.0.0-23-generic或5.4.0及之后的版本,\n若当前虚拟机内核版本不符合要求，则需要更换内核，使用以下命令安装5.0.0-23-generic的内核，注意用sudo是管理员权限的操作，需要输入密码，为了安全，密码一般是不显示的但是可以输入，并不是bug，输入后按回车。\n1 2 sudo apt install \u0026#39;linux-image-5.0.0-23-generic\u0026#39; sudo apt install \u0026#39;linux-headers-5.0.0-23-generic 安装完成后，需要重启虚拟机，并在启动时连按shift键，进入grub引导页，更换启动的Linux内核。\n之后安装基本组件：\n1 2 3 4 5 6 7 8 9 10 sudo apt install git-all sudo apt-get install curl sudo apt install make sudo apt -y install gcc sudo apt -y install g++ sudo apt -y install autoconf sudo apt -y install libtool sudo apt -y install pkg-config sudo apt -y install libmnl-dev sudo apt -y install libyaml-dev 如果人为设置了vpn就不用按照教程里替换源了。之后安装go语言环境，注意安装go语言环境时必须为普通用户安装，否则会导致后续安装出现问题。输入命令：\n1 go 以确认是否存在其他版本的go，若存在，则通过以下命令删除\n1 sudo rm -rf /usr/local/go 之后安装最新版本的go：\n1 2 3 cd ~ wget https://dl.google.com/go/go1.20.4.linux-amd64.tar.gz sudo tar -C /usr/local -zxvf go1.20.4.linux-amd64.tar.gz 安装完成后，需要通过以下命令配置环境变量（此过程按回车不会有输出）\n1 2 3 4 5 mkdir -p ~/go/{bin,pkg,src} echo \u0026#39;export GOPATH=$HOME/go\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export GOROOT=/usr/local/go\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export PATH=$PATH:$GOPATH/bin:$GOROOT/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 之后输入命令：\n1 go #有版本号等输出就行。安装成功，再补充安装以下模块，该模块为free5gc独立部署的日志模块，容器化部署也可以安装：\n1 #go get -u github.com/sirupsen/logrus 通过官方安装脚本安装docker，\n1 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun 安装完成后，运行命令docker \u0026ndash;version验证。之后需要安装docker-compose，通过以下命令完成：\n1 sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.28.5/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose 重启docker服务即可完成docker的部署：\n1 systemctl restart docker 完成后，需要将当前普通用户加入docker用户组，docker用户组在上述安装时自动创建，无需手动创建：\n1 2 sudo gpasswd -a $USER docker #将当前普通用户加入docker用户组 newgrp docker #更新docker用户组 此步目的在于防止后续free5gc容器化部署时，到make base步骤，出现permission denied。\n更换镜像源 这是很重要的一步，原理是部分镜像源起到类似代理的作用。在网上搜索最新docker镜像源，参考\n国内能用的Docker镜像源【2025最新持续更新】_docker 镜像-CSDN博客\n国内仍然可用docker镜像源汇总，长期维护，定期更新（2025年3月21日）_docker 国内镜像源-CSDN博客\nDocker换源加速(更换镜像源)详细教程（2025.3最新可用镜像，全网最详细） - 知乎\n然后输入类似于\n1 sudo nano /etc/docker/daemon.json 来创造或修改配置文件，在里面写入代理网站，但是下面这个是一开始的，现在被ban掉了好多，不推荐，\n1 2 3 4 5 6 7 8 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://mirror.baidubce.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://mirror.ccs.tencentyun.com\u0026#34; ] } 推荐这些，或者去我给的参考链接找最新存活的：\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker-0.unsee.tech\u0026#34;, \u0026#34;https://docker-cf.registry.cyou\u0026#34;, \u0026#34;https://docker.1panel.live\u0026#34;, \u0026#34;https://docker.xuanyuan.me\u0026#34;, \u0026#34;https://1ms.run\u0026#34;, \u0026#34;https://hub.fast360.xyz\u0026#34;, \u0026#34;https://hub.littlediary.cn\u0026#34; ] } 然后保存离开，输入以下命令清除缓存、重启docker\n1 2 sudo systemctl daemon-reload sudo systemctl restart docker 然后可以查看docker源是否更改：\n1 docker info | grep -A 1 \u0026#34;Registry Mirrors\u0026#34; 最后可以尝试验证：\n1 docker pull hello-world 拉取成功即可以。\n继续安装其他组件 安装cmake：\n1 sudo snap install cmake –classic 安装mongodb\n1 2 3 sudo apt -y update sudo apt -y install mongodb wget git sudo systemctl start mongodb 此时可能会报错Package 'mongodb' has no installation candidate，有可能因为ubuntu没更新找不到安装包，可以试试导入mongodb的公钥，运行以下命令：\n1 wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo gpg --dearmor -o /usr/share/keyrings/mongodb-archive-keyring.gpg 并添加其到apt，\n1 echo \u0026#34;deb [signed-by=/usr/share/keyrings/mongodb-archive-keyring.gpg] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/6.0 multiverse\u0026#34; | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list 更新包列表：\n1 sudo apt-get update 安装mongodb：\n1 sudo apt-get install mongodb-org 之后可以选择性安装yarn**（独立化部署的话则是必须）**\n1 2 3 curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo \u0026#34;deb https://dl.yarnpkg.com/debian/ stable main\u0026#34; | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt update \u0026amp;\u0026amp; sudo apt install yarn 构建GTP5G模块 注意构建GTP5G模块时，需要用普通用户构建，否则后续使用时会出错\n1 2 3 cd~ git clone https://github.com/free5gc/gtp5g.git cd gtp5g 编译\n1 2 make sudo make install 此时可能会遇到报错，如果是类似于warning: the compiler differs from the one used to build the kernel等，原因是找不到那几个编译器。\n需要安装：\n1 2 sudo apt update sudo apt install gcc-12 然后重新编译：\n1 2 3 make clean make sudo make install 容器化部署free5gc模拟核心网 首先，通过git clone下载项目代码（注意这里和教程文章的不太一样，教程里面那个链接好像用不了）：\n1 2 3 cd ~ git clone https://github.com/free5gc/free5gc-compose.git cd free5gc-compose 如果有vpn就不用按照文章里说的改代理\n编译代码：\n1 2 3 cd ~/free5gc-compose make base docker-compose build 其中docker-compose build一步可能报版本错误，原因在于上面安装docker、docker-compose时可能安装了较低版本的docker、docker-compose，此时可以选择重装docker、docker-compose或者修改docker-compose.yaml文件中第一行版本号3.8为当前版本，建议优先选择重装docker、docker-compose，以回避未知错误，可以通过命令docker \u0026ndash;version命令查询当前版本。（不过如果是按教程里的话一般没问题）\n交叉编译 交叉编译指利用不同的平台进行处理，并把安装或编译好的东西转移到所需平台。这种思想不仅仅用于以下案例，可以用到很多地方。\n如果不按照刚才说法设置镜像代理的话，编译过程中也有可能遇到golang bullseye报错，\n=\u0026gt; ERROR [internal] load metadata for docker.io/library/golang: 21.5s ------ \u0026gt; [internal] load metadata for docker.io/library/golang:1.21.8-bullseye\n如果实在不打算使用镜像源，可以利用交叉编译在主机windows挂梯子下载docker desktop软件，下载好后在上面搜索golang，选1.24.0-bullseye（我用的1.23.6-bullseye，这俩都行），点击pull，即可把包下载到电脑里。\n然后在主机打开命令行，输入\n1 docker images ，可以看到包已经下载到了主机里，然后输入命令：\n1 docker save -o E:\\go\\golang_1.24.0-bullseye.tar golang:1.24.0-bullseye 可以把包保存到电脑路径里\n从主机把压缩包拷贝到虚拟机：\n在虚拟机输入\n1 2 cd ~/free5gc-compose/base vim Dockerfile 进入配置文件，\n输入I，进入修改，把版本号改成所用版本(1.24.0之类的)，然后Esc，输入命令:wq（带冒号）即可保存退出。\n然后虚拟机终端输入：\n1 sudo docker load \u0026lt; /home/golang_1.24.0-bullseye 之后重复原本步骤编译就可以。\n至此，free5gc容器化部署完成\n二、安装UERANSIM 1 2 cd ~ git clone https://github.com/aligungr/UERANSIM 然后：\n1 2 3 4 5 6 7 sudo apt update sudo apt upgrade sudo apt install make sudo apt install g++ sudo apt install libsctp-dev sudo apt install lksctp-tools sudo apt install iproute2 编译代码：\n1 2 cd ~/UERANSIM make 编译完成就安装成功了。\n三、启动free5gc和ueransim环境 Free5gc，启动\n1 2 cd ~/free5gc-compose docker-compose up -d 之后\n1 ifconfig 这是查看网卡地址，启动free5gc后，会在本地虚拟化出一系列的网卡，这一步需要关注的是原先虚拟机自带的网卡，通常这类网卡的ip地址均处于192.168.*.*网段，网卡名类似ens33，eth0之类，可以以此特征区分出来\n接下来需要查看并记录amf网元的ip地址：\n1 docker inspect amf 找到上面记录有\u0026quot;IPAddress\u0026quot;: 的一行，后面记录的即是amf的ip地址\n记录下这两个ip地址后，就可以完成UERANSIM中gnb的配置了，通过修改free5gc-gnb.yaml配置文件完成这一步操作：\n1 2 cd ~/UERANSIM/config/ vim free5gc-gnb.yaml 输入I进行修改模式，需要修改其中的ngapIp、gtpIp为本机ip\n修改其中的amfconfig一项下的address为amf的ip，然后Esc，再输入 :wq\n表示保存并退出，类似还有 :q :qa :exit :^X之类的。\n每次重启机器后，amf地址可能改变，注意更改。\n至此，UERANSIM的基站配置完成，接下来需要在free5gc中注册UERANSIM的UE部分：\n访问地址 http://localhost:5000/可进入到free5gc的webui处，登录：\n用户名：admin 密码：free5gc\n之后通过free5gc的webui新增一个ue的注册信息（否则会报错说无法注册），此处配置的UE信息原则上需要和~/UERANSIM/config/free5gc-ue.yaml中的信息一致，但由于此处UERANSIM的代码作者已经设置好，所以实际上无需做任何更改，直接拉到最下面点create就ok\n之后启动UERANSIM模拟设备\n1 2 cd ~/UERANSIM/build#启动一个shell，执行启动gnb的流程 ./nr-gnb -c ../config/free5gc-gnb.yaml#通过nr-gnb程序，指定使用的gnb配置文件，启动模拟基站 另起一个shell，执行启动UE的流程\n1 2 cd ~/UERANSIM/build #通过nr-ue程序，指定使用的ue配置文件，启动模拟用户设备 sudo ./nr-ue -c ../config/free5gc-ue.yaml #此处因为需要虚拟出一张ue的网卡，所以需要root权限执行 启动的两个shell不可关闭，可以后台执行，但建议前台执行方便实时查看状态信息。启动完成后，执行ifconfig可以看到多了一张名为uesimtun0的网卡；另外，在free5gc的webui处，查看REALTIME STATUS可以看到有一个UE处于连接状态，此时即证明UERANSIM的环境启动成功：\n之后测试一下\n1 ping www.baidu.com -I uesimtun0 能通就ok，在free5gc官网也可以看到connect信息。\n核心网的基站层测试 一、以基站配置测试ueransim软件层 修改PLMN值，IP不变，进行测试、注册\n先改文件里PLMN的mcc、mnc分别为001、01，按照自己的目录（我的涉及/home/wxy/free5gc-compose/config/amfcfg.yaml、gnbcfg.yaml、smfcfg.yaml、uecfg.yaml，/home/wxy/UERANSIM/config/free5gc-gnb.yaml、free5gc-ue.yaml，包括修改ue里supi的前五位为00101：\n还有nrfcfg文件的mcc、mnc\n改完save，然后关闭docker\n1 docker-compose down 再重新跑核心网流程wireshark抓包查看闪退原因\n启动核心网环境\n1 2 cd ~/free5gc-compose sudo docker-compose up -d 启动UERANSIM的gnb\n1 2 cd ~/UERANSIM/build ./nr-gnb -c ../config/free5gc-gnb.yaml 启动ue\n1 2 cd ~/UERANSIM/build sudo ./nr-ue -c ../config/free5gc-ue.yaml 并重新create，修改mcc、mnc、id后注册\n然后根据一般ueransim测试流程启动，查询有无报错，并用uesimtun0来ping一下百度。\n虚拟网卡测试\n1 ping www.baidu.com -I uesimtun0 之后需要修改ip： 分别在docker-compose文件里面添加upf的ip、设置amf的ip，还有network的范围\n地址范围subnet要可以覆盖，改为192.168.0.0/16\n由于n3iwue、free5gc-n3iwf两部分此时用不到，可以把这两段代码分别注释掉。我们要修改amf为192.168.2.198，smf内的upf与N3口192.168.8.198。\n在amfcfg将ngapip改为基站的192.168.2.198\nsmfcfg的upf ip改为基站upf 192.168.8.198\n以及upfcfg的N3 ip为192.168.8.198\n改完保存，把docker给down掉关闭，然后重复一般测试流程，成功标志为\n1 ping baidu.com -I uesimtun0 可以收到包。\n问题1：遇到ue、gnb成功连接且核心网成功配置但是不通网 这个问题比较随机，因为每个人虚拟机、或者linux子系统的默认配置不同。诊断方法可以考虑进入upf的网络空间进行调整；或者docker logs amf那几个看一下日志；或根据路由、配合抓包进行诊断。具体命令在本文档第二部分。\n查询UERANSIM内的UE和GNB有没有建立PDU连接：\n如图可见二者连接正常，说明ue与gnb连接正常。之后可以分别查看一下amf、upf有没有正常启动、报错，用以下命令检查日志：\n1 docker logs amf 1 docker logs upf 如果没有报错，排查gnb通过N3口连接upf：\n1 ping 192.168.8.198 -I uesimtun0 还是通的话，可以进入upf的网络空间，看upf联网如何，\n利用pid方法进入网络空间进行ifconfig或者ping操作\n1 pgrep -f upf #获取pid： 1 sudo nsenter -t \u0026lt;pid\u0026gt; -n #进入网络空间 之后，在里面输入：\n1 ifconfig 查找网络工具，若有upfgtp或者eth0就可以，之后输入ping命令\n1 ping 8.8.8.8 如果有包收发，那就证明upf可以上网，所以整个网络连接没有问题。\n问题出在哪里？IP 转发 和 NAT 配置\n我们需要启用 IPv4 数据包转发功能（将接收到的 IP 数据包从一个网络接口转发到另一个网络接口），以及开启NAT模式（数据包离开当前网络时进行源地址转换），并允许转发的数据包通过防火墙。ICMP\n把docker-compose文件的upf部分的command修改添加以下代码\n1 2 3 4 5 6 7 8 command: - /bin/bash - -c - | sysctl -w net.ipv4.ip_forward=1 iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -I FORWARD 1 -j ACCEPT ./upf -c /free5gc/config/upfcfg.yaml 修改后save，然后把ueransim关掉，docker给down掉，重新跑一遍启动流程，之后测试\n1 ping www.baidu.com -I uesimtun0 跑通成功。\n二、连接实体基站进行测试 用SSH来连接基站，需要下载ssh工具\n1 2 sudo apt update sudo apt install openssh-client 启动基站流程：\n分别打开三个终端shell\n在第一个shell输入\n1 ssh root@192.168.2.246 输入密码，然后输入\n1 cd /root/yzmm/rel/GNB/phy/bin ./run.sh 挂在后台，别关闭。\n在第二个shell输入\n1 ssh root@192.168.2.246 输入密码，然后输入\n1 cd /root/yzmm/rel/GNB/cu/cu/nr_hl_cu/build ./run.sh 在第三个shell输入\n1 ssh root@192.168.2.246 再\n1 cd /root/yzmm/rel/GNB/du/ran/DU/build/intel/du_bin/bin ./run.sh 需要关掉ueransim，防止ip干扰冲突。\n然后添加端口映射38412，此处对amf修改无严格要求。\n添加upf网口2152:2152 /udp，经测试ue无法注册信号，产生mac相关报错，原因可能是free5gc默认和ueransim的对齐导致一些文件配置有问题。于是采用修改的核心网，工程文件来源于师兄distributed-core-network-control-plane。\n写在结尾 特别鸣谢：两位师兄和两位老师\n后期工作：\n梳理流程思路，抓包分析\n深入理解代码、规范、抓包工具；实体基站和软件ueransim的不同\n步进调试环境搭建、分布式核心网改进\n本文只涉及基础的Free5GC内容，由于传统核心网的集中性、易受攻击的问题，可以采取分布式架构进行优化，下篇文章阐述何为分布式核心网、市场空间、技术路线等。\n","date":"2025-05-23T12:45:04+08:00","permalink":"https://mosfish.github.io/p/5g%E6%A0%B8%E5%BF%83%E7%BD%91%E9%83%A8%E7%BD%B2%E5%8F%8A%E6%B5%8B%E8%AF%95%E6%89%8B%E5%86%8C/","title":"5G核心网部署及测试手册"},{"content":"水木清华的深冬与初春 喜欢摄影，随快门声记录下那些难忘的时刻，一点一点凑成碎片，从流年中捞起那不变的永恒。\n记得刚到这里的时候，也才将将春节之后。京城的街道充满了三种氛围：节日的热闹，假期的冷清，外来旅客潮的繁忙——巧了，我也是这众多旅人中的一员，听着火车站大哥亲切的京腔，似乎很久都没回家了，今年有幸在华北（家的地方），感受到了发自内心的愉悦与放松。\n喜欢家里的饺子，喜欢家里的炒饭炒面，喜欢门外吆喝的小商小贩，喜欢小区门口的庆丰包子铺，喜欢地铁口的煎饼果子、加俩蛋。\n人间烟火气，最抚凡人心\n或者说，最抚凡人心的，是熟悉的、家乡的烟火气。五道口这里，是恩师的母校，也是故友求学的地方，虽时运不济恰恰错峰难以见一面故友，不过我相信，来日方长，未来我应该还会来五道口访问交流，希望到那时我们都摆脱了升学的压力，在饺子铺一叙过往吧。\n热情好客的寒潮与零下十度的京城 回想起来，倒是第一次因为学业而来到京城，不想麻烦亲朋好友，便匆匆租了个房了事。好巧不巧，海淀的初春也遭遇了难得一遇的大寒潮，尽管我裹得严严实实（但是面部、双耳没裹上），也是被冻得丢了半条命，肚子痛得打滚，点了一份疙瘩汤当晚饭（还有美团送药），草草了事。\n在清华忙碌的日子 相较于康乐园，似乎清华园也差不多大，当然如果康乐园的家属区感觉占比大一点。冬日（或者初春）的清华简直就像人民公园，一眼看上去和我家门口的公园感觉差不多，寥寥几个学生、打卡的游客，以及我这种无业游民哈哈（旅客？学生？应该算visiting student吧）。在这里的时候，每天上午9点多来，忙到晚上9点多10点回去，我摸鱼的时候学了Latex、Markdown，真的挺有意思；除此之外其实主线任务是学习Linux开发、Arm开发、free5GC核心网架构，其实第一次开会我真的啥也没听懂，倒是快走人的时候才知道——哦，原来我是这么干的啊！\n在FIT楼，五道口的一些碎片 在FIT楼，还挺热，在平常应该穿毛衣的情况下，我在里面基本都是短袖，以至于每次楼下取外卖的时候，门口大爷会一脸错愕的盯着我：“小伙子，火力是旺！”此外，我在知乎还看到了一个问题：《为什么FIT楼这么热》，倒是个性化推送了。\n学校大，食堂离我在的FIT楼远，因此我吃了20多天外卖（海淀的饭真的有点贵的\u0026hellip;）。其实有的生活过的挺苦逼的，比如：\nCase1 早上没有报备入校咋办？ 很正常，系统甚至有延迟，五道口旁边有一个咖啡店，也有喜茶、霸王茶姬，“欢迎来chang”是我这段时间听到最多的话，因为好喝、便宜、人少、离学校和车站近，所以奶茶店学习挺常见的哈\u0026hellip;\n奶茶办公让我想起了“学术酒吧” Case2 苦逼生活？咋个苦法？ 其实只是相对的苦不是绝对的苦，只是不如母校的生活罢了。压力大，任务繁重，好几次连外卖都没顾得上吃，楼道里自动售货机的面包、泡面凑合凑合\u0026hellip;\u0026hellip;\n和呆了一个月的地方合个影吧 但是好景不长，之后就被发配到基站屋子里，和一堆服务器大铁疙瘩待一块去了，一看就是我们臭理工男的工作室，鉴于FIT楼建成已久，所以基建也没那么新。\n我的工位与发配楼上的工位 吃啥 探店+食堂，海淀的饭不便宜。这几天主要还是在照澜园吃饭了，毕竟离得近；有些窗口很便宜，但是教职工自选那里确实是有点贵了（也有可能是我挑贵的拿了）\n饮食记录 还去了方砖厂炸酱面，免费续面条真不错，就是面一般般，可能网红属性就这样。\n有点腻的方砖厂炸酱面，一碗30块钱 结束与告别 用UERANSIM和free5GC、分布式核心网搭建了通信系统，学到了很多，写了个文档，我想想，等之后有时间我写给文档（脱敏版）上传github，就叫《全网最详细版free5gc和ueransim核心网通信链路部署搭建手册》啦！（如果我不懒没有鸽的话）\n汇报 清华的老师：恭喜你完成了一个有意义的short stay，希望你学到了东西！\n时间过得好快，中午被实验室的师兄们拉去清华的某个餐厅点菜聚餐（倒是挺贵，不过师兄们未来应该不会差钱的），当天晚上最后和师兄在金谷园吃了顿饺子，本来打算请师兄吃饭的，没想到师兄提前把单买好了。\n没有回眸，转身离开 告别 收拾东西，感慨诸多，感谢这一段经历，感谢老师、师兄的帮助。其实很多时候人都会被他人灌输的“高价值”事物洗脑，以为某些东西看上去“高级”或者“先进”，实际上祛魅的最好方式是真正去体验一下，去感受一下，才能知道是否和他人描述的相符，这也有点像小马过河吧。\n没有调查就没有发言权。\n这次实地的“考察”让我感觉到，清华，我国工科教育当之无愧的殿堂，事实上里面的学生和大家也没什么不同，只是一批更聚焦、更能坚守的同龄人，在深耕的领域有自己的taste，在朋辈身旁有更大的peer pressure（这可能有优有劣）；清华的设施也没想象中那么新规或者那么老旧，一切都是刚刚好，甚至和母校都很像。\n每一次来到这里，都能唤起许多美好的回忆，又能够发现许多新鲜的东西，心情不同，时间不同，景色也不同。\n说了这么多，权当老夫痴言乱语罢了，前途似海，来日方长。\n","date":"2025-03-12T10:52:03+08:00","image":"https://mosfish.github.io/p/days-in-tsinghua./10_hu_1d5a0457f3f45ccc.webp","permalink":"https://mosfish.github.io/p/days-in-tsinghua./","title":"Days in Tsinghua."},{"content":"突然告知要代表学院出差 事情是这样的，突然接到通知让我和其他几位（我是某校级科创协会会长，其他一块的有类似学生会主席、研会主席、团副这种）去西安交流，于是我请了假、上了车，去了机场。一切是那么的突然，似乎没给我纠结的时间，但是我听到“出差补贴”四个字的时候直接蹦了起来，学生群体真好拿捏呀。\n出行前的轶事：刚好DSP课点名了，我给李教授说请假了，结果教授说：\n不相信，班里总有不少人假冒假条，一点到名就说请假。\n我倒是没翘过课被抓，老师讲的比较带感，唯独我对此不感兴趣，甚至不知道自己是不是真正喜欢现在的EE专业。不过这次“请假”确实是真的，毕竟我都上飞机了。于是，迎着落日余晖和熟悉的干燥的冷气，我到了西安，感受到熟悉却又陌生的气息。\n西安的东西和我家很像，街角的包子铺、肉夹馍，面条饺子之类的，却都是我喜欢吃的东西。酒店有健身房，倒也是不错。\n且向长安那畔行 西电交流 首日，由于我们是第一批，天大寒、砚冰坚，雨夹雪实在有些冷人了。起初进不去，找了迪卡侬取取暖。首日去了西电的研究生校区进行交流，后来发现我们的交流具有现实意义——学硕、专硕与工程博士的区分设立。这里的冬天夹杂了一份秋的踪影，地上金黄色的落叶飞舞着。这段时光或许是人生里很惬意的时光之一了，跟着老师和师兄们，在迪卡侬躲避寒风，品尝泡馍\u0026hellip;\u0026hellip;\n不许动！说了不许动！ 也是有资格上桌交流了 西交交流——创新港，偷摸鱼 好吧这次就没资格上桌了，于是校园走走，嗯，还挺好的其实。\n创新港与电信楼 瞎逛 其实这个地方来了很多次，所以也不知道逛啥，会了会老友（我们仨），然后和大部队一块去了趟省博物馆、大唐不夜城啥的，买了一堆纪念币，除此之外就\u0026hellip;就只是吃吃吃了。\n结语 好吃，太冷，太干，下次不来了。\n当初若选大学的时候来了西交，那可能会有些不一样；不过我不后悔来我的母校，因为我这次访问的时候，他们说西交压力太大了。\n","date":"2024-12-17T18:29:48+08:00","image":"https://mosfish.github.io/p/%E8%A5%BF%E5%AE%89%E4%B8%A4%E4%B8%89%E4%BA%8B/1_hu_efcab814805c5b8e.webp","permalink":"https://mosfish.github.io/p/%E8%A5%BF%E5%AE%89%E4%B8%A4%E4%B8%89%E4%BA%8B/","title":"西安两三事"},{"content":"坎坷而深刻的舞台体验 之前一直喜欢看剧，话剧、舞台剧、音乐剧、交响音乐会都有涉及，不过个人更喜欢音乐剧一些，从法红与黑、法扎、莫里哀（明年订票去），到德扎、伊丽莎白、蝴蝶梦，几乎看了个遍，自学了一丢丢德语、在学校学了法语，不为别的，就为了看剧；相反，话剧的兴趣可能没那么大。\n最近的汇总，被门票爆金币了 偶然进入话剧社 进入这个地方的原因多种多样，有为了扩(ren)大(shi)社(mei)交(zi)的，有喜欢搞剧的，像我这种陪好兄弟去玩结果兄被刷了我进去的还是蛮少见的。整体氛围活泼，只不过好像老东西和新一届有点矛盾，我也不想掺和，现在看来，有一丝后悔、一分无奈、和一点愤怒，这也是后话了。\n第一次见面 主角竟是我？ 作为完全零经验小白，被迫出演《控方证人》的威尔爵士，链接https://www.bilibili.com/video/BV1Rb4y1371f\n一直想着结束后说点啥感想，恰逢一周年吧，又看了看自己演的话剧的录播，其实感触颇多。记得学姐说：\n“你可以出师啦”\n是吗？或许是吧，一个没有经验的小白，至少完完整整的演完了2个小时；从坐排天天挨骂，念台词被各种批斗；从第一次写人物小传被当众处刑破防，记得好像是第12周，那一周家族企业管理一共写了9000多字的报告，智能车队要验收，信发的好多活动需要我策划，即使是在这种情况下，我依然从头再造的写完了1w多字的人物小传。为了话剧，我放弃了许多，最可惜的莫过于时间不足先是在无人机队招新时中途退出，再是放弃了智能车队验收，没有办法，没有资格去参加明年的飞思卡尔了。\n排练的碎片 后悔吗？不后悔。\n为了排练，我牺牲了时间；每次排练都全勤，一次迟到也没有，甚至连加练都是提前10分钟到，每次除了在工作里挨骂就是被剧组屌，有的时候感觉真的难绷但是还是挺过来了。12月9日有ai有e的活动，那天我记得我上下楼走了2万多步，很累。唯一高兴的事情就是中午吃饭的时候见到了控方证人的海报，那天结束边收拾实验室，边拿着麦克风背台词、练台词；第一个剧本，翻的次数太多，皱的不成样子，于是我自己又印了一个。多少日夜，多少排练，多少绕口令，回忆涌上心头，可惜曾经的朋友居然因为理念不同反戈相向，惋惜、痛心。\n学好声韵辨四声，阴阳上去要分明， 部位方法须找准，开齐合撮属口形。 双唇班抱必百波，抵舌当地斗点钉， 舌根高狗工耕故，舌面机结教坚精， 翘舌主争真志照，平舌资责早在增。 擦音发翻飞分复，送气查柴产彻称。 合口忽午枯胡鼓，开口河坡哥安争。 嘴撮虚学寻徐剧，齐齿衣优摇业英。 抵颚恩音烟弯稳，穿鼻昂迎中拥生。 咬紧字头归字尾，不难达到纯和清。\n仍记得定妆照那天赶完ddl就睡了三个小时；早上穿着戏服上课当显眼包，中午没吃饭就去拍照，结果因为负责的同学没有提前对接被赶了出来。下午提前做完实验赶到现场，依稀记得女搭档们忙前忙后给我化妆的样子，仍然记得学姐顶着发烧来给我们拍照\u0026hellip;\u0026hellip;太多太多瞬间。\n这是哪位呢？我不认识 临门一脚 前几天晚上最后一次联排，相当于演习。整体很仓促，大家都容易着急；导演也急，期待最后一天能超常发挥。\n啊？ 早上8点半到活动中心搬东西，我一个“老头”还得一步一步从那里把门扛到新传剧场，早上多亏三个小煎饺，中午饭贼清淡吃了几口就扔了，眯了几分钟还被吵醒；快彩之后，蹬去明德做发型＋戴隐形，来回就用了14分钟；彩排后才开始化妆，化完之后还没有歇口气就又要正式演出\u0026hellip;\u0026hellip;\n忙到乱成一团了 在聚光灯下 演出很顺利，起初面对聚光灯手抖的不行，逐渐到泰然自若、享受掌声，这是这段时间的成长。剧组每个人都很辛苦，在这短短的一个月每个人都舍弃了不少，但是收获的更多。有很多感动，很多欢乐，很多友谊，以及那天晚上的牛肉煲真他娘的香！\n场照、合影与谢幕 岁月不居，时节如流。\n所以，青春永不落幕，对吗？\n在聚光灯下 ","date":"2024-12-08T12:25:21+08:00","image":"https://mosfish.github.io/p/%E8%AE%B0%E4%B8%80%E6%AC%A1%E8%AF%9D%E5%89%A7%E4%B8%BB%E8%A7%92%E7%BB%8F%E5%8E%86/12_hu_3f2c33362a33220b.jpg","permalink":"https://mosfish.github.io/p/%E8%AE%B0%E4%B8%80%E6%AC%A1%E8%AF%9D%E5%89%A7%E4%B8%BB%E8%A7%92%E7%BB%8F%E5%8E%86/","title":"记一次话剧主角经历"}]